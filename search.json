{
  "articles": [
    {
      "path": "Distill_Quality_Control.html",
      "title": "Distill Quality Control",
      "description": "Checking the Qualiy of our distill website\n",
      "author": [
        {
          "name": "Nils Ratnaweera",
          "url": {}
        }
      ],
      "date": "2021-11-01",
      "contents": "\n\nContents\nMultiple html files in folders?\n\n\n\n\nThis file was rendered 2022-01-18 07:56:17.\nMultiple html files in folders?\nThis next chunk checks if there are mulitple html files within the folders. It actually removes html files that don’t match the rmd file’s name.\n\nShow code\nmydirs <- list.dirs(recursive = FALSE,full.names = FALSE)\n\ntopics <- mydirs[startsWith(mydirs, \"_\")]\narticles <- list.dirs(topics,recursive = FALSE)\nrm_hmtl <- function(folder){\n  html <- list.files(folder, \"\\\\.html$\")\n  html_noext <- sub(\"(.+)\\\\.html$\", \"\\\\1\", html)\n  rmd <- list.files(folder, \"\\\\.Rmd$\", ignore.case = TRUE)\n  rmd_noext <- sub(\"(.+)\\\\.[Rr]md$\", \"\\\\1\", rmd)\n  html_extra <- html[!html_noext %in% rmd_noext]\n  \n  if(length(html_extra)>0){\n    html_rm <- file.path(folder, html_extra)\n    file.remove(html_rm)\n    return(paste(\"Multiple html files found. Removing: \", html_extra))\n  }\n  if(length(rmd)>1){\n    return(\"Multiple Rmd files found\")\n  }\n}\n\nres <- sapply(articles, function(x){rm_hmtl(x)})\n\n\nres[sapply(res, is.null)] <- NULL\n\nif(length(res)>0){\n  res\n} else{\n  print(\"all good, nothing to report\")\n}\n\n\n[1] \"all good, nothing to report\"\n\n\n\n\n",
      "last_modified": "2022-01-18T07:56:17+00:00"
    },
    {
      "path": "Distill_Quality_Control.html",
      "title": "Distill Quality Control",
      "description": "Checking the Qualiy of our distill website\n",
      "author": [
        {
          "name": "Nils Ratnaweera",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nMultiple html files in folders?\n\n\n\n\nThis file was rendered 2022-01-18 07:56:17.\nMultiple html files in folders?\nThis next chunk checks if there are mulitple html files within the folders. It actually removes html files that don’t match the rmd file’s name.\n\nShow code\nmydirs <- list.dirs(recursive = FALSE,full.names = FALSE)\n\ntopics <- mydirs[startsWith(mydirs, \"_\")]\narticles <- list.dirs(topics,recursive = FALSE)\nrm_hmtl <- function(folder){\n  html <- list.files(folder, \"\\\\.html$\")\n  html_noext <- sub(\"(.+)\\\\.html$\", \"\\\\1\", html)\n  rmd <- list.files(folder, \"\\\\.Rmd$\", ignore.case = TRUE)\n  rmd_noext <- sub(\"(.+)\\\\.[Rr]md$\", \"\\\\1\", rmd)\n  html_extra <- html[!html_noext %in% rmd_noext]\n  \n  if(length(html_extra)>0){\n    html_rm <- file.path(folder, html_extra)\n    file.remove(html_rm)\n    return(paste(\"Multiple html files found. Removing: \", html_extra))\n  }\n  if(length(rmd)>1){\n    return(\"Multiple Rmd files found\")\n  }\n}\n\nres <- sapply(articles, function(x){rm_hmtl(x)})\n\n\nres[sapply(res, is.null)] <- NULL\n\nif(length(res)>0){\n  res\n} else{\n  print(\"all good, nothing to report\")\n}\n\n\n[1] \"all good, nothing to report\"\n\n\n\n\n",
      "last_modified": "2022-01-18T07:56:17+00:00"
    },
    {
      "path": "Fallstudien.html",
      "title": "Fallstudien",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:18+00:00"
    },
    {
      "path": "index.html",
      "title": "Modul Research Methods",
      "author": [],
      "contents": "\n\n\n\nDas Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\nDiese Website wurde am 2022-01-18 08:56:18 zum letzten Mal aktualisiert.\n\n\n\n",
      "last_modified": "2022-01-18T07:56:18+00:00"
    },
    {
      "path": "InfoVis_abstract.html",
      "title": "Informations Visualisierung",
      "author": [],
      "contents": "\n\nContents\nInfovis 1\nInfovis 2\n\nInfovis 1\nDie konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.\nInfovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.\n\n\n\n",
      "last_modified": "2022-01-18T07:56:18+00:00"
    },
    {
      "path": "InfoVis.html",
      "title": "InfoVis",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:19+00:00"
    },
    {
      "path": "mce.html",
      "title": "Multi Criteria Evaluation",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:19+00:00"
    },
    {
      "path": "PrePro_abstract.html",
      "title": "Preprocessing: Die Vorverarbeitung von Daten",
      "author": [],
      "contents": "\nDie Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.\nDie Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.\n\n\n\n",
      "last_modified": "2022-01-18T07:56:20+00:00"
    },
    {
      "path": "PrePro.html",
      "title": "Daten Preprocessing",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:20+00:00"
    },
    {
      "path": "RaumAn_abstract.html",
      "title": "Räumliche Analyse",
      "author": [],
      "contents": "\n\nContents\nTeil 1\nTeil 2\nTeil 3\n\nTeil 1\nDie erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen umfassen den Spatial Join (Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten) sowie Puffer-Operationen. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP). Inhaltlich orientiert sich die Übung an Bodeneigenschaften für den Untersuchungsraum Schweiz.\nTeil 2\nIn dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen ein kontinuierliches Raster von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Bevor wir aber starten, schauen wir uns die Punktdatensätze genauer an indem wir die G-Function berechnen und plotten.\nTeil 3\nZum Abschluss des Themenblockes Spatial Data Science berechnen wir mit dem Moran’s I einen Index zur Berechnung der räumlichen Autokorrelation einer Choroplethenkarte. Wir verwenden nochmals die aggregierten Choroplethenkarten zur Wasserverfügbarkeit aus der ersten Übung und schauen uns an, wie stark die Werte für die Kantone und die Bezirke autokorreliert sind. Anstatt einfach eine Funktion zur Berechnung von Moran’s I aufzurufen und diese dann wie eine Black Box anzuwenden, wollen wir Formel für die Berechnung des Index in Ihre Bausteine zerlegen und diese Schritt für Schritt selber nachrechnen. So seht Ihr, wie Moran’s I wirklich funktioniert und könnte dabei erst noch die zuvor gelernten Data Science Techniken repetieren.\nAusserdem zeigen wir Euch ganz einfache Verfahren, um die bereits erstellten Karten interaktiv zu machen.\nAuf geht’s!\n\n\n\n",
      "last_modified": "2022-01-18T07:56:21+00:00"
    },
    {
      "path": "RaumAn.html",
      "title": "Räumliche Analyse: 'Spatial Data Science'",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:21+00:00"
    },
    {
      "path": "Readme.html",
      "title": "Anleitungen für Admins",
      "description": "Im Kurs Research Methods verwenden wir seit einigen Jahren RMarkdown um die R Unterlagen für die Studenten bereit zu stellen. Diese Anleitung soll alle nötigen Informationen für \"Admins\" zur Verfügung stellen, damit dieser Prozess möglichst reibungslos abläuft. \n\nDie Anleitung besteht aus einer Kurzen Einführung in die Verwendeten Tools (\"distill\" und \"renv\") sowie drei Anleitungen (1. Software aufsetzen, 2. Projekt aufsetzen, 3. Inhalte editieren und veröffentlichen)\n",
      "author": [],
      "contents": "\n\nContents\nAllgemeinesDistill\nRenv\n\nAnleitung 1: Software AufsetzenR, RStudio und Git installieren\nRStudio Konfigurieren\nGit konfigurieren\n\nAnleitung 2: Projekt aufsetzenRepo Klonen\n“Upstream” setzen\nNotwendige Packages installieren\n\nAnleitung 3: Inhalte Editieren und veröffentlichenRmd erstellen\nRmd editieren\nRmd Kompilieren\nÄnderungen veröffentlichen\n\nAnleitung 4 (Fortgeschritten): Listings editieren und veröffentlichen\n\nAllgemeines\nDer Workflow in Kürze:\n\nDistill\nBis und mit HS2020 haben wir für die Bereitstellung der Unterlagen bookdown verwendet, im HS2021 wollen wir zu distill wechseln.\nVorteil: Mit Bookdown müssen bei Änderungen jeweils alle .Rmd-Files neu kompiliert werden, was unter umständen sehr lange dauern kann. Mit distill ist jedes .Rmd File wie ein eigenes kleines Projekt und kann eigenständig kompiliert werden.\nNachteile:\nWerden files in mehreren .Rmd Files benutzt müssen diese für jedes .Rmd file abgespeichert werden\nein PDF kann nicht ohne weiteres generiert werden\n\nPS: Wir verwenden eine von Nils leicht modifizierte Version von distill\nRenv\nIm Unterricht werden sehr viele RPackages verwendet. Bei gewissen Packgages ist es kritisch, das wir mit der gleichen Version arbeiten (dies gilt v.a. für distill). Um sicher zu stellen, das wir alle mit der gleichen Version dieser Packages arbeiten verwenden wir das RPackage renv. Das arbeiten mit renv bringt folgende Änderungen mit sich:\nPackages werden alle im Projektfolder installiert (renv/library) statt wie üblich in C:/Users/xyz/Documents/R/win-library/3.6 bzw. C:/Program Files/R/R-3.6.1/library\nDies wird durch .Rprofile sichergestellt (.Rprofile wird automatisch beim Laden des Projektes ausgeführt)\nDer Folder renv/library wird nicht via github geteilt (dies wird mit renv/.gitignore sichergestellt)\n\nDie Liste der Packages wird in renv.lock festgehalten (mit dem Befehl renv::snapshot(), mehr dazu später)\nDie Packages werden mit renv::restore() lokal installiert\nAnleitung 1: Software Aufsetzen\nR, RStudio und Git installieren\n(wer dies bereits gemacht hat oder auf dem RStudio Server arbeitet, kann diesen Schritt überspringen)\nWer Lokal auf seinem eingenen PC arbeiten will, muss eine aktuell version von R, RStudio und Git installieren. Siehe dazu folgende Anleitungen:\nhappygitwithr: Install or upgrade R and RStudio\nhappygitwithr: Install Git\nRStudio Konfigurieren\nIch empfehle folgende Konfiguration in RStudio (Global Options):\nR Markdown\nShow document outline by default: checked (Stellt ein Inhaltsverzeichnis rechts von .Rmd files dar)\nSoft-wrap R Markdown files: checken (macht autmatische Zeilenumbrüche bei .Rmd files)\nShow in document outline: Sections Only (zeigt nur “Sections” im Inhaltsverzeichnis)\nShow output preview in: Window (beim kopilieren von Rmd Files wird im Anschluss ein Popup mit dem Resultat dargestellt)\nShow equation an image previews: In a popup\nEvaluate chunks in directory: Document (<- wichtig !)\n\nCode > Tab “Saving”\nDefault Text Encoding: UTF-8 (<- wichtig !)\n\nGit konfigurieren\n(wer dies bereits gemacht hat, kann diesen Schritt überspringen)\nNach der Installation muss git konfiguriert werden. Siehe dazu folgende Kapitel:\nhappygitwithr: Introduce yourself to Git\nhappygitwithr: Cache credentials for HTTPS\nAnleitung 2: Projekt aufsetzen\nRepo Klonen\nUm die ganzen *.Rmd Files lokal bearbeiten zu können, muss das Repository geklont werden. Mit RStudio ist dies sehr einfach, siehe dazu nachstehende Anleitung. Als Repo-URL folgendes einfügen: https://github.com/ResearchMethods-ZHAW/HS21.git\nhappygitwithr: New RStudio Project via git clone\n“Upstream” setzen\nUm das Github repo als standart “upstream” zu setzen muss man im Terminal nachstehenden Befehl eingeben. Danach RStudio neu starten und das entsprechende Projekt laden. Nun sollte im “Git” fenster der “Push” button nicht mehr inaktiv sein.\ngit branch -u origin/main\nNotwendige Packages installieren\nWie bereits erwähnt, verwenden wir im Projekt renv.\nInstalliere dieses Package mit install.packages(\"renv\")\nInstalliere alle notwendigen Packages mit renv::restore()\nWichtig: Es hat sich herausgestellt, das die Vereinigungsmenge aller Packages eine unhandlich grosse Liste von Packages ist: Damit nicht alle Personen alle Packages installieren müssen, verwenden wir renv mit der Einstellung snapshot.type: explicit. Damit werden in renv.lock nur Packages inkludiert, welche in DESCRIPTION festgehalten sind.\nZudem haben wir die RVersion Manuell gesetzt, und zwar via r.version: 4.1.0\nWeitere infos zu renv für Kollaboration: Collaborating with renv\nAnleitung 3: Inhalte Editieren und veröffentlichen\n\n\n\nEine distill Webseite besteht aus einzelnen .Rmd Files\nPro Rmd-File exisitert ein eigener Ordner im Projekt\nDiese Rmd fiels werden beim Kompilieren (knìt) in .html-Files konvertiert\nUm daraus eine zusammenhängende Website zu machen ist das File _site.yml verantwortlich.\nRmd erstellen\nDie meisten Inhalte exisitieren bereits und ihr müsst sie nur noch anpassen. Falls ihr aber ein neues .Rmd File erstellen möchtet, müsst ihr einen Unterordner in einem der Ordner _fallstudien, _infovis, _mce, _prepro, _rauman, _statistik oder _statistik-konsolidierung erstellen. Der Name des Unterordners bestimmt einerseits den URL des Beitrags und andererseits die Position der Erscheinung in der “Listing”.\nFügt dem Artikel danach einen YAML header hinzu. Nutzt dazu am besten die Vorlage eines ähnlichen Artikels.\nRmd editieren\nUm Inhalte zu editieren, öffnet ihr das entsprechende .Rmd file in einem der Ordner _fallstudien, _infovis, _mce, _prepro, _rauman, _statistik oder _statistik-konsolidierung. Ihr könnt dieses File wie ein reguläres, eigenständiges .Rmd File handhaben. Wichtig: Alle Pfade im Dokument sind relativ zum .Rmd File zu verstehen: Das Working directory ist der Folder des entsprechenden Rmd Files!!.\nRmd Kompilieren\nUm das Rmd in Html zu konvertieren (“Kompilieren”) klickt ihr auf “Knit” oder nutzt die Tastenkombination Ctr + Shift + K.\nÄnderungen veröffentlichen\nUm die Änderungen zu veröffentlichen (für die Studenten sichtbar zu machen) müsst ihr diese via git auf das Repository “pushen”. Vorher aber müsst ihr die Änderungen stage-en und commit-en. Ich empfehle, dass ihr zumindest zu beginn mit dem RStudio “Git” Fenster arbeitet.\nstage: Setzen eines Häckchens bei “Staged” (im Terminal mit git add .)\ncommit: Klick auf den Button “commit” (im Terminal mit git commit -m \"deine message\")\npull: Klick auf den Button “Pull” (im Terminal mit git pull)\npush: Click auf den button “Push” (im Terminal mit git push)\nAchtung:\nUm Änderungen, die ihr am .Rmd gemacht habt, sichtbar zu machen müsst ihr das .Rmd File zuerst kompilieren (mit Ctrl+Shift+K oder dem button “Knit”)\nEure Beitrag werden in einem html file gespeichert, welches gleich heisst wie euer Rmd file (aber eben mit der html Erweiterung)\nDas “builden” der site passiert nach jedem Push via einer github action\nAnleitung 4 (Fortgeschritten): Listings editieren und veröffentlichen\n(dieser Teil ist eher “advanced” und nicht für alle Interessant)\ndistill verfügt über die Möglichkeit, sogenannte “Collections” zu machen. Eine collection ist eine Sammlung von .Rmd files zu einem bestimmten Thema, für welche automatisch eine Übersichtsseite (sog. “Listing”) erstellt wird.\nUm eine collection zu erstellen:\nneuer Ordner mit entsprechendem Namen und vorangestelltem Underscore (_) im Projektordner erstellen (z.b. _fallstudien)\nneues Rmd-File mit einem passenden Namen im Projektordner erstellen (z.B. infoVis.Rmd)\ndas neue Rmd File mit einem Rmd-YAML Header verstehen mit mindestens folgendem Inhalt: title und listing (siehe unten)\n---\ntitle: \"InfoVis\"      # <- der title kann selbst gewählt werden\nlisting: infovis      # <- hier kommt der ordnername der \"collection\" ohne Underscore\n---\ndie Listing in _site.yml zugänglich machen:\nnavbar:\n  right:\n    - text: \"InfoVis1\"     # <- der Text kann frei gewählt werden\n      href: InfoVis.html   # <- hier kommt der Name des Rmd files mit der Endung .html\n\n\n\n",
      "last_modified": "2022-01-18T07:56:21+00:00"
    },
    {
      "path": "Statistik_abstract.html",
      "title": "Statistik",
      "author": [],
      "contents": "\n\nContents\nStatistik 1\nStatistik 2\nStatistik 3\nStatistik 4\nStatistik 5\nStatistik 6\nStatistik 7\nStatistik 8\n\nStatistik 1\nIn Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.\n\n\n\n\nStatistik 2\nIn Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.\n\n\nStatistik 3\nStatistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.\nStatistik 4\nHeute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs).\nStatistik 5\nIn Statistik 5 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben.\nStatistik 6\nStatistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS.\nStatistik 7\nIn Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch\nStatistik 8\nIn Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann. Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.\n\n\n\n",
      "last_modified": "2022-01-18T07:56:22+00:00"
    },
    {
      "path": "Statistik-Konsolidierung_abstract.html",
      "title": "Statistik Konsolidierung",
      "author": [],
      "contents": "\n\nContents\nStatistik Konsolidierung\nStatstik Konsolidierung 1\nStatistik Konsolidierung 2\nStatistik Konsolidierung 3\nStatistik Konsolidierung 4\n\nStatistik Konsolidierung\nIn den vier Blöcken “Konsolidierung Statistik” repetieren die Studierenden die wichtigen Verfahren der Inferenz-Statistik. Beginnend mit den beiden Fällen, dem \\(Chi ^{2}/\\)-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Die Studierene wiederholen auch die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Weiter geht es mit der Repetition von komplexere Versionen linearer Regressionen und generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Abschliessend bekommen die Studierenden eine Einführung in die Welt der Ordinationen z.B. PCA.\nStatstik Konsolidierung 1\nIn diesem Block beschäftigen wir uns mit folgenden Inhalten:\nWarum Statistik? Warum mit R? Genereller Ablauf einer statistischen Analyse \\(Chi ^{2}/\\)-Test- bzw. Fishers Test (für kategoriale Daten) t-Test (für metrische Daten)\nStatistik Konsolidierung 2\nIn Statistik Konsolidierung 2 bekommen die Studierenden eine Einführung in das Thema der Ordinationen, eine Technik der deskriptiven Statistik. Diese Methoden visualisiert die Strukturen in multivariaten Datensätzen via Dimensionsreduktion. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich NMDS.\nStatistik Konsolidierung 3\nIn Statistik Konsolidierung 3 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R. Im Fokus steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind.\nStatistik Konsolidierung 4\nIn Statistik Konsolidierung 4 kennen die Studierende alles Rund um das Thema der lineare Regressionen (inkl. nicht-lineare Regressionen). Die Studierenden bekommen eine Einführung in die generalized linear models (GLMs), eine Methode die einige wesentliche Limitierungen von linearen Modellen überwindenwerden können. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und logistische Regression für ja/nein-Daten anschauen.\n\n\n\n",
      "last_modified": "2022-01-18T07:56:22+00:00"
    },
    {
      "path": "Statistik-Konsolidierung.html",
      "title": "Statistik Konsolidierung",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:23+00:00"
    },
    {
      "path": "Statistik.html",
      "title": "Statistik",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-01-18T07:56:23+00:00"
    }
  ],
  "collections": ["fallstudien/fallstudien.json", "infovis/infovis.json", "mce/mce.json", "prepro/prepro.json", "rauman/rauman.json", "statistik-konsolidierung/statistik-konsolidierung.json", "statistik/statistik.json"]
}
