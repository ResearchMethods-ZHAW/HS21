[
  {
    "path": "fallstudien/BE_N_0_Vorbemerkung/",
    "title": "Vorbemerkung Fallstudie WPZ - Profil N",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-10-11",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nAktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_N_0_Vorbemerkung/Reh_graf.jpg",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_1_Aufgabe3_Datenverarbeitung_Loesung/",
    "title": "KW42 Daten(vor)verarbeitung - Loesung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N) Musterloesung"
    ],
    "contents": "\nProjektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen muessen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der spaeteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden koennen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)\nAufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklaert werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS21. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur (ich verwende hier den Projektordner mit den Unterordnern Skripts, \n# Feldaufnahmen, Data, Results, Plots)\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwaehnten Kopf des Skripts) 4 Kapitel: \n\n\n\n1. Datenimport\n2. Datenvorverarbeitung\n3. Analyse\n4. Visualisierung\nBereitet euer Skript also nach dieser Struktur vor. Nutzt fuer den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################\n\n\n\nlibraries laden: hier tidyverse\n\n\nlibrary(tidyverse)\n\n\n\nHerunterladen der Daten der Feldaufnahme von Moodle, Einlesen, Sichtung der Datensaetze und der Datentypen\n\n\n# Die Datensätze aller Teams müssen erst noch in CSVs umgewandelt werden, bevor sie \n# eingelesen werden können \n\ndf_team1 <- read_delim(\"Felderhebung Waldstruktur_TEAM_1_türkis.csv\", \n                       delim = \";\")\n\ndf_team2 <- read_delim(\"Felderhebung_Team_2.csv\", delim = \";\")\n# Achtung! Beim Datensatz des Teams 2 ist eine zusaetzliche Zeile eingefuegt, die\n# das Einlesen erschwert. --> loeschen\n# Ausserdem gibt es bei den Zeilen DG Rubus, DG Strauchschicht und DG Baumschicht ein  \n# Problem mit dem Datentyp resp. den Zahlen. \n# --> manuell in Excel , suchen und mit . ersetzen\n\ndf_team3 <- read_delim(\"ReMe_Felderhebung_Gruppe3.csv\", delim = \",\")\n# Achtung! Hier ist beim Einlesen etwas falsch gelaufen. --> \",\" statt \";\"\n\ndf_team4 <- read_delim(\"Felderhebung_Waldstruktur_Team_4.csv\", \n                       delim = \";\")\n\ndf_team5 <- read_delim(\"Felderhebung_Waldstruktur_Team5.csv\", \n                       delim = \";\")\n# Achtung! Beim Umwandeln in das CSV muss hier die Titelzeile entfernt werden damit\n# das Einlesen reibungslos funktioniert\n\ndf_team6 <- read_delim(\"Aufnahmen_Landforst_HS21_Gruppe_6.csv\", \n                       delim = \";\")\n\n\n# hier koennen die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(\"Aufgabe3_Reh_Waldstruktur_211014.csv\", delim = \";\")\nstr(df_reh)\n\n\n# Die eingelesenen Datensaetze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n\n\n\nAufgabe 1:\n1.1 Einfuegen zusaetzliche Spalte pro Datensatz mit der Gruppenzugehoerigkeit (Team1-6)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensaetzen gleich sind und der Gesamtdatensatz zusammengefuegt werden kann\n–> Befehle mutate und rename, mit pipes (%>%) in einem Schritt moeglich\n\n\n#.#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n#.#################################################################################\n\n\ndf_team1 <- df_team1 %>%\n  mutate(team = \"team1\") %>%\n  rename(KreisID = \"Kreis (r=12.5)\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\n\ndf_team2 <- df_team2 %>%\n  mutate(team = \"team2\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"DG Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team3 <- df_team3 %>%\n  mutate(team = \"team3\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\n\ndf_team4 <- df_team4 %>%\n  mutate(team = \"team4\") %>%\n  rename(KreisID = \"Kreis\",\n         DG_Rubus = \"Deckungsgrad Rubus sp.\",\n         DG_Strauchschicht = \"DG Strauchschicht\",\n         DG_Baumschicht = \"DG Baumschicht\")\n\ndf_team5 <- df_team5 %>%\n  mutate(team = \"team5\") %>%\n  rename(KreisID = \"Kreis (r12.5)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp.[%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team6 <- df_team6 %>%\n  mutate(team = \"team6\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\n\n\nAufgabe 2:\nZusammenfuehren der Teildatensaetze zu einem Datensatz\n\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\n\n\n\nAufgabe 3:\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz.\n–> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensaetze\n\n\ndf_with_LIDAR <- left_join(df_gesamt,df_reh, by = c(\"X\" = \"x\", \"Y\" = \"y\"))\n\n\n\nAufgabe 4:\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusaetzlich Einfaerben der Gruppen und Regressionslinie darueberlegen).\n\n\n#.#####################################################################################\n# 4. VISUALISERUNG #####\n#.#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_us, DG_Strauchschicht, color = team)) + geom_point() + \n  stat_smooth(method = \"lm\")\n\n\n\nwrite_delim(df_with_LIDAR, \"df_with_lidar.csv\", delim = \";\")\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_N_1_Aufgabe3_Datenverarbeitung_Loesung/BE_N_Aufgabe3_Datenverarbeitung_Loesung_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_N_1_Aufgabe3_Datenverarbeitung/",
    "title": "KW42: Daten(vor)verarbeitung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nProjektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen muessen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der spaeteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden koennen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)\nAufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklaert werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS21. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur (ich verwende hier den Projektordner mit den Unterordnern Skripts, \n# Feldaufnahmen, Data, Results, Plots)\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwaehnten Kopf des Skripts) 4 Kapitel: \n\n\n\n1. Datenimport\n2. Datenvorverarbeitung\n3. Analyse\n4. Visualisierung\nBereitet euer Skript also nach dieser Struktur vor. Nutzt fuer den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################\n\n\n\nlibraries laden: hier tidyverse\n\n\nlibrary(tidyverse)\n\n\n\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensaetze und der Datentypen\n\n\n# Die Datensätze aller Teams müssen erst noch in CSVs umgewandelt werden, bevor sie \n# eingelesen werden können \n\ndf_team1 <- read_delim(\"Felderhebung Waldstruktur_TEAM_1_türkis.csv\", \n                       delim = \";\")\n\ndf_team2 <- read_delim(\"Felderhebung_Team_2.csv\", delim = \";\")\n# Achtung! Beim Datensatz des Teams 2 ist eine zusaetzliche Zeile eingefuegt, die\n# das Einlesen erschwert.\n# Ausserdem gibt es bei den Zeilen DG Rubus, DG Strauchschicht und DG Baumschicht ein  \n# Problem mit dem Datentyp resp. den Zahlen.\n\ndf_team3 <- read_delim(\"ReMe_Felderhebung_Gruppe3.csv\", delim = \";\")\n# Achtung! Hier ist beim Einlesen etwas falsch gelaufen. \n\ndf_team4 <- read_delim(\"Felderhebung_Waldstruktur_Team_4.csv\", \n                       delim = \";\")\n\ndf_team5 <- read_delim(\"Felderhebung_Waldstruktur_Team5.csv\", \n                       delim = \";\")\n# Achtung! Beim Umwandeln in das CSV muss hier die Titelzeile entfernt werden damit\n# das Einlesen reibungslos funktioniert\n\ndf_team6 <- read_delim(\"Aufnahmen_Landforst_HS21_Gruppe_6.csv\", \n                       delim = \";\")\n\n\n# hier koennen die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(\"Aufgabe3_Reh_Waldstruktur_211014.csv\", delim = \";\")\nstr(df_reh)\n\n\n# Die eingelesenen Datensaetze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n\n\n\nAufgabe 1:\n1.1 Einfuegen zusaetzliche Spalte pro Datensatz mit der Gruppenzugehoerigkeit (Team1-6)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensaetzen gleich sind und der Gesamtdatensatz zusammengefuegt werden kann\n–> Befehle mutate und rename, mit pipes (%>%) in einem Schritt moeglich\nAufgabe 2:\nZusammenfuehren der Teildatensaetze zu einem Datensatz\nAufgabe 3:\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz.\n–> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensaetze\nAufgabe 4:\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusaetzlich Einfaerben der Gruppen und Regressionslinie darueberlegen).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_2_Aufgabe3_Berechnung_Homeranges_Loesung/",
    "title": "KW43: Homeranges - Loesung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N) Musterloesung"
    ],
    "contents": "\nBerechung der Home-Ranges der Rehe\nBenötigte Libraries laden\n\n\nipak <- function(pkg){\nnew.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\nif (length(new.pkg))\ninstall.packages(new.pkg, dependencies = TRUE)\nsapply(pkg, require, character.only = TRUE)\n}\npackages <- c(\"sf\", \"raster\", \"tidyverse\", \"adehabitatHR\", \"maptools\", \"sp\", \n              \"ggspatial\", \"rgeos\", \"rgdal\", \"pastecs\")\nipak(packages)\n\n\n\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\n\nRehe <- read_delim(\"Aufgabe3_Homeranges_Rehe_landforst_20211014.csv\", delim = \";\")\n\nstr(Rehe)\n\n\n\nAufgabe 1: In Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\n\nRehe <- Rehe %>%\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nAufgabe 2: Herumschrauben an den Einstellungen von:\n- href (in der Funktion kernelUD)\n- an der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n–> Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\n\nx <- Rehe$X[Rehe$TierID== \"RE13\"]    \ny <- Rehe$Y[Rehe$TierID== \"RE13\"]\nxy <- data.frame(cbind (x, y, rep(1, length(x))))       \ncoordinates(xy)<-c(\"x\",\"y\")                             \nproj4string(xy)<-CRS(\"+init=epsg:21781\")  \n\nplot(xy, col = \"blue\", pch = 19, cex = 1.5)\n\n# Berechnung von href nach: Pebsworth et al. (2012) Evaluating home range techniques: \n# use of Global Positioning System (GPS) collar data from chacma baboons\n\nsigma <- 0.5*(sd(x)+sd(y))                              \nn <- length(x)\nhref <- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud <- kernelUD(xy, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange <- getverticeshr(kud, percent=50)             \n\n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr <- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\",  \n         delete_layer = T )\n\n\n\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden\n\n\npk25_wpz <- brick(\"C:/Users/sigb/Beni/WPZ_Fallstudie/HS20/Data/pk25_wpz.tif\")\n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\nannotation_spatial(pk25_wpz) +\ngeom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\nXmin bzw. Ymin des Grids: c(684000, 234000)\ncellsize des Grids: c(25, 25)\nAnzahl Kreise in X und Y Richtung: c(100, 160)\n\n\nx25       <- GridTopology(c(684000, 234000), c(25, 25), c(100, 160)) \ndata25    = data.frame(1:(100*160))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid25    <- SpatialGridDataFrame(x25, data25,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel25   <- as(grid25, \"SpatialPixelsDataFrame\")\n\n\n# zweites Sampling Grid für einen Ausschnitt aufbauen, plotten\n# -> dient nur der Visualisierung des Sampling Grids um einen Eindruck zu erhalten\n\nx       <- GridTopology(c(684200, 236900), c(25, 25), c(35, 35)) \ndata    = data.frame(1:(35*35))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid    <- SpatialGridDataFrame(x, data,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel  <- as(grid, \"SpatialPixelsDataFrame\")\n\npoints <- as(pixel, \"SpatialPointsDataFrame\")\n\ngrid_plot <- st_buffer(st_as_sf(points), 12.5)\n\nplot(st_geometry(grid_plot))\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\ngeom_sf(data = xy_p, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nAufgabe 3: Testen der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\nDie Theorie zu Korrelation folgt erst ab 1.11\n\n\ndf_with_lidar <- read_delim(\"df_with_lidar.csv\", delim =\";\")\n\n\nlibrary(pastecs)\n\nround(stat.desc(cbind(df_with_lidar$DG_us,df_with_lidar$DG_os,\n                      df_with_lidar$DG_Strauchschicht,df_with_lidar$DG_Baumschicht), \n                basic= F, norm= T), 3)\n\n\n                 V1     V2      V3      V4\nmedian        0.272  0.824  35.000  50.000\nmean          0.295  0.788  38.949  50.273\nSE.mean       0.013  0.013   1.926   1.782\nCI.mean.0.95  0.027  0.026   3.806   3.522\nvar           0.027  0.025 556.585 476.549\nstd.dev       0.164  0.159  23.592  21.830\ncoef.var      0.557  0.201   0.606   0.434\nskewness      0.661 -1.034   0.438   0.042\nskew.2SE      1.662 -2.601   1.106   0.105\nkurtosis     -0.068  0.670  -0.834  -0.930\nkurt.2SE     -0.087  0.848  -1.059  -1.182\nnormtest.W    0.959  0.910   0.953   0.974\nnormtest.p    0.000  0.000   0.000   0.007\n\n# Histogram der Verteilung und die aus den Daten berechnete Normalverteilung als Linie \n# dargestellt\n\nggplot(df_with_lidar, aes(DG_os)) + geom_histogram(aes(y=..density..), \n        color = \"black\", fill = \"white\") + \n     stat_function(fun = dnorm, args = list(mean = \n        mean(df_with_lidar$DG_os, na.rm = T), \n        sd = sd(df_with_lidar$DG_os, na.rm = T)), color = \"black\",size = 1)\n\n\n\n# testen auf Korrelation \n\ncor.test(~ DG_Baumschicht+DG_os, data = df_with_lidar, method=\"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  DG_Baumschicht and DG_os\nt = 5.4918, df = 147, p-value = 1.705e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2696941 0.5377248\nsample estimates:\n      cor \n0.4126009 \n\n\n\n\n",
    "preview": "fallstudien/BE_N_2_Aufgabe3_Berechnung_Homeranges_Loesung/BE_N_Aufgabe3_Berechnung_Homeranges_Loesung_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_N_2_Aufgabe3_Berechnung_Homeranges/",
    "title": "KW43: Homeranges",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nBerechung der Home-Ranges der Rehe\nBenötigte Libraries laden\n\n\nipak <- function(pkg){\nnew.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\nif (length(new.pkg))\ninstall.packages(new.pkg, dependencies = TRUE)\nsapply(pkg, require, character.only = TRUE)\n}\npackages <- c(\"sf\", \"raster\", \"tidyverse\", \"adehabitatHR\", \"maptools\", \"sp\", \n              \"ggspatial\", \"rgeos\", \"rgdal\")\nipak(packages)\n\n\n\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\n\nRehe <- read_delim(\"Aufgabe3_Homeranges_Rehe_landforst_20211014.csv\", delim = \";\")\n\nstr(Rehe)\n\n\n\nAufgabe 1: In Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\n\nRehe <- Rehe %>%\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nAufgabe 2: Herumschrauben an den Einstellungen von:\n- href (in der Funktion kernelUD)\n- an der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n–> Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\n\nx <- Rehe$X[Rehe$TierID== \"RE13\"]    \ny <- Rehe$Y[Rehe$TierID== \"RE13\"]\nxy <- data.frame(cbind (x, y, rep(1, length(x))))       \ncoordinates(xy)<-c(\"x\",\"y\")                             \nproj4string(xy)<-CRS(\"+init=epsg:21781\")  \n\nplot(xy, col = \"blue\", pch = 19, cex = 1.5)\n\n# Berechnung von href nach: Pebsworth et al. (2012) Evaluating home range techniques: \n# use of Global Positioning System (GPS) collar data from chacma baboons\n\nsigma <- 0.5*(sd(x)+sd(y))                              \nn <- length(x)\nhref <- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud <- kernelUD(xy, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange <- getverticeshr(kud, percent=95)             \n\n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr <- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\",  \n         delete_layer = T )\n\n\n\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden\n\n\npk25_wpz <- brick(\"C:/Users/sigb/Beni/WPZ_Fallstudie/HS20/Data/pk25_wpz.tif\")\n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\nannotation_spatial(pk25_wpz) +\ngeom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\nXmin bzw. Ymin des Grids: c(684000, 234000)\ncellsize des Grids: c(25, 25)\nAnzahl Kreise in X und Y Richtung: c(100, 160)\n\n\nx25       <- GridTopology(c(684000, 234000), c(25, 25), c(100, 160)) \ndata25    = data.frame(1:(100*160))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid25    <- SpatialGridDataFrame(x25, data25,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel25   <- as(grid25, \"SpatialPixelsDataFrame\")\n\n\n# zweites Sampling Grid für einen Ausschnitt aufbauen, plotten\n# -> dient nur der Visualisierung des Sampling Grids um einen Eindruck zu erhalten\n\nx       <- GridTopology(c(684200, 236900), c(25, 25), c(35, 35)) \ndata    = data.frame(1:(35*35))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid    <- SpatialGridDataFrame(x, data,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel  <- as(grid, \"SpatialPixelsDataFrame\")\n\npoints <- as(pixel, \"SpatialPointsDataFrame\")\n\ngrid_plot <- st_buffer(st_as_sf(points), 12.5)\n\nplot(st_geometry(grid_plot))\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\ngeom_sf(data = xy_p, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nAufgabe 3: Testen der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\nDie Theorie zu Korrelation folgt erst ab 1.11\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_3_Aufgabe4_Multivariate_Modelle_Loesung/",
    "title": "KW44: Einstieg Multivariate Modelle - Loesung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N) Musterloesung"
    ],
    "contents": "\nEinstieg Habitatselektionsmodell / Multivariate Modelle\nlibraries laden\n\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"psych\", \"car\")\n\nipak(packages)\n\n\n\nAufgabe 1: Einlesen des Gesamtdatensatzes von Moodle\n- Sichtung des Datensatzes und der Datentypen\n- Kontrolle wieviele Rehe in diesem Datensatz enthalten sind\n\n\nDF_mod <- read_delim(\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\", \n                     delim = \";\")\n\nstr(DF_mod)\n\n\nspec_tbl_df [8,370 x 16] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ pres_abs          : num [1:8370] 0 1 0 1 1 1 1 1 1 0 ...\n $ nmb               : num [1:8370] 0 1 0 1 3 1 5 8 2 0 ...\n $ x                 : num [1:8370] 682250 682250 682225 682225 682225 ...\n $ y                 : num [1:8370] 237725 237750 237700 237725 237750 ...\n $ forest            : num [1:8370] 0 0 0 0 0 0 0 0 0 0 ...\n $ slope             : num [1:8370] 10.15 9.97 8.85 10.33 12.66 ...\n $ dist_road_all     : num [1:8370] 60.3 56.3 53.1 68.5 50.2 ...\n $ dist_road_only    : num [1:8370] 60.3 56.3 53.1 68.5 50.2 ...\n $ dist_build        : num [1:8370] 36.16 12.24 57.51 32.52 7.86 ...\n $ forest_prop       : num [1:8370] 0.022246 0.000931 0.018563 0.000824 0 ...\n $ us                : num [1:8370] 0 0.0218 0 0 0 ...\n $ os                : num [1:8370] 0 0.06744 0 0 0.00406 ...\n $ GPStot            : num [1:8370] 420 420 420 420 420 420 420 420 420 420 ...\n $ id                : chr [1:8370] \"RE03\" \"RE03\" \"RE03\" \"RE03\" ...\n $ time_of_day       : chr [1:8370] \"night\" \"night\" \"night\" \"night\" ...\n $ stoerungskategorie: chr [1:8370] \"gering\" \"gering\" \"gering\" \"gering\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   pres_abs = col_double(),\n  ..   nmb = col_double(),\n  ..   x = col_double(),\n  ..   y = col_double(),\n  ..   forest = col_double(),\n  ..   slope = col_double(),\n  ..   dist_road_all = col_double(),\n  ..   dist_road_only = col_double(),\n  ..   dist_build = col_double(),\n  ..   forest_prop = col_double(),\n  ..   us = col_double(),\n  ..   os = col_double(),\n  ..   GPStot = col_double(),\n  ..   id = col_character(),\n  ..   time_of_day = col_character(),\n  ..   stoerungskategorie = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nclass(DF_mod$time_of_day)\n\n\n[1] \"character\"\n\ntable(DF_mod$id)\n\n\n\nRE02 RE03 RE04 RE05 RE06 RE07 RE08 RE09 RE10 RE11 RE12 RE13 \n1204  260  512  388  306  484  980  836  492 1208  652 1048 \n\nDF_mod %>% group_by(id) %>% summarize(anzahl = n())\n\n\n# A tibble: 12 x 2\n   id    anzahl\n   <chr>  <int>\n 1 RE02    1204\n 2 RE03     260\n 3 RE04     512\n 4 RE05     388\n 5 RE06     306\n 6 RE07     484\n 7 RE08     980\n 8 RE09     836\n 9 RE10     492\n10 RE11    1208\n11 RE12     652\n12 RE13    1048\n\nlength(unique(DF_mod$id))\n\n\n[1] 12\n\nAufgabe 2: Unterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\n\n\nDF_mod_night <- DF_mod %>%\n  filter(time_of_day == \"night\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n# Kontrolle\n\ntable(DF_mod_night$time_of_day)\n\n\n\nnight \n 4185 \n\ntable(DF_mod_day$time_of_day)\n\n\n\n day \n4185 \n\nAufgabe 3: Erstellen von Density Plots der Praesenz / Absenz in Abhaengigkeit der unabhaengigen Variablen (für Tag und Nacht)\n\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz \n\npar(mfrow=c(3,3), mar=c(4, 4, 3, 3))\nfor (i in 6:12) {                           # innerhalb des for()-loops die Nummern der \n                                            # gewuenschten Spalten einstellen\n  d  <-  DF_mod_day %>% pull(i)\n  d  <-  density(d)\n  dp  <-  DF_mod_day %>% filter(pres_abs == 1) %>% pull(i)\n  dp <- density(dp)\n  da  <-  DF_mod_day %>% filter(pres_abs == 0) %>% pull(i)\n  da <- density(da)\n  plot(0,0, type=\"l\", xlim=range(c(dp$x,da$x)), ylim=range(dp$y,da$y), \n       xlab=names(DF_mod_day[i]), ylab=\"Density\")\n  lines(dp$x, dp$y, col=\"blue\")             # Praesenz\n  lines(da$x, da$y, col=\"red\")              # Absenz\n}\n\n\n\n\nAufgabe 4: Testen erklärenden Variablen auf Normalverteilung (nur kontinuierlichen)\n\n\n# klassischer Weg mit shapiro-wilk (mehrere Spalten, verschiedenene statistische\n# Kenngrössen werden angezeigt. Normalverteilung: Wert ganz unten. p>0.05 = ja)\n\nround(stat.desc(DF_mod_day[6:12], basic= F, norm= T), 3)\n\n\n               slope dist_road_all dist_road_only dist_build\nmedian        13.442        28.248         33.693    129.838\nmean          15.055        41.289         46.374    154.502\nSE.mean        0.158         0.642          0.665      1.625\nCI.mean.0.95   0.311         1.259          1.303      3.186\nvar          104.994      1725.368       1848.647  11050.895\nstd.dev       10.247        41.538         42.996    105.123\ncoef.var       0.681         1.006          0.927      0.680\nskewness       0.753         1.914          1.666      0.631\nskew.2SE       9.945        25.285         22.003      8.341\nkurtosis      -0.042         4.250          3.147     -0.537\nkurt.2SE      -0.279        28.079         20.790     -3.545\nnormtest.W     0.942         0.800          0.837      0.943\nnormtest.p     0.000         0.000          0.000      0.000\n             forest_prop     us      os\nmedian             0.632  0.059   0.754\nmean               0.590  0.120   0.586\nSE.mean            0.005  0.002   0.006\nCI.mean.0.95       0.010  0.005   0.013\nvar                0.107  0.023   0.173\nstd.dev            0.328  0.151   0.416\ncoef.var           0.555  1.259   0.710\nskewness          -0.366  1.674  -0.388\nskew.2SE          -4.832 22.115  -5.124\nkurtosis          -1.094  3.061  -1.586\nkurt.2SE          -7.226 20.226 -10.481\nnormtest.W         0.919  0.792   0.791\nnormtest.p         0.000  0.000   0.000\n\n# empfohlener Weg\n\nggplot(DF_mod_day, aes(slope)) + geom_histogram(aes(y=..density..), color = \"black\", \n                                                fill = \"white\") + \n  stat_function(fun = dnorm, args = list(mean = mean(DF_mod_day$slope, na.rm = T), \n                                         sd = sd(DF_mod_day$slope, na.rm = T)), \n                color = \"black\",size = 1)\n\n\n\n# Aufgabe 4: die Korrelation bei einem Teildatensatz testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht\n\n\n\nAufgabe 5: Explorative Analysen der Variablen mit Scatterplots, Scatterplotmatrizen\n1) Zu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\n2) Testen der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)\n\n\nchart.Correlation(DF_mod_day[6:12], histogram=TRUE, pch=19, method = \"kendall\")\n\n\n\n#?chart.Correlation\n\npairs.panels(DF_mod_day[6:12], \n             method = \"kendall\", # correlation method\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # show density plots\n             ellipses = TRUE # show correlation ellipses\n             )\n\n\n\n# Aufgabe 5: die Korrelation bei einem Teildatensatz testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht.\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_N_3_Aufgabe4_Multivariate_Modelle_Loesung/BE_N_Aufgabe4_Einstieg_Multivariate_Modelle_Loesung_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_N_3_Aufgabe4_Multivariate_Modelle/",
    "title": "KW44: Einstieg Multivariate Modelle",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nEinstieg Multivariate Modelle / Habitatselektionsmodell\nlibraries laden\n\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"car\")\n\nipak(packages)\n\n\n\nAufgabe 1: Einlesen des Gesamtdatensatzes für die Multivariate Analyse von Moodle\n1) Sichtung des Datensatzes, der Variablen und der Datentypen\n2) Kontrolle wieviele Rehe in diesem Datensatz enthalten sind\nAufgabe 2: Unterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\nAufgabe 3: Erstellen von Density Plots der Praesenz / Absenz in Abhaengigkeit der unabhaengigen Variablen\n\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz \n\npar(mfrow=c(3,3), mar=c(4, 4, 3, 3))\nfor (i in 6:12) {          # innerhalb des for()-loops die Nummern der gewuenschten \n                           # Spalten einstellen\n  d  <-  DF_mod_day %>% pull(i)\n  d  <-  density(d)\n  dp  <-  DF_mod_day %>% filter(pres_abs == 1) %>% pull(i)\n  dp <- density(dp)\n  da  <-  DF_mod_day %>% filter(pres_abs == 0) %>% pull(i)\n  da <- density(da)\n  plot(0,0, type=\"l\", xlim=range(c(dp$x,da$x)), ylim=range(dp$y,da$y), \n       xlab=names(DF_mod_day[i]), ylab=\"Density\")\n  lines(dp$x, dp$y, col=\"blue\")             # Praesenz\n  lines(da$x, da$y, col=\"red\")              # Absenz\n}\n\n\n\nAufgabe 4: Testen eurer erklärenden Variablen auf Normalverteilung (nur kontinuierliche)\nAufgabe 5: Explorative Analysen der Variablen mit Scatterplots / Scatterplotmatrizen\n1) Zu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\n2) Testen der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_4_Aufgabe4_Modelle_Variablenselektion_Loesung/",
    "title": "KW44: Variablenselektion Multivariate Modelle - Loesung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Biodiversity & Ecosystems (N) Musterloesung"
    ],
    "contents": "\nVariablenselektion Multivariate Modelle / Habitatselektionsmodell\nlibraries laden\n\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"lme4\", \n              \"bbmle\", \"MuMIn\", \"MASS\", \"magrittr\")\n\nipak(packages)\n\n\n\nVariablenselektion\n-> Vorgehen analog Coppes et al. \nAufgabe 1: Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden.\n\n\nDF_mod <- read_delim(\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\", \n                     delim = \";\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n\nround(cor(DF_mod_day[,6:12], method = \"kendall\"),2)\n\n# hier kann die Schwelle fuer die Korrelation gesetzt werden, 0.7 ist liberal / \n# 0.5 konservativ\n\ncor <- round(cor(DF_mod_day[,6:12], method = \"kendall\"),2) \ncor[abs(cor)<0.7] <-0\ncor\n\n\n\nSelektion der Variablen in einem univariaten Model\nAufgabe 2: Skalieren der Variablen, damit ihr Einfluss vergleichbar wird (Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern))\n\n\nDF_mod_day %<>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\n\n\nAufgabe 3: Ein erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann:\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n\n# wir werden das package lme4 mit der Funktion glmer verwenden \n# ausserdem brauchen wir noch das package bbmle\n# --> installieren & laden\n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhaengige Variable ~ Erklaerende Variable + Random Factor \n# In unseren Modellen kontrollieren wir fuer individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren => (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# hier binomial\n\n# warum binomial? Verteilung Daten der Abhaengigen Variable Präsenz/Absenz \n\nggplot(DF_mod_day, aes(pres_abs)) + geom_histogram()\n\n# --> Binaere Verteilung => Binomiale Verteilung mit n = 1 \n\n# und wie schaut es bei der Verteilung der Daten der Abhaengigen Variable \n# Nutzungsintensitaet (nmb) aus?\n\nggplot(DF_mod_day, aes(nmb)) + geom_histogram()\n\n# --> Negativbinomiale Verteilung \n\n\n\nAufgabe 4: Mit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\nAls abhaengige Variable verwenden wir in der ersten Phase die Praesenz/Absenz der Rehe in den Kreisen\n\n\n# Die erklaerende Variable in m1 ist die erste Variable der korrelierenden Beziehung\n# Die erklaerende Variable in m2 ist die zweite Variable der korrelierenden Beziehung\n\nm1 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# mit dieser Funktion koennen die Modellergebnisse inspiziert werden\nsummary(m1)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander \n# abgeschaetzt werden\nbbmle::AICtab(m1, m2)\n\n# tieferer AIC -> besser (AIC = Akaike information criterion) -> als deltaAIC ausgewiesen\n\n# Hier ein Beispiel: Distanz zu Strassen und Wegen versus Distanz zu Strassen\n\nm1 <- glmer(pres_abs ~ dist_road_all_scaled + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(pres_abs ~ dist_road_only_scaled + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# summary(m1)  \n\nbbmle::AICtab(m1, m2)\n\n# --> tiefer AIC -> besser == Distanz zu Strassen\n\n# ==> dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz \n# (geringe Störung/starke Störung) durchgeführt werden, um nur noch nicht (R < 0.7) \n# korrelierte Variablen in das Modell einfliessen zu lassen \n\n\n\nSelektion der Variablen in einem multivariaten Model\nAufgabe 5: Mit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf <- pres_abs ~ \n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm <- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m <- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nimportance(all_m)\n\n\n                     dist_road_only_scaled forest_prop_scaled\nSum of weights:      1.00                  1.00              \nN containing models:   32                    32              \n                     us_scaled slope_scaled dist_build_scaled\nSum of weights:      1.00      0.95         0.67             \nN containing models:   32        32           32             \n                     os_scaled\nSum of weights:      0.39     \nN containing models:   32     \n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\n\nCall:\nmodel.avg(object = get.models(object = all_m, subset = delta < \n    2), rank = \"AICc\")\n\nComponent model call: \nglmer(formula = pres_abs ~ <3 unique rhs>, data = DF_mod_day, \n     family = binomial, na.action = na.fail)\n\nComponent models: \n       df   logLik    AICc delta weight\n12356   7 -2337.01 4688.05  0.00   0.48\n123456  8 -2336.48 4689.00  0.95   0.30\n2356    6 -2338.78 4689.58  1.53   0.22\n\nTerm codes: \n    dist_build_scaled dist_road_only_scaled    forest_prop_scaled \n                    1                     2                     3 \n            os_scaled          slope_scaled             us_scaled \n                    4                     5                     6 \n\nModel-averaged coefficients:  \n(full average) \n                      Estimate Std. Error Adjusted SE z value\n(Intercept)           -0.49074    0.14774     0.14779   3.321\ndist_build_scaled     -0.07877    0.06433     0.06434   1.224\ndist_road_only_scaled  0.44281    0.04792     0.04793   9.239\nforest_prop_scaled     0.83786    0.06487     0.06489  12.912\nslope_scaled          -0.13548    0.04973     0.04975   2.723\nus_scaled              0.40130    0.04101     0.04102   9.784\nos_scaled              0.01926    0.04529     0.04530   0.425\n                      Pr(>|z|)    \n(Intercept)           0.000898 ***\ndist_build_scaled     0.220857    \ndist_road_only_scaled  < 2e-16 ***\nforest_prop_scaled     < 2e-16 ***\nslope_scaled          0.006463 ** \nus_scaled              < 2e-16 ***\nos_scaled             0.670605    \n \n(conditional average) \n                      Estimate Std. Error Adjusted SE z value\n(Intercept)           -0.49074    0.14774     0.14779   3.321\ndist_build_scaled     -0.10139    0.05508     0.05510   1.840\ndist_road_only_scaled  0.44281    0.04792     0.04793   9.239\nforest_prop_scaled     0.83786    0.06487     0.06489  12.912\nslope_scaled          -0.13548    0.04973     0.04975   2.723\nus_scaled              0.40130    0.04101     0.04102   9.784\nos_scaled              0.06466    0.06284     0.06286   1.029\n                      Pr(>|z|)    \n(Intercept)           0.000898 ***\ndist_build_scaled     0.065756 .  \ndist_road_only_scaled  < 2e-16 ***\nforest_prop_scaled     < 2e-16 ***\nslope_scaled          0.006463 ** \nus_scaled              < 2e-16 ***\nos_scaled             0.303636    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ==> für alle weiteren Datensätze muss der gleiche Prozess der Variablenselektion \n# durchgespielt werden. \n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_4_Aufgabe4_Modelle_Variablenselektion/",
    "title": "KW44: Variablenselektion Multivariate Modelle",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nVariablenselektion Multivariate Modelle / Habitatselektionsmodell\nlibraries laden\n\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"lme4\", \n              \"bbmle\", \"MuMIn\", \"MASS\", \"magrittr\")\n\nipak(packages)\n\n\n\nVariablenselektion\n-> Vorgehen analog Coppes et al. \nAufgabe 1: Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden.\n\n\nDF_mod <- read_delim(\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\", \n                     delim = \";\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n\nround(cor(DF_mod_day[,6:12], method = \"kendall\"),2)\n\n# hier kann die Schwelle fuer die Korrelation gesetzt werden, 0.7 ist liberal / \n# 0.5 konservativ\n\ncor <- round(cor(DF_mod_day[,6:12], method = \"kendall\"),2) \ncor[abs(cor)<0.7] <-0\ncor\n\n\n\nSelektion der Variablen in einem univariaten Model\nAufgabe 2: Skalieren der Variablen, damit ihr Einfluss vergleichbar wird (Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern))\n\n\nDF_mod_day %<>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\n\n\nAufgabe 3: Ein erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann:\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n\n# wir werden das package lme4 mit der Funktion glmer verwenden \n# ausserdem brauchen wir noch das package bbmle\n# --> installieren & laden\n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhaengige Variable ~ Erklaerende Variable + Random Factor \n# In unseren Modellen kontrollieren wir fuer individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren => (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# hier binomial\n\n# warum binomial? Verteilung Daten der Abhaengigen Variable Präsenz/Absenz \n\nggplot(DF_mod_day, aes(pres_abs)) + geom_histogram()\n\n# --> Binaere Verteilung => Binomiale Verteilung mit n = 1 \n\n# und wie schaut es bei der Verteilung der Daten der Abhaengigen Variable \n# Nutzungsintensitaet (nmb) aus?\n\nggplot(DF_mod_day, aes(nmb)) + geom_histogram()\n\n# --> Negativbinomiale Verteilung \n\n\n\nAufgabe 4: Mit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\nAls abhaengige Variable verwenden wir in der ersten Phase die Praesenz/Absenz der Rehe in den Kreisen\n\n\n# Die erklaerende Variable in m1 ist die erste Variable der korrelierenden Beziehung\n# Die erklaerende Variable in m2 ist die zweite Variable der korrelierenden Beziehung\n\nm1 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# mit dieser Funktion koennen die Modellergebnisse inspiziert werden\nsummary(m1)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander \n# abgeschaetzt werden\nbbmle::AICtab(m1, m2)\n\n# tieferer AIC -> besser (AIC = Akaike information criterion)\n\n# ==> dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz \n# (Tag/Nacht) durchgeführt werden, um nur noch nicht (R < 0.7) korrelierte Variablen \n# in das Modell einfliessen zu lassen \n\n\n\nSelektion der Variablen in einem multivariaten Model\nAufgabe 5: Mit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf <- pres_abs ~ \n  V1 +\n  V2 +\n  V3 +\n  V4 +\n  V5 +\n  V6 \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm <- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m <- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nimportance(all_m)\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n# ==> für den Nachtdatensatz muss der gleiche Prozess der Variablenselektion \n# durchgespielt werden. \n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_N_5_Aufgabe4_Modelle_und_Diagnostics_Loesung/",
    "title": "KW46: Modellgüte und -diagnostics - Loesung",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-22",
    "categories": [
      "Biodiversity & Ecosystems (N) Musterloesung"
    ],
    "contents": "\nModellguete und -diagnostics MM / Habitatselektionsmodell\nNeue packages die wir fuer die Modelle und die Diagnostics brauchen\n\n\n# neue Packages: DHARMa, car, MASS, ROCR, sjPlot, sjstats, ggeffects, cowplot, gstat\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"lme4\", \"bbmle\", \"MuMIn\", \"tidyverse\", \"DHARMa\", \"car\", \"MASS\", \"ROCR\", \n              \"sjPlot\",  \"ggeffects\", \"sjstats\", \"cowplot\", \"magrittr\", \"gstat\")\n\nipak(packages)\n\n\n\nDer Modellfit von letzter Woche als Ausgangspunkt für die heutige Übung\n\n\nDF_mod_day <- read_delim(\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\", \n                         delim = \";\") %>%\n  filter(time_of_day == \"day\") %>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\nf <- pres_abs ~\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled\n\nf <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\nm_day <- glmer(f, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m <- dredge(m_day)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\n\nDie Modellresultate aus dem avgmodel sind grundaetzlich die finalen Resultate die bereits interpretiert werden koennten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen muessen (an den Resultaten aendert sich dadurch nichts)\n\n\nf_pres_abs <- pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id)\n\nm_day <- glmer(f_pres_abs, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# hier noch zum Vergleich, dass die Resulate sich nur marginal veraendern \n\nsummary(avgmodel)\nsummary(m_day)\n\n# https://stats.stackexchange.com/questions/153611/interpreting-random-effect-variance-in-glmer\n\n# 95% range of the roe deer effects is approximately: -0.97 - 0.97\n\n\n\nAufgabe 1: Berechung der AUC (area under the receiver operating characteristic curve)\n= Mass der Modellguete\nFuer die Berechnung des AUC findet ihr weiterfuehrende Informationen unter: https://www.wsl.ch/staff/niklaus.zimmermann/programs/progs/simtest.pdf)\n\n\nprob <- predict(m_day,type=c(\"response\"))   \npred <- prediction(prob, DF_mod_day$pres_abs)    \n\n# AUC\n\nauc <- performance(pred, measure = \"auc\")@y.values[[1]]\nauc\n\n\n[1] 0.7769194\n\nAufgabe 2: Interpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\nunbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n\n# Residuals werden ueber eine Simulation auf eine Standard-Skala transformiert und \n# koennen anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt \n# werden (dauert je nach dem sehr lange)\n\nsimulationOutput <- simulateResiduals(fittedModel = m_day, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\n\n\ntestResiduals(simulationOutput)\n\n\n\n$uniformity\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  simulationOutput$scaledResiduals\nD = 0.02194, p-value = 0.03559\nalternative hypothesis: two-sided\n\n\n$dispersion\n\n    DHARMa nonparametric dispersion test via sd of residuals\n    fitted vs. simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n\n$outliers\n\n    DHARMa outlier test based on exact binomial test with\n    approximate expectations\n\ndata:  simulationOutput\noutliers at both margin(s) = 1, observations = 4185, p-value =\n0.567\nalternative hypothesis: true probability of success is not equal to 0.00019998\n95 percent confidence interval:\n 6.049637e-06 1.330610e-03\nsample estimates:\nfrequency of outliers (expected: 0.0001999800019998 ) \n                                         0.0002389486 \n$uniformity\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  simulationOutput$scaledResiduals\nD = 0.02194, p-value = 0.03559\nalternative hypothesis: two-sided\n\n\n$dispersion\n\n    DHARMa nonparametric dispersion test via sd of residuals\n    fitted vs. simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n\n$outliers\n\n    DHARMa outlier test based on exact binomial test with\n    approximate expectations\n\ndata:  simulationOutput\noutliers at both margin(s) = 1, observations = 4185, p-value =\n0.567\nalternative hypothesis: true probability of success is not equal to 0.00019998\n95 percent confidence interval:\n 6.049637e-06 1.330610e-03\nsample estimates:\nfrequency of outliers (expected: 0.0001999800019998 ) \n                                         0.0002389486 \n\n# The most common concern for GLMMs is overdispersion, underdispersion and \n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals\n    fitted vs. simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros\n    with simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0388, p-value = 0.4544\nalternative hypothesis: two.sided\n\n# test for spatial Autocorrelation\n\n# calculating x, y positions per group\ngroupLocations = aggregate(DF_mod_day[, 3:4], list(DF_mod_day$x, DF_mod_day$y), mean)\ngroupLocations$group <- paste(groupLocations$Group.1,groupLocations$Group.2)\n\n# calculating residuals per group\nres2 = recalculateResiduals(simulationOutput, group = groupLocations$group)\n\n# running the spatial test on grouped residuals\ntestSpatialAutocorrelation(res2, groupLocations$x, groupLocations$y, plot = F)\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.0149472, expected = -0.0002661, sd = 0.0010081,\np-value < 2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\n# Testen auf Multicollinearitaet (dh zu starke Korrelationen im finalen Modell, zB falls \n# auf Grund der oekologischen Plausibilitaet stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes > 10), if mean of VIF values \n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(m_day)\n\n\n    dist_build_scaled dist_road_only_scaled    forest_prop_scaled \n             1.508604              1.129679              2.542465 \n         slope_scaled             us_scaled             os_scaled \n             1.500129              1.219237              2.455742 \n\nmean(car::vif(m_day))\n\n\n[1] 1.725976\n\nAufgabe 4: Ermittlung des individuellen Beitrags der einzelen Variablen im Gesamtmodell\nBestimmen delta AIC nach Coppes et al. 2017 -> Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable.\n\n\nm_os <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_us <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_road <- glmer(pres_abs ~\n  dist_build_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_forest <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_build <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_slope <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nbbmle::AICtab(m_day, m_os, m_us, m_road, m_forest, m_build, m_slope)\n\n\n         dAIC  df\nm_os       0.0 7 \nm_day      0.9 8 \nm_build    0.9 8 \nm_slope    7.3 7 \nm_us      92.2 7 \nm_road    94.3 7 \nm_forest 141.9 7 \n\nAuftrag auf den 22.11.2021: Jede Gruppe kurze Vorstellung der Modellresultate & Diagnostics im Plenum und Diskussion der Ergebnisse (keine PP-Praesentation noetig)\n\n\n\n",
    "preview": "fallstudien/BE_N_5_Aufgabe4_Modelle_und_Diagnostics_Loesung/BE_NAufgabe4_MM_Guete_und_Diagnostics_Loesung_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_N_5_Aufgabe4_Modelle_und_Diagnostics/",
    "title": "KW46: Modellgüte und -diagnostics",
    "description": {},
    "author": [
      {
        "name": "Beni Sigrist",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "Biodiversity & Ecosystems (N)"
    ],
    "contents": "\nModellguete und -diagnostics MM / Habitatselektionsmodell\nNeue packages die wir fuer die Modelle und die Diagnostics brauchen\n\n\n# neue Packages: DHARMa, car, MASS, ROCR, sjPlot, sjstats, ggeffects, cowplot, gstat\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"lme4\", \"bbmle\", \"MuMIn\", \"tidyverse\", \"DHARMa\", \"car\", \"MASS\", \"ROCR\", \n              \"sjPlot\",  \"ggeffects\", \"sjstats\", \"cowplot\", \"magrittr\", \"gstat\")\n\nipak(packages)\n\n\n\nDer Modellfit von letzter Woche als Ausgangspunkt für die heutige Übung\n\n\nDF_mod_day <- read_delim(\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\", \n                         delim = \";\") %>%\n  filter(time_of_day == \"day\") %>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\nf <- pres_abs ~\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled\n\nf <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\nm_day <- glmer(f, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m <- dredge(m_day)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\n\nDie Modellresultate aus dem avgmodel sind grundaetzlich die finalen Resultate die bereits interpretiert werden koennten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen muessen (an den Resultaten aendert sich dadurch nichts)\n\n\nf_pres_abs <- pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id)\n\nm_day <- glmer(f_pres_abs, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# hier noch zum Vergleich, dass die Resulate sich nur marginal veraendern \n\nsummary(avgmodel)\nsummary(m_day)\n\n\n\nAufgabe 1: Berechung der AUC (area under the receiver operating characteristic curve)\n= Mass der Modellguete\nFuer die Berechnung des AUC findet ihr weiterfuehrende Informationen unter: https://www.wsl.ch/staff/niklaus.zimmermann/programs/progs/simtest.pdf)\n\n\nprob <- predict(m_day,type=c(\"response\"))   \npred <- prediction(prob, DF_mod_day$pres_abs)    \n\n?prediction\n\n# AUC\n\nauc <- performance(pred, measure = \"auc\")@y.values[[1]]\nauc\n\n\n\nAufgabe 2: Interpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\nunbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n\n# Residuals werden ueber eine Simulation auf eine Standard-Skala transformiert und \n# koennen anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt \n# werden (dauert je nach dem sehr lange)\n\nsimulationOutput <- simulateResiduals(fittedModel = m_day, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\ntestResiduals(simulationOutput)\n\n# The most common concern for GLMMs is overdispersion, underdispersion and \n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n# test for spatial Autocorrelation\n\n# calculating x, y positions per group\ngroupLocations = aggregate(DF_mod_day[, 3:4], list(DF_mod_day$x, DF_mod_day$y), mean)\ngroupLocations$group <- paste(groupLocations$Group.1,groupLocations$Group.2)\n\n# calculating residuals per group\nres2 = recalculateResiduals(simulationOutput, group = groupLocations$group)\n\n# running the spatial test on grouped residuals\ntestSpatialAutocorrelation(res2, groupLocations$x, groupLocations$y)\n\n# Testen auf Multicollinearitaet (dh zu starke Korrelationen im finalen Modell, zB falls \n# auf Grund der oekologischen Plausibilitaet stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes > 10), if mean of VIF values \n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(m_day)\nmean(car::vif(m_day))\n\n\n\nAufgabe 3: Graphische Darstellung der Modellresultate\n\n\n# graphische Darstellung der gesamten Modellresultate\n\nplot_model(m_day,transform = NULL, show.values = TRUE, value.offset = .3)\n\n# Plotten der vorhergesagten Wahrscheinlichkeit, dass ein Kreis besetzt ist, in \n# Abhaengigkeit der erklaerenden Variable basierend auf den Modellresultaten.\n\nplot_model(m_day,type = \"pred\", terms = \"us_scaled [all]\")\n\n# Problem: skalierte Variablen lassen sich nicht so ohne weiteres plotten, hier ein quick-\n# and-dirty hack um das Problem zu umgehen. Die Einstellungen muessen fuer jede Variable \n# geaendert werden\n\np <- plot_model(m_day,type = \"pred\", terms = \"us_scaled [all]\") \n\nlabels <- round(seq(floor(min(DF_mod_day$us)), ceiling(max(DF_mod_day$us)), \n                    length.out = 8),2)\n\np <- p + scale_x_continuous(breaks=c(-1,0,1,2,3,4,5,6), labels=c(labels))\n\np\n\n# Funktion um viele Plots auf einem zusammenbringen: cowplot-package (hat auch sonst \n# gute Funktionen fuer schoene layouts von Plots)\n\ncowplot::plot_grid()\n\n\n\nAufgabe 4: Ermittlung des individuellen Beitrags der einzelen Variablen im Gesamtmodell\nBestimmen delta AIC nach Coppes et al. 2017 -> Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable.\nAuftrag auf den 22.11.2021: Kurze Vorstellung der Modellresultate & Diagnostics im Plenum und Diskussion der Ergebnisse (keine PP-Praesentation noetig)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_S_1_Einleitung/",
    "title": "KW40: Einführung",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-09-27",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nEinleitung\nHintergrund\nDas rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich. Seine Rolle als Naherholungsgebiet für die Stadt Zürich ist von grosser Bedeutung.\nIm Perimeter gelten verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren und Hunde müssen an der Leine geführt werden. Damit soll im Schutzgebiet die Balance zwischen Schutz und Nutzen bewahrt werden, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwicklen dürfen.\n\n\n\nDamit diese Balance erreicht werden kann, muss das Management auf solide, empirisch erhobene Daten zur Natur und zu den Besuchenden zurückgreifen können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab.\nIm Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise Besuchende. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in der Fallstudie Profil S mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine Zeitintensive Arbeit und wir dürfen hier mit einem wahren, sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von drei ausgewählten automatischen Zählstellen.\n\n\n\n\nHinweis:\nDie Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt Richtungsgetrennt.\nDer Zähler 204 kann nicht zwischen Nutzungsarten unterscheiden; er erfasst alle Passagen auf den Hochwachtturm als Fussgänger:innen. Der Sensror hat keine Richtungserkennung und die Besuchenden werden jeweils 2x gezählt, einmal beim Aufstieg und einmal beim Abstieg. Das ist im Kalibrierunsgfaktor berücksichtigt, die Kalibrierte Zahl gibt also die Anzahl der Turmbesuche an.\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb) und Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht, da dies die Kapazitäten übersteigen würde.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis.\nZiele\nIn dieser Fallstudie zeigen wir, welche Einfluss der Lockdown während der Covid19-Pandemie im Frühjahr 2020 sowie jener im Winter 2020/2021 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte.\nErgänzend beschreiben wir den Zusammenhang der Besuchszahlen mit verschiedenen Wetterparametern. Die Hypothese lautet, je mehr Sonnenstunden und je höher die Temperatur, desto mehr Besuchende sind im Untersuchungsgebiet unterwegs; je mehr Niederschlag gemessen wird, desto weniger Besuchende werden gezählt.\nDa neben dem Wetter aber auch saisonale Muster, wie z.B. Schulferien, einen grossen Einfluss auf Besuchszahlen haben können, ziehen wir diese und weitere Parameter (Wochentage, Kalenderwoche, Jahr) ebenfalls in die Auswertung ein.\nDiese kombinierte, statistisch schliesssende, Betrachtung erlaubt uns Aussagen darüber, ob “nur” aufgrund des schönen Frühlings 2021 mehr Menschen in Wald unterwegs waren, oder ob der Lockdown tatsächlich einen so deutlich positiven Einfluss auf die Besuche hatte.\nGrundlagen\nZur Verfügung stehen:\ndie stündlichen, richtungsgetrennten Zählungen von Fussgänger:innen und Velos an drei Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung\n\n\n\n",
    "preview": "fallstudien/BE_S_1_Einleitung/Perimeter.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 2001,
    "preview_height": 1051
  },
  {
    "path": "fallstudien/BE_S_2_Felderhebung_Loesung/",
    "title": "KW42: Loesung Felderhebung Grüntal",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-18",
    "categories": [
      "Biodiversity & Ecosystems (S) Musterloesung"
    ],
    "contents": "\n\n\n\n\n\n#.###############################################################################################\n# Besuchermonitoring Grüntal - Auswertung der Besucherzahlen ####\n# Modul Research Methods, HS20. Adrian Hochreutener ####\n#.################################################################################################\n#.##############################################################################################\n# METADATA UND DEFINITIONEN ####\n#.################################################################################################\n# Ordnerstruktur ####\n# Im Ordner in dem das R-Projekt abgelegt ist muessen folgende Unterordner bestehen:\n# - Skripts\n# - Felderhebungen (Rohdaten hier ablegen)\n# - Results\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(data.table)# schnelles Dateneinlesen\n\n#.###############################################################################################\n# 1. DATENIMPORT #####\n#.###############################################################################################\n\n# 1.1 Einlesen ####\n# lese die Daten mithilfe von data.table ein. Je nach Bedarf muss der Speicherort sowie der\n# Dateiname angepasst werden\ncam <- fread(\"DummyData.csv\")\n\n\n\n\n\n#.###############################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.###############################################################################################\n\n# 2.1 erstes Sichten und anpassen der Datentypen ####\nstr(cam)\n\n\nClasses 'data.table' and 'data.frame':  100 obs. of  11 variables:\n $ Person_Auswertung: chr  \"Bsp\" \"Bsp\" \"Bsp\" \"Bsp\" ...\n $ Kamerastandort   : chr  \"ost\" \"ost\" \"ost\" \"ost\" ...\n $ ID_Foto          : chr  \"bsp_001\" \"bsp_002\" \"bsp_003\" \"bsp_004\" ...\n $ Datum            : chr  \"08.10.2020\" \"07.10.2020\" \"06.10.2020\" \"09.10.2020\" ...\n $ Stunde           : int  3 0 16 24 17 23 14 7 5 23 ...\n $ Art              : chr  \"Mensch\" \"Wildtier\" \"Wildtier\" \"Wildtier\" ...\n $ Anzahl           : int  3 4 1 6 7 10 2 6 9 6 ...\n $ Richtung         : chr  \"Bergauf\" \"Bergauf\" \"Bergauf\" \"Bergab\" ...\n $ Aktivitaet       : chr  \"Spaziergaenger\" \"0\" \"0\" \"0\" ...\n $ Begleittier      : chr  \"Hund_ohne_Leine\" \"0\" \"0\" \"0\" ...\n $ Wildtier         : chr  \"0\" \"Hase\" \"Hase\" \"Hase\" ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\nhead(cam)\n\n\n   Person_Auswertung Kamerastandort ID_Foto      Datum Stunde\n1:               Bsp            ost bsp_001 08.10.2020      3\n2:               Bsp            ost bsp_002 07.10.2020      0\n3:               Bsp            ost bsp_003 06.10.2020     16\n4:               Bsp            ost bsp_004 09.10.2020     24\n5:               Bsp            ost bsp_005 07.10.2020     17\n6:               Bsp            ost bsp_006 12.10.2020     23\n        Art Anzahl Richtung     Aktivitaet     Begleittier Wildtier\n1:   Mensch      3  Bergauf Spaziergaenger Hund_ohne_Leine        0\n2: Wildtier      4  Bergauf              0               0     Hase\n3: Wildtier      1  Bergauf              0               0     Hase\n4: Wildtier      6   Bergab              0               0     Hase\n5: Wildtier      7  Bergauf              0               0    Fuchs\n6:   Mensch     10  Bergauf   Unbestimmbar         Anderes        0\ncam <- cam %>% \n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\"))%>%\n  mutate(Kamerastandort = factor(Kamerastandort))%>%\n  mutate(Art = factor(Art))%>%\n  mutate(Richtung = factor(Richtung))%>%\n  mutate(Aktivität = factor(Aktivitaet))%>%\n  mutate(Begleittier = factor(Begleittier))%>%\n  mutate(Wildtier = factor(Wildtier))\n\n# Datensatz trennen ####\n# Kamera ost und West sind noch in einem Datensatz.\n# Wir betrachten jeden Standort einzeln --> trennen aufgrund Name Standort\nost <- filter(cam, Kamerastandort == \"ost\")\nwest <- filter(cam, Kamerastandort == \"west\")\n\n# 2.3 Verteilung pruefen ####\n# mittels Histogram \n# bei explorativen Analysen macht es immer Sinn sich die Verteilung der Daten anzuschauen\nhist(ost$Anzahl[ost$Art==\"Mensch\" &# wir sind vorerst nur an den Menschen interessiert\n                  !ost$Anzahl==0], # hier schliesse ich die Nuller aus der Visualisierung aus\n     breaks = 10) \n\nhist(west$Anzahl[west$Art==\"Mensch\" &\n                  !west$Anzahl==0], \n     breaks = 10) \n\n\n\n# 2.4 mittels Scatterplot ####\nplot(x=ost$Datum[ost$Art==\"Mensch\" &\n                   !ost$Anzahl==0], \n     y=ost$Anzahl[ost$Art==\"Mensch\" &\n                    !ost$Anzahl==0], \n     xlab = \"Datum\")\n\n\n\nplot(x=west$Datum[west$Art==\"Mensch\" &\n                   !west$Anzahl==0], \n     y=west$Anzahl[west$Art==\"Mensch\" &\n                    !west$Anzahl==0], \n     xlab = \"Datum\")\n\n\n\n# Filter ####\n# fuer die weiteren Analysen schliessen wir die Wildtiere komplett aus\nost <- filter(ost, Art == \"Mensch\")\nwest <- filter(west, Art == \"Mensch\")\n\n# Dennoch wolle wir auch wissen, welche Tiere auf dem Areal unterwegs sind\n# Dafuer gibts einen separaten Datensatz\nTiere <- filter(cam, cam$Art == \"Wildtier\")\n\n\n\n\n\n#.##############################################################################################\n# 3. ANALYSE #####\n#.###############################################################################################\n\n# Fuer die Analyse Eigenschaften Datensatz anschauen\nsummary(ost)\n\n\n Person_Auswertung  Kamerastandort   ID_Foto         \n Length:26          ost :26        Length:26         \n Class :character   west: 0        Class :character  \n Mode  :character                  Mode  :character  \n                                                     \n                                                     \n                                                     \n     Datum                Stunde            Art         Anzahl      \n Min.   :2020-10-06   Min.   : 0.00   Mensch  :26   Min.   : 1.000  \n 1st Qu.:2020-10-07   1st Qu.: 6.25   Wildtier: 0   1st Qu.: 3.000  \n Median :2020-10-08   Median :14.00                 Median : 6.000  \n Mean   :2020-10-08   Mean   :13.04                 Mean   : 5.308  \n 3rd Qu.:2020-10-10   3rd Qu.:20.75                 3rd Qu.: 6.000  \n Max.   :2020-10-12   Max.   :23.00                 Max.   :10.000  \n         Richtung   Aktivitaet                 Begleittier\n Bergab      : 7   Length:26          0              :8   \n Bergauf     :15   Class :character   Anderes        :6   \n Unbestimmbar: 4   Mode  :character   Hund_angeleint :9   \n                                      Hund_ohne_Leine:3   \n                                                          \n                                                          \n    Wildtier           Aktivität\n 0      :26   0             :0  \n Anderes: 0   Anderes       :8  \n Fuchs  : 0   Biker         :3  \n Hase   : 0   Landwirtschaft:3  \n Reh    : 0   Spaziergaenger:7  \n              Unbestimmbar  :5  \n\nsummary(west)\n\n\n Person_Auswertung  Kamerastandort   ID_Foto         \n Length:28          ost : 0        Length:28         \n Class :character   west:28        Class :character  \n Mode  :character                  Mode  :character  \n                                                     \n                                                     \n                                                     \n     Datum                Stunde            Art         Anzahl     \n Min.   :2020-10-06   Min.   : 1.00   Mensch  :28   Min.   :0.000  \n 1st Qu.:2020-10-07   1st Qu.: 6.75   Wildtier: 0   1st Qu.:1.750  \n Median :2020-10-09   Median :11.00                 Median :3.500  \n Mean   :2020-10-09   Mean   :12.07                 Mean   :4.143  \n 3rd Qu.:2020-10-11   3rd Qu.:16.75                 3rd Qu.:7.250  \n Max.   :2020-10-12   Max.   :24.00                 Max.   :9.000  \n         Richtung   Aktivitaet                 Begleittier\n Bergab      :11   Length:28          0              :5   \n Bergauf     : 8   Class :character   Anderes        :8   \n Unbestimmbar: 9   Mode  :character   Hund_angeleint :8   \n                                      Hund_ohne_Leine:7   \n                                                          \n                                                          \n    Wildtier           Aktivität\n 0      :28   0             :0  \n Anderes: 0   Anderes       :5  \n Fuchs  : 0   Biker         :8  \n Hase   : 0   Landwirtschaft:2  \n Reh    : 0   Spaziergaenger:5  \n              Unbestimmbar  :8  \n# Anzahl Total / standort ####\nAnzahl_Ost <- sum(ost$Anzahl)\nAnzahl_West <- sum(west$Anzahl)\n\n# Meiste Aktivitaet ####\nAkt_ost <- ost %>%\n  group_by(Aktivität)%>%       # Hier sagen wir nach was wir gruppieren \n  summarise (n = sum(Anzahl)) %>%      # und dann sagen wir, dass R zusammenfassen soll und zwar die Anzahl\n  mutate(freq = n / sum(n))%>% # und dann soll und R das prozentuale Verhaeltniss berechnen\n  arrange(desc(n))             # und dann das ganze absteigend sortieren\n\n# das ganze wiederholen wir fuer den zweiten Standort\n# Jetzt kommt die grosse Staerke von R. \n# Wir haben den Code zur Berechnung der Anzahl pro Gruppe bereits geschrieben.\n# fuer die folgenden Auswertungen koennen wir ihn einfach \"recyceln\"\n Akt_west<- west %>%\n  group_by(Aktivität)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\nAktivitaet_West <- west %>%\n  group_by(Aktivität)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\n# Begleittier\nBegleit_ost <- ost %>%\n  group_by(Begleittier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n# uns interessiert es nicht, wie viele Leute kein Begleittier dabei hatten\nBegleit_ost <- filter(Begleit_ost, !Begleittier == \"0\")\n\nBegleit_west <- west %>%\n  group_by(Begleittier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\nBegleit_west <- filter(Begleit_west, !Begleittier == \"0\")\n\n# Wildtier\nWildtier <- Tiere %>%\n  group_by(Wildtier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\n\n\n#.###############################################################################################\n# 4. VISUALISIERUNG #####\n#.###############################################################################################\n\n# Verteilung der Aktivitaeten als Pie Chart ####\n# Zuerst eine Palette mit 5 Farben definieren\npal <- hcl.colors(5, palette = \"heat\")\n# Dann als Kreisdiagramm plotten\npie(Akt_west$n, labels = c(\"Anderes\", \"Biker\", \"Landwirtschaft\", \"Spaziergaenger\", \"unbestimmbar\"),\n    main = \"Prozentuales Verhaeltnis West\",\n    col = pal) \n\n\n\n# Begleittier als Bar Chart ####\nggplot(Begleit_ost,                      # hier den Datensatz spezifizieren\n       mapping=aes(x=Begleittier, y = n))+ # Absolute Anzahl darstellen\n  geom_col(width=0.9,position = \"dodge\")+# hier sage ich, dass ich ein Balkendiagramm will\n  labs(x=\"Begleittier\", y= \"Anzahl\")+    # Achsenbeschriftung setzen\n  theme_classic(base_size = 15)+         # Und zu guter letzt: Stil definieren\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # sowie Achsenbeschr.ausrichten\n\n\n\n# Wildtier als Bar Chart ####\nggplot(Wildtier, mapping=aes(x=Wildtier, y=freq*100))+ # kann auch prozentual dargestellt werden\n  geom_col(width=0.9,position = \"dodge\")+\n  labs(x=\"Begleittier\", y= \"Prozent [%]\")+\n  theme_classic(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n# Bei Bedraf koennen die selben Plots fuer den zweiten Standort gemacht werden.\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_S_2_Felderhebung_Loesung/Musterloesung_Felderhebung_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_S_2_Felderhebung/",
    "title": "KW40 + KW42: Aufgabe Felderhebung Grüntal",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nEinführung und Installation\nEs gibt eine Vielzahl an möglichen Methoden zur Erfassung der Besucherzahlen. Automatische Zählgeräte bieten die Möglichkeit lange und durchgehende Zeitreihen zu erfassen. Diese müssen aber natürlich auch ausgewertet werden. Hier erhaltet ihr erste Inputs dazu.\n\n\n\nZiele\nDie Studierenden können das eingesetzte Gerät installieren und kennen die Vor- und Nachteile verschiedener Methoden.\nSie können die Daten auslesen und explorativ analysieren.\nGrundlagen\nDie Geräte werden innerhalb der auf Abbildung 1 gekennzeichneten Standorte platziert. Damit soll überprüft werden, wie stark frequentiert die Waldränder der ökologisch aufgewerteten Seeparzelle sind.\n\n\n\nDatenschutz ist ein wichtiges Thema. Die Besuchenden werden über den Zweck der Kameras informiert, die Daten nach der Bearbeitung wieder gelöscht und nicht weitergegeben.\n\n\n\nNun geht es ins Feld uns die Geräte werden installiert.\nAuswertung\nAUFGABE ab dem 12.10.2021\nNachdem die Kameras für zwei Wochen im Einsatz standen, werden die Daten ausgelesen, die Sichtungen in Excel festgehalten und die explorativen Analysen durchgeführt.\nBereits beim Detektieren der Sichtungen muss einem klar sein, was man auswerten möchte. Nur so können die relevanten Variablen erfasst werden.\nIm Rahmen dieser Felderhebung erhaltet ihr von Adrian eine Excel-Vorlage zur Verifizeriung der automatisch detektierten Sichtungen.\nVerifiziert kurz, was euch der Algorithmus geliefert hat.\nAls nächstes geht es ins R. Da wir für unsere Auswertungen zu wenige Sichtungen haben, verwendet bitte den Datensatz DummyData (ReMe HS21 MSc ENR_/Fallstudie Biodiversity & Ecosystems/S_Daten/Felderhebungen). Eure Verifizierung braucht ihr nicht mehr.\nDatenanalyse in R\nVorbereitungen\nFuer diese Aufgabe benoetigen wir folgende Bibliotheken:\n\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(data.table)# schnelles Dateneinlesen\n\n\n\nLese nun zuerst den bereitsgestellen, respektiven den selbst erstellten Datensatz (csv) mithilfe von fread() oder read.csv() ein und nennt ihn cam.\nPruefe die Daten. Wurden sie richtig eingelesen? Wie sieht die Struktur der Daten aus?\nTipp: Brauch zum pruefen den Befehl str() sowie head().\nAufgabe 1: Datentypen\nViele Befehle zum Einlesen erkennen die Datentypen automatisch. Bei Faktoren funktioniert das aber nicht (sie sind ja eigentlich einfach Text und R weiss nicht, was wir damit wollen).\nAuch das Datum muss vielfach manuell definiert werden (hier muessen wir R sagen, wie das Format dieses aussieht).\n\n\ncam <- cam %>% \n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\"))%>%\n  mutate(Kamerastandort = factor(Kamerastandort))%>%\n  ...\n\n\n\nDefiniert nun die restlichen (relevanten) Variablen als Faktor.\nAufgabe 2: Datensatz trennen\nUnser Datensatz enthaellt die Angeben zu ost und west. Wir wollen die Auswertungen aber pro Standort machen.\nTrennt den Datensatz aufgrund des Standorts. Nutzt dazu filter().\n\n\nost <- filter(DATENSATZ, SPALTENNAME == \"Attribut\")\nwest <- ...\n\n\n\nAufgabe 3: Verteilung pruefen\nBei explorativen Analysen macht es immer Sinn sich die Verteilung der Daten anzuschauen. Pruefe daher die Verteilung pro Datensatz mittels Histogram und Scatterplot.\nBeim Histogram sollen nur die Menschen angezeigt und die 0er ausgeschlossen werden. Das kann mit folgendem Code erreicht werden:\n\n\nhist(west$Anzahl[west$Art==\"Mensch\" &\n                  !west$Anzahl==0], # das \"!\" bedeutet \"nicht gleich\"\n     breaks = 10)                   # wie viele Balken brauchen wir im Histogram?\n\n\n\nBeim Scatterplot soll auf der x-Achse das Datum stehen, auf der y-Achse die Anzahl der Personen. Auch hier wollen wir keine Wildtiere im Plot.\nAufgabe 3: Daten ausschliessen\nFuer die weiteren Analysen schliessen wir die Wildtiere komplett aus.\nNutzt dazu wiederum den Befehl filter() und ueberschreibt die Datensaetze ost und west.\nDennoch wolle wir auch wissen, welche Tiere auf dem Areal (ost und west zusammen, also df cam) unterwegs sind.\nDafuer gibts einen separaten Datensatz namens Tiere. Nutzt dazu den Befehl filter().\nAufgabe 4: Explorative Analysen\nBerechnet zuerst die totale Anzahl Menschen / Standort mit sum(DATENSATZ$SPALTENNAME).\nGruppieren und summieren:\nBerechnet nun die Anzahl Menschen pro Aktivität und Standort (= Akt_ost und Akt_west).\nBerechnet auch die Anzahl Begleittier pro Kategorie und Standort (= Begleittier_ost und Begleittier_west).\nUntenstehender Code eigent sich dazu ganz gut:\n\n\nAkt_ost <- ost %>%\n  group_by(Aktivitaet)%>%      # Hier sagen wir nach was wir gruppieren \n  summarise (n = sum(Anzahl)) %>%      # und dann sagen wir, dass R zusammenfassen soll und zwar die Anzahl\n  mutate(freq = n / sum(n))%>% # und dann soll und R das prozentuale Verhaeltniss berechnen\n  arrange(desc(n))             # und dann das ganze absteigend sortieren\n\n\n\nNun soll noch berechnet werden, wie viele unterschiedliche Wildtiere auf dem ganzen Areal gezaehlt wurden.\nRecycelt dazu obenstenenden Code.\nAufgabe 5: Visualisieren\nVerteilung der Aktivitäten als Pie Chart\nZuerst eine Palette mit 5 Farben definieren:\n\n\npal <- hcl.colors(5, palette = \"heat\")\n\n\n\nDann als Kreisdiagramm plotten.\n\n\npie(Akt_west$n, labels = c(\"Anderes\", \"Biker\", \"Landwirtschaft\", \"Spaziergaenger\", \"unbestimmbar\"),\n    main = \"Prozentuales Verhaeltnis West\",\n    col = pal) \n\n\n\n\nHinweis: Die labels im base R plot müssen manuell definiert werden. ggplot als Alternative macht das selbst.\nFRAGE: eignen sich Pie Charts überhaupt für solche Darstellungen? Wie könnten die Aktivitäten auch noch dargestellt werden? Welches sind eure eigenen Ideen zur Visualisierung?\nBegleittier als Bar Chart\n\n\n# Begleittier als Bar Chart ####\nggplot(Begleit_ost,                      # hier den Datensatz spezifizieren\n       mapping=aes(x=Begleittier, y = n))+ # Absolute Anzahl darstellen\n  geom_col(width=0.9,position = \"dodge\")+# hier sage ich, dass ich ein Balkendiagramm will\n  labs(x=\"Begleittier\", y= \"Anzahl\")+    # Achsenbeschriftung setzen\n  theme_classic(base_size = 15)+         # Und zu guter letzt: Stil definieren\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # sowie Achsenbeschr.ausrichten\n\n\n\n\nUnd schliesslich: Wildtier als Bar Chart Stellt hier auf der y-Achse die Anzahl nicht total sondern relativ (in Prozent) dar.\n\n\n\nBei Bedraf koennen die selben Plots fuer den zweiten Standort gemacht werden.\n\n\n\n",
    "preview": "fallstudien/BE_S_2_Felderhebung/gruental.jpg",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "fallstudien/BE_S_3_Aufgabenstellung/",
    "title": "KW 43: Aufgabenstellung Wildnispark",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nAbschlussbericht über die multivariate Analyse\nZiele\nBisher habt ihr euch mit dem Untersuchungsgebiet beschäftigt und habt selbst ein (kleines) Besuchermonitoring durchgeführt. Das Besuchermonitoring Grüental ist nun abgeschlossen und wir beschäftigen uns voll und ganz mit dem Wildnispark Zürich.\nIm Rahmen dessen programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren (Lockdown, Wetter, Ferien, Wochentag, Kalenderwoche) beschreiben. Dank ihnen können wir sagen, wie die Besucher:innen auf die untersuchten Faktoren reagieren (siehe dazu [Einleitung], Ziele).\nKonkret sollen folgende Fragestellungen beantwortet werden:\n\nWelchen Einfluss haben neben den Lockdowns auch die Wetterparameter (Sonnenscheindauer, Tageshöchsttemperatur, Niederschlagssumme) sowie der Wochentag, die Ferien, die Kalenderwoche und das Jahr auf die Besuchszahlen?\nDabei interessiert uns besonders, wie stark die jeweiligen Einflüsse sind, welche Effektrichtungen beobachtbar sind und welche der untersuchten Parameter signifikant sind.\nKönnen deutliche Unterschiede zwischen den “normalen”, vor-Covid19-Jahren und danach bei Tages-, Wochen-, und / oder Saisongang sowie den wichtigsten, deskriptiven Kennzahlen gefunden werden?\n\nJede Gruppe wertet ausschliesslich Daten eines Zählers aus. Sprecht miteinander ab, wer welchen Zähler behandelt (204, 211 oder 502; Spezifikationen siehe [Einleitung], Hinweis). Jeder Zähler soll nur von einer Gruppe ausgewertet werden!\nFür euren Zähler stehen eventuell Zahlen zu Fussgänger:innen und Velos zur Verfügung (siehe [Einleitung], Hinweis). Entscheidet euch in diesem Fall selbst, ob ihr Fussgänger:innen ODER Velos auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüntal erwarten wir keine Angaben.\n\n\n\nErwartungen\nStruktur / Aufbau\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber natürlich für den Bericht geschärft und optimal formuliert und konkretisiert werden.)\nMethoden (kurzes Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir Euch eine Auswahl relevanter Papers bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\nGesamtumfang max. 7500 Zeichen (inkl. Leerzeichen; exkl. Einleitung, Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 9.1.2022 auf Moodle oder per Mail an hoce@zhaw.ch\nBewertungskriterien\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)\nZusammensetzung der Fallstudiennote:\nFallstudie-Leistungsnachweis 1 - Forschungsplan: 30%\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 70%\n\n\n\n",
    "preview": "fallstudien/BE_S_3_Aufgabenstellung/Aufbau_Fallstudie_Profil_S.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1429,
    "preview_height": 903
  },
  {
    "path": "fallstudien/BE_S_4_Projektierung/",
    "title": "KW 41: Aufgabe R Projekt vorbereiten",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nArbeiten mit Projekten\nVor den eigentlichen Auswertungen muessen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein Mehrfaches eingespart.\n\n\n\nIch empfehle generell mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –> Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).\nErstellt an einem passenden Speicherort ein neues Projekt mit einem treffenden Namen:\n–> File / New Project\nAufgabe 1: Projektaufbau\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, müssen vor dem Wort # und nach dem Wort #### eingefügt werden.\n\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Titel\n# Texte, vor denen ein # steht, erklaeren den Ablauf\n# Zeilen ohne vorangehendes # sind Operationen\n\n# Wenn man rechts neben \"Source\" und links neben \"Environment\" klickt \n# (oder CTRL + SHIFT + O --> Show document Outline), \n# oeffnet sich die UEbersicht zu den UEberschriften\n\n\n\nTipp:\nAlt + - = <-\nCtrl + Shift + M = %>%\nCtrl + Shift + C = # vor der ausgewaehlten Zeile\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (Wildnispark und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektornder mit den Unterordnern __skripts, data, results).\nBeschreibt zudem folgendes die verwendete Meteodaten (siehe dazu Metadata Meteodaten, –> order_XXX_legend.txt)\nEin Skript soll in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthällt (nach den bereits erwähnten Definitionen) 4 Kapitel:\nMetadaten und Definitionen\nDatenimport,\nVorbereitung,\nDeskriptive Analyse und Visualisierung und\nMultifaktorielle Analyse und Visualisierung.\nBereitet euer Sktipt mit diesen Kapitel vor.\n\n\n#.###########################################################################################\n# Einfluss von COVID19 auf das Naherholungsverhalten in WPZ ####\n# Fallstudie Modul Research Methods, HS21. Autor/in ####\n#.##########################################################################################\n\n#.##########################################################################################\n# METADATA UND DEFINITIONEN ####\n#.##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n#.##########################################################################################\n# 1. DATENIMPORT #####\n#.##########################################################################################\n\n\n\nAufgabe 2: Laden der Bibliotheken\nGeplottet wird mit ggplot, daher wird tidyverse geladen. Diese Bibliothek ergaenzt BASE R in vielerlei Hinsicht uns ist eigentlich fast immer nötig. Da wir es bei Besucherdaten immer mit einem zeitlichen Bezug zu tun haben, benoetigen wir eine passende Bibliothek. Ich arbeite mit lubridate, POSIXct waere natuerlich auch moeglich. Base R bietet verschiedene Funktionen um Daten einzulesen. data.table ergaenzt diese Basisfunktionen sehr gut. ggpubr brauchen wir für das Darstellen von mehreren verschiedenen Plots in nur einem. PerformanceAnalytics, MuMIn, AICcmodavg, fitdistrplus, lme4 und sjPlot werden fuer die spaeteren multivariaten Analysen benoetigt. Die Modellguete werden wir mittels lattice, blmeco und lattice pruefen.\nLadet nun also die benoetigten Bibliotheken.\nAllenfalls muessen diese zuerst mit install.packages(“NAME”) installiert werden.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(data.table)# schnelles Dateneinlesen\nlibrary(ggpubr)    # to arrange multiple plots in one graph\nlibrary(PerformanceAnalytics) # Plotte Korrelationsmatrix\nlibrary(MuMIn)     # Multi-Model Inference\nlibrary(AICcmodavg)# Modellaverageing\nlibrary(fitdistrplus)# Prueft die Verteilung in Daten\nlibrary(lme4)      # Multivariate Modelle\nlibrary(blmeco)    # Bayesian data analysis using linear models\nlibrary(sjPlot)    # Plotten von Modellergebnissen (tab_model)\nlibrary(lattice)   # einfaches plotten von Zusammenhängen zwischen Variablen\n\n\n\nAufgabe 3: Zeitliche Definitionen\nDefiniert den zeitlichen Horizont, also Start sowie Ende der Untersuchungen. Bezieht in eure Auswertungen den gesamten verfügbaren Zeitraum ein.\nDafür müsst ihr in die Rohdaten eures Zählers schauen. Am einfachsten direkt in der .csv Datei.\n\n\ndepo_start <- as.Date(\"YYYY-MM-DD\")\ndepo_end <- ...\n\n\n\nWichtiger Teil unserer Auswertungen ist der Einfluss des Lockdown auf das Besuchsverhalten. -Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\n\nlock_1_start_2020 <- as.Date(\"2020-03-16\")\nlock_1_end_2020 <- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 <- as.Date(\"2020-12-22\")\nlock_2_end_2021 <- as.Date(\"2021-03-01\")\n\n\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollstaendig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl week() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume (in meinem Beispiel ab 2019, je nach dem müsst ihr das anpassen) muüsen daher bekannt sein. Zur Definition der Ferien kann z.B. folgend vorgegangen werden:\n\n\n# (https://www.schulferien.org/schweiz/ferien/2020/)\nFruehlingsferien_2019_start <- as.Date(\"2019-04-13\")\nFruehlingsferien_2019_ende <- as.Date(\"2019-04-28\")\nSommerferien_2019_start <- as.Date(\"2019-07-6\")\nSommerferien_2019_ende <- as.Date(\"2019-08-18\")\nHerbstferien_2019_start <- as.Date(\"2019-10-05\")\nHerbstferien_2019_ende <- as.Date(\"2019-10-20\")\nWinterferien_2019_start <- as.Date(\"2019-12-21\")\nWinterferien_2019_ende <- as.Date(\"2020-01-02\")\n\nFruehlingsferien_2020_start <- as.Date(\"2020-04-11\")\nFruehlingsferien_2020_ende <- as.Date(\"2020-04-26\")\nSommerferien_2020_start <- as.Date(\"2020-07-11\")\nSommerferien_2020_ende <- as.Date(\"2020-08-16\")\nHerbstferien_2020_start <- as.Date(\"2020-10-03\")\nHerbstferien_2020_ende <- as.Date(\"2020-10-18\")\nWinterferien_2020_start <- as.Date(\"2020-12-19\")\nWinterferien_2020_ende <- as.Date(\"2021-01-03\")\n\nFruehlingsferien_2021_start <- as.Date(\"2021-04-24\")\nFruehlingsferien_2021_ende <- as.Date(\"2021-05-09\")\nSommerferien_2021_start <- as.Date(\"2021-07-17\")\n\n\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann beginnen.\n\n\n\n",
    "preview": "fallstudien/BE_S_4_Projektierung/the-r-project-for-statistical-computing.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 797,
    "preview_height": 298
  },
  {
    "path": "fallstudien/BE_S_5_Import_Vorverabeitung/",
    "title": "KW 43: Aufgabe Import und Datenvorverarbeitung",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-26",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nAufgabe 1: Zähldaten\nDie Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden. Das tidyverse-Universum (u.a. pipes, ggplot usw.) ist in unseren Auswertungen zentral.\n\n\n\nDie Zähldaten des Wildnispark Zürich wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\ndepo <- read_csv(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\") \n# Speicherort sowie Dateiname anpassen\n\n\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr.\n1a)\nIm Datensatz des Wildnisparks sind Datum und Uhrzeit in einer Spalte. Diese müssen getrennt werden (Ich schlage hier den Ansatz des piping ( %>% ) vor. Damit können in einem “Rutsch” mehrere Operationen ausgeführt werden).\nEbenfalls muss das Datum als solches definiert werden. Welches Format hat es (im Code: format = “HIER DATUMSFORMAT”)?\nSchliesslich schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n\n\nstr(depo)\n\ndepo <- depo %>%\n  mutate(Datum_Uhrzeit = as.character(DatumUhrzeit)) %>%\n  separate(Datum_Uhrzeit, into = c(\"Datum\", \"Zeit\"), sep = \" \")%>% # mit seperate() trennt man\n                                                                   # 1 Spalte in 2.\n  mutate(Datum = as.Date(Datum, format = \"HIER DATUMSFORMAT\")) %>% # hier wird Text zum Datum\n  # Schneide das df auf den gewünschten Zeitraum zu\n  filter(Datum >= depo_start, Datum <=  depo_end) # das Komma hat die gleiche Funktion wie ein &\n\n\n\n1b)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Velos untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\nEntfernt die überflüssigen Spalten aus dem Datensatz.\n1c)\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist (wiederum mit piping).\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\nEntfernt nun alle NA-Werte mit na.omit().\nAufgabe 2: Meteodaten\n2a)\nLest die Meteodaten ein und speichert sie unter meteo.\n2b)\nAuch hier müssen die Datentypen manuell gesetzt werden.\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\n\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n\n\nHinweis Was ist eigentlich Niederschlag:\nhttps://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n2c)\nJetzt müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden.\nTipp: Entweder geht das mit na.omit() für alle Spalten oder, etwas konservativer, können mit filter() die zu filternden Spalten definiert werden. Mit folgendem Codeblock können z.B. alle Werte gefiltert werden, die in der Spalte stn nicht gleich NA sind (es werden also die Werte behalten, die vorhanden sind). Der Code muss für die anderen relevanten Spalten noch ergänzt werden.\n\n\nmeteo <- meteo %>%\n  filter(!is.na(stn))%>%\n  ...%>%\n  ...\n\n\n\nHinweis: … steht im Code für folgende oder vorhergehende Zeilen im Code (in einer Pipe)\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden (sum(is.na(df$Variable))). Stimmen alle Datentypen?\nAufgabe 3: Datenvorverarbeitung (Mutationen)\n3a)\nJetzt fügen wir viele Convinience Variabeln hinzu. Wir brauchen:\nWochentag; der Befehl dazu ist weekdays()\nTipp: R sortiert die Levels alphabetisch. Da das in unserem Fall aber sehr unpraktisch ist, müssen die Levels manuell bestimmt werden\n\n\n  ...\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\")))\n  ...\n\n\n\nFrage: Was bedeutet base:: vor den eigentlichen Befehl?\nIst es ein Werktag oder Wochenende?\n\n\n  ...\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))\n  ...\n\n\n\nFrage: Was bedeuten die | (zu erstellen mit AltGr + 7)? Welches ist das if Argument, welches das else?\nKalenderwoche: week()\nMonat: month()\nJahr: year()\nPhase Covid (Code untenstehend)\nHinweis I: ich mache den letzten Punkt nachgelagert, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können. Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown) in eine Spalte –> long-format ist schöner (und praktischer für das plotten) als wide-format.\n\n\ndepo <- depo %>%\nmutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                          \"Lockdown_1\",\n                       if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                               \"Lockdown_2\",\n                               if_else(Datum < lock_1_start_2020,\n                                  \"Normal\", \"Covid\"))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\n\n\nFrage: Welches ist das if Argument, welches das else?\nÄndert die Datentypen der Spalten Wochenende, KW, Phase zu factor.\n3b)\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Diese Angabe muss etwas mühsam aus den Daten gezogen werden (darum hier der fertige Code dazu):\n\n\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M\"),\"%H\"))\n\n\n\n3c)\nDie Daten wurden durch den WPZ kalibriert (Kommastellen).\nRundet sie auf 0 Nachkommastellen (Ganzzahl; unser Modell kann nicht mit Kommazahlen in der ahbängigen Variable umgehen).\nDefiniert sie sicherheitshalber als Integer\nMacht das für IN, OUT und Total.\n\n\ndepo$... <- round(..., digits = 0)\ndepo$... <- as.integer(...)\n\n\n\nAufgabe 4: Aggregierung der Stundendaten\n4a)\nUnsere Daten liegen im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen können.\nDie Stundendaten müssen zu ganzen Tagen aggregiert werden. Macht das wiederum einer Pipe Bezieht folgende Gruppierungen (group_by()) mit ein: Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase. Summiert die Zählmengen separat (Total, IN, OUT) auf und speichert das Resultat unter depo_d.\nTipp: Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in das neue df übernommen und müssen nicht nochmals hinzugefügt werden\n\ndepo_d <- depo %>% \n  group_by(VARIABLE1, VARIABLE2, ...) %>%   # Gruppieren nach den Variablen\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n4b)\nAggregiere die Stundenwerte nach dem Monat (Gruppierungen Monat, Jahr) und speichert das neue df unter depo_m.\nTipp: Braucht wiederum group_by() und summarise(). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr.\nFügt den neu erstellten df eine Spalte mit Jahr + Monat hinzu. Das ist etwas mühsam, darum hier der fertige Code dazu:\n\n\n# vergewissere, dass sicher df\ndepo_m <- as.data.frame(depo_m)\n# sortiere das df anhand zwei Spalten aufsteigend (damit die Reihenfolge sicher stimmt)\ndepo_m[with(depo_m, order(Jahr, Monat)),]\n\ndepo_m <- depo_m %>% \n  mutate(Jahr = as.factor(Jahr)) %>% # mache dann aus Jahr und Monat Faktoren\n  mutate(Monat = as.factor(Monat)) %>% \n  mutate(Ym = paste(Jahr, Monat)) %>% # und mache eine neue Spalte, in der Jahr und Monat in zusammen sind\n  mutate(Ym= factor(Ym, levels=unique(Ym))) # auch dass soll ein Faktor sein, \n        # die Levels sind die einzelnen Einträge in der Spalte (welche ja bereits geordnet sind)\n\n\n\n4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n–> Erklärt dem Plenum am 26.10.2021 was ihr gemacht habt, was eure Daten zeigen und präsentiert diese einfachen Plots. \nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die Analyse.\n\n\n\n",
    "preview": "fallstudien/BE_S_5_Import_Vorverabeitung/tidyverse.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 718,
    "preview_height": 424
  },
  {
    "path": "fallstudien/BE_S_5_Import_Vorverarbeitung_Loesung/",
    "title": "KW 43: Loesung Import und Datenvorverarbeitung",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-10-27",
    "categories": [
      "Biodiversity & Ecosystems (S) Musterloesung"
    ],
    "contents": "\n\n\n\nAufgabe 1: Zähldaten\n\n\n#.################################################################################################\n# 1. DATENIMPORT #####\n#.################################################################################################\n\n# Beim Daten einlesen koennen sogleich die Datentypen und erste Bereinigungen vorgenommen werden\n\n# 1.1 Zaehldaten ####\n# Die Zaehldaten des Wildnispark wurden vorgaengig bereinigt. z.B. wurden Stundenwerte \n# entfernt, an denen am Zaehler Wartungsarbeiten stattgefunden haben.\n\n# lese die Daten mithilfe der Bibliothek data.table ein (alternative zu read_csv und dergleichen). \n# Je nach Bedarf muss der Speicherort sowie der Dateiname angepasst werden\ndepo <- fread(\"211_sihlwaldstrasse_2017_2021.csv\")\n\n# Hinweis zu den Daten:\n# In hourly analysis format, the data at 11:00 am corresponds to the counts saved between \n# 11:00 am and 12:00 am.\n\n# Anpassen der Datentypen und erstes Sichten\nstr(depo)\n\ndepo <- depo %>%\n  mutate(Datum_Uhrzeit = as.character(DatumUhrzeit)) %>%\n  separate(Datum_Uhrzeit, into = c(\"Datum\", \"Zeit\"), sep = \" \")%>%\n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\")) %>% \n  # Schneide das df auf den gewuenschten Zeitraum zu\n  filter(Datum >= depo_start, Datum <=  depo_end) # das Komma hat die gleiche Funktion wie ein &\n\n# In dieser Auswertung werden nur Velos betrachtet!\ndepo <- depo[,-c(1,4,5), drop=FALSE] # mit diesem Befehl lassen wir Spalten \"fallen\", \n                                     # aendern aber nichts an der Form des data.frames\n\n# Berechnen des Totals, da dieses in den Daten nicht vorhanden ist\ndepo <- depo%>%\n  mutate(Total = Fuss_IN + Fuss_OUT)\n\n# Entferne die NA's in dem df.\ndepo <- na.omit(depo)\n\n\n\nAufgabe 2: Meteodaten\n\n\n# 1.2 Meteodaten ####\n# Einlesen\nmeteo <- fread(\"order_97149_data.txt\")\n\n# Datentypen setzen\n# Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewaldelt werden aus dem dann\n# das eigentliche Datum herausgelesen werden kann\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo <- meteo%>%\n  mutate(tre200jx = as.numeric(tre200jx))%>%\n  mutate(rre150j0 = as.numeric(rre150j0))%>%\n  mutate(sremaxdv = as.numeric(sremaxdv)) %>% \n  filter(time >= depo_start, time <=  depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo <- meteo %>%\n  filter(!is.na(stn)) %>%\n  filter(!is.na(time))%>%\n  filter(!is.na(tre200jx))%>%\n  filter(!is.na(rre150j0))%>%\n  filter(!is.na(sremaxdv))\n# Pruefe ob alles funktioniert hat\nstr(meteo)\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an\n\n\n\nAufgabe 3: Datenvorverarbeitung (Mutationen)\n\n\n#.################################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.################################################################################################\n\n# 2.1 Convinience Variablen ####\n# fuege dem Dataframe (df) die Wochentage hinzu\ndepo <- depo %>% \n  mutate(Wochentag = weekdays(Datum)) %>% \n  # R sortiert die Levels aplhabetisch. Da das in unserem Fall aber sehr unpraktisch ist,\n  # muessen die Levels manuell manuell bestimmt werden\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\"))) %>% \n  # Werktag oder Wochenende hinzufuegen\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))%>%\n  #Kalenderwoche hinzufuegen\n  mutate(KW= week(Datum))%>%\n  # monat und Jahr\n  mutate(Monat = month(Datum)) %>% \n  mutate(Jahr = year(Datum))\n\n# Lockdown \n# Hinweis: ich mache das nachgelagert, da ich die Erfahrung hatte, dass zu viele \n# Operationen in einem Schritt auch schon mal durcheinander erzeugen koennen.\n# Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown)\n# in eine Spalte --> long ist schoener als wide\ndepo <- depo %>%\nmutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                          \"Lockdown_1\",\n                       if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                               \"Lockdown_2\",\n                               if_else(Datum < lock_1_start_2020,\n                                  \"Normal\", \"Covid\"))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\n# aendere die Datentypen\ndepo <- depo %>% \n  mutate(Wochenende = as.factor(Wochenende)) %>% \n  mutate(KW = factor(KW)) %>% \n  # mit factor() koennen die levels direkt einfach selbst definiert werden.\n  # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem \n  # mix-up mit anderen packages\n  mutate(Phase = base::factor(Phase, levels = c(\"Normal\", \"Lockdown_1\", \"Lockdown_2\", \"Covid\")))\n\nstr(depo)\n  \n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M\"),\"%H\"))\n\n# Die Daten wurden kalibriert. Wir runden sie fuer unserer Analysen auf Ganzzahlen\ndepo$Total <- round(depo$Total, digits = 0)\ndepo$Fuss_IN <- round(depo$Fuss_IN, digits = 0)\ndepo$Fuss_OUT <- round(depo$Fuss_OUT, digits = 0)\n\n\n\nAufgabe 4: Aggregierung der Stundendaten\n\n\n# 2.3 Aggregierung der Stundendaten zu ganzen Tagen ####\n# Zur Berechnung von Kennwerten ist es hilfreich, wenn neben den Stundendaten auch auf Ganztagesdaten\n# zurueckgegriffen werden kann\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d <- depo %>% \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase) %>% \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n# Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in \n# das neue df uebernommen und muessen nicht nochmals hinzugefuegt werden\n\n# pruefe das df\nhead(depo_d)\n\n# Gruppiere die Werte nach Monat\ndepo_m <- depo %>% \n  group_by(Jahr, Monat) %>% \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m <- as.data.frame(depo_m)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\n# mache dann aus Jahr und Monat faktoren\ndepo_m <- depo_m %>% \n  mutate(Jahr = as.factor(Jahr)) %>% \n  mutate(Monat = as.factor(Monat)) %>% \n  mutate(Ym = paste(Jahr, Monat)) %>% # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= factor(Ym, levels=unique(Ym))) # Monat in zusammen sind\n\n# Beispiele pruefen der Daten:\n\n# Verteilung mittels Histogram pruefen\nhist(depo$Total[!depo$Total==0] , breaks = 100) \n\n\n\n# hier schliesse ich die Nuller aus der Visualisierung aus\n\n# Verteilung mittels Scatterplot pruefen\nplot(x=depo$Datum, y=depo$Total)\n\n\n\n# Temperaturmaximum\nggplot(data=meteo, mapping=aes(x=time, y=tre200jx))+\n  geom_point()+\n  geom_smooth(col=\"red\")\n\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_S_5_Import_Vorverarbeitung_Loesung/Musterloesung_Import_Vorverarbeitung_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_S_6_Deskriptive_Analysen_Loesung/",
    "title": "KW 44: Loesung Deskriptive Analysen",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (S) Musterloesung"
    ],
    "contents": "\nAufgabe 1: Verlauf der Besuchszahlen / m\n\n\n#.################################################################################################\n# 3. DESKRIPTIVE ANALYSE UND VISUALISIERUNG #####\n#.################################################################################################\n\n# 3.1 Verlauf der Besuchszahlen / m ####\n# Monatliche Summen am Standort\n\n# wann beginnt die Datenreihe schon wieder?\nfirst(depo_m$Ym)\n# und wann ist die fertig?\nlast(depo_m$Ym)\n\n# Plotte\nggplot(depo_m, mapping = aes(Ym, Total, group = 1))+ # group = 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n  #zeichne Lockdown 1\n  geom_rect(mapping = aes(xmin=\"2020 3\", xmax=\"2020 5\",\n                          ymin =0, ymax=max(Total+(Total/100*10))),\n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  #zeichne Lockdown 2\n  geom_rect(mapping = aes(xmin=\"2020 12\", xmax=\"2021 3\", \n                          ymin =0, ymax=max(Total+(Total/100*10))), \n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  geom_line(alpha = 0.6, size = 1.5)+\n  scale_x_discrete(breaks = c(\"2019 1\", \"2019 7\",\"2019 1\",\"2020 1\",\"2020 7\",\"2021 1\",\"2021 7\"),\n                   labels = c(\"2019 1\", \"2019 7\",\"2019 1\",\"2020 1\",\"2020 7\",\"2021 1\",\"2021 7\"))+\n  labs(title= \"\", y=\"Fussgaenger:innen pro Monat\", x = \"Jahr\")+\n  theme_linedraw(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n# Beachtet, dass ich in der Musterloesung keine Resultate exportiere, untenstehend aber der Code dazu:\n\n# ggsave(\"Entwicklung_Zaehlstelle.png\", width=20, height=10, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n\n\nAufgabe 2: Wochengang\n\n\n# mean / d / phase\nmean_phase_wd <- depo_d %>% \n  group_by(Wochentag, Phase) %>% \n  summarise(Total = mean(Total))\n\n# write.csv(mean_phase_wd, \"_fallstudien/_R_analysis/results/mean_phase_wd.csv\")\n\n#plot\nggplot(data = depo_d)+\n  geom_boxplot(mapping = aes(x= Wochentag, y = Total, fill = Phase))+\n  labs(title=\"\", y= \"Fussgaenger:innen pro Tag\")+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"))+\n  theme_classic(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n        legend.title = element_blank())\n\n\n\n# ggsave(\"Wochengang_Lockdown.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\")\n\n# Statistik: Unterschied WE und WO waehrend Lockdown 1\nt.test(depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Werktag\"], \n       depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Wochenende\"])\n\n\n\nAufgabe 3: Tagesgang\n\n\n# 3.3 Tagesgang ####\n# Bei diesen Berechnungen wird jeweils der Mittelwert pro Stunde berechnet. \n# wiederum nutzen wir dafuer \"pipes\"\nMean_h <- depo %>% \n  group_by(Wochentag, Stunde, Phase) %>% \n  summarise(Total = mean(Total)) \n\n# Plotte den Tagesgang, unterteilt nach Wochentagen\n\n# Normal\ntag_norm <- ggplot(subset(Mean_h, Phase %in% c(\"Normal\")), \n                   mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5),  \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 1\ntag_lock_1 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_1\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 2\ntag_lock_2 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_2\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Covid\ntag_covid <- ggplot(subset(Mean_h, Phase %in% c(\"Covid\")), \n                    mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Arrange und Export Tagesgang\nggarrange(tag_lock_1+            # plot 1 aufrufen\n            rremove(\"x.text\")+   # plot 1 braucht es nicht alle Achsenbeschriftungen\n            rremove(\"x.title\"),            \n          tag_lock_2+            # plot 2 aufrufen\n            rremove(\"y.text\")+   # bei plot 2 brauchen wir keine Achsenbeschriftung\n            rremove(\"y.title\")+\n            rremove(\"x.text\")+\n            rremove(\"x.title\"),\n          tag_norm,\n          tag_covid+\n            rremove(\"y.text\")+   \n            rremove(\"y.title\"),\n          ncol = 2, nrow = 2,    # definieren, wie die plots angeordnet werden\n          heights = c(0.9, 1),  # beide plots sind wegen der fehlenden Beschriftung nicht gleich hoch\n          widths = c(1,0.9),    \n          labels = c(\"a) Lockdown 1\", \"b) Lockdown 2\", \"c) Normal\", \"d) Covid\"),\n          label.x = 0.1,        # wo stehen die Plottitel\n          label.y = 0.99,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\n\n\n# ggsave(\"Tagesgang.png\", width=25, height=25, units=\"cm\", dpi=1000,\n#        path = \"_fallstudien/_R_analysis/results/\")\n\n\n\nAufgabe 4: Kennzahlen\n\n\n# 3.4 Kennzahlen ####\ntotal_phase <- depo_d %>% \n  # gruppiere nach Phasen inkl. Normal. Diese Levels haben wir bereits definiert\n  group_by(Phase) %>% \n  summarise(Total = sum(Total),\n            IN = sum(Fuss_IN),\n            OUT = sum(Fuss_OUT))\n\n# write.csv(total_phase, \"_fallstudien/_R_analysis/results/total_phase.csv\")\n\n# mean besser Vergleichbar, da Zeitreihen unterschiedlich lange\nmean_phase_d <- depo_d %>% \n  group_by(Phase) %>% \n  summarise(Total = mean(Total),\n            IN = mean(Fuss_IN),\n            OUT = mean(Fuss_OUT))\n# berechne prozentuale Richtungsverteilung\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  mutate(Proz_OUT = round(100/Total*OUT,1))\n\n# write.csv(mean_phase_d, \"_fallstudien/_R_analysis/results/mean_phase_d.csv\")\n\n# selektiere absolute Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_abs <- mean_phase_d %>% dplyr::select(-c(Total, Proz_IN, Proz_OUT))\n\n# transformiere fuer Plotting\nmean_phase_d_abs <- pivot_longer(mean_phase_d_abs, cols = c(\"IN\",\"OUT\"), \n             names_to = \"Gruppe\", values_to = \"Durchschnitt\")\n\n# selektiere relative Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_proz <- mean_phase_d %>% dplyr::select(-c(Total:OUT))\n\n# transformiere fuer Plotting\nmean_phase_d_proz <- pivot_longer(mean_phase_d_proz, cols = c(\"Proz_IN\",\"Proz_OUT\"), \n                                  names_to = \"Gruppe\", values_to = \"Durchschnitt\")\n\n# Visualisierung abs\nabs <- ggplot(data = mean_phase_d_abs, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [mean]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Visualisierung %\nproz <- ggplot(data = mean_phase_d_proz, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [%]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Arrange und Export Verteilung\nggarrange(abs,            # plot 1 aufrufen\n          proz,            # plot 2 aufrufen\n          ncol = 2, nrow = 1,    # definieren, wie die plots angeordnet werden\n          heights = c(1),        # beide sind bleich hoch\n          widths = c(1,0.95),    # plot 2 ist aufgrund der fehlenden y-achsenbesch. etwas schmaler\n          labels = c(\"a) Absolute Verteilung\", \"b) Relative Verteilung\"),\n          label.x = 0,        # wo stehen die labels\n          label.y = 1.0,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\n\n\n# ggsave(\"Verteilung.png\", width=20, height=15, units=\"cm\", dpi=1000,\n#        path = \"_fallstudien/_R_analysis/results/\")\n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_S_6_Deskriptive_Analysen_Loesung/Musterloesung_Desktiptive_Analysen_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_S_6_Deskriptive_Analysen/",
    "title": "KW 44: Aufgabe Deskriptive Analysen",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nAufgabe 1: Verlauf der Besuchszahlen / m\nNachdem wir die Projektstruktur aufgebaut haben und die Daten vorbereitet sind, machen wir uns an die deskriptive Analyse. Dies macht eigentlich immer Sinn. Bevor mach sich an die schliessende Statistik macht, muss man ein “Gefühl” für die Daten bekommen. Dies funktioniert am einfachsten mit deskriptiven, explorativen Analysen.\nWir interessieren uns in den Analysen grundsätzlich für 4 Zeitabschnitte:\nNormal (vom Start der Erhebungen bis vor dem ersten Lockdown)\nLockdown 1\nLockdown 2\nSeit Covid, aber nicht während eines Lockdowns\nWir haben relativ lange Zeitreihen. Zur Visualisierung des generellen Trends greifen wir darum auf die aggregierten Daten zurück - das macht den Plot übersichtlicher und damit aussagekräftiger.\nPlottet den Verlauf der monatlichen Besuchszahlen an eurer Zählstelle. Auf der x-Achse steht dabei dabei Jahr und Monat (gespeichert im df depo_m), auf der y-Achse die monatlichen Besuchszahlen. Zeichnet auch die beiden Lockdown ein (Hinweis: rundet das Start- und Enddatum des Lockdowns auf den Monat, da im Plot die monatlichen Zahlen gezeigt werden).\nHaltet euch dabei an untenstehenden Plot:\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nggplot(data = depo_m, mapping = aes(Ym, Total, group = 1))+ # group 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n  # zeichne Lockdown 1; ein einfaches Rechteck. bestimme mit min und max die Dimensionen\n  geom_rect(mapping = aes(xmin=\"2020 3\", xmax=\"2020 5\",\n                          ymin =0, ymax=max(Total+(Total/100*10))), # das Rechteck soll 10 % grösser als die maximale Besuchszahl sein \n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  # zeichne Lockdown 2\n    ...+\n  # zeichne die Linie\n  geom_line(...)+\n  # bestimme manuell, wo auf der x-Achse die breaks sind und wie diese angeschrieben werden sollen\n  scale_x_discrete(breaks = c(\"...\", \"...\", ...),\n                   labels = c(\"...\", \"...\", ...))+\n  theme_linedraw(base_size = 15)+\n  ...\n\n\n\nExportiert euren Plot mit ggsave() nach results. Breite = 20, Höhe = 10, Einheiten = cm, dpi = 1000\nAufgabe 2: Wochengang\nNachdem wir nun wissen, wie sich die Besuchszahlen während der Untersuchungsdauer monatlich entwickelt haben, möchten wir genauer untersuchen, wie sich die Zahlen je nach Phase (Normal, Lockdown 1. Lockdown 2 und Covid) auf die Wochentage verteilen.\n2a)\nBerechnet zuerst die Totale Anzahl pro Wochentag pro Phase.\n\n\nmean_phase_wd <- depo_d %>% \n  group_by(...) %>% \n  ...\n\n\n\nSpeichert das als .csv\n\n\nwrite.csv(mean_phase_wd, \"results/mean_phase_wd.csv\")\n\n\n\n2b)\nErstellt einen Boxplot nach untenstehender Vorgabe:\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nggplot(data = depo_d)+\n  geom_boxplot(mapping = aes(x= Wochentag, y = Total, fill = Phase))+\n  ...\n\n\n\nExportiert auch diesen Plot mit ggsave(). Welche Breite und Höhe passt hier?\n2c)\nSind die Unterschiede zwischen Werktag und Wochenende wirklich signifikant? Falls ja, in allen Phasen oder nur während bestimmter?\nPrüft das pro Phase mit einem einfachen t.test.\nAufgabe 3: Tagesgang\nVom Grossen zum Kleinen, von der Übersicht ins Detail. Jetzt widmen wir uns dem Tagesgang, das heisst der Verteilung der Besuchenden auf die 24 Tagesstunden je nach Phase.\n3a)\nBerechnet zuerst den Mittelwert der Totalen Besuchszahlen pro Wochentag pro Stunde pro Phase. (ganz ähnlich wie unter 2a) und speichert das df unter Mean_h.\nggplots haben Daten lieber im Format long als wide.\n3b)\nPlottet den Tagesgang, unterteilt nach den 7 Wochentagen nun für unsere 4 Phasen.\n\n\n\nFür die Phase “Normal” benutze ich folgenden Codeschnipsel. Speichert den Plot ab (hier: tag_norm).\n\n\ntag_norm <- ggplot(subset(Mean_h, Phase %in% c(\"Normal\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  ...\n\n\n\nHinweis: Achtet darauf, dass die Skalierung der y-Achse bei allen 4 Plots dieselbe ist (z.B. immer vom 0 bis 25).\n3c)\nArrangiert die vier erstellten Plots und speichert das Resultat. Das ist etwas tricky, darum hier der vollständige Code.\n\n\n# Arrange und Export Tagesgang\nggarrange(tag_lock_1+            # plot 1 aufrufen\n            rremove(\"x.text\")+   # plot 1 braucht es nicht alle Achsenbeschriftungen\n            rremove(\"x.title\"),            \n          tag_lock_2+            # plot 2 aufrufen\n            rremove(\"y.text\")+   # bei plot 2 brauchen wir keine Achsenbeschriftung\n            rremove(\"y.title\")+\n            rremove(\"x.text\")+\n            rremove(\"x.title\"),\n          tag_norm,\n          tag_covid+\n            rremove(\"y.text\")+   \n            rremove(\"y.title\"),\n          ncol = 2, nrow = 2,    # definieren, wie die plots angeordnet werden\n          heights = c(0.9, 1),  # beide plots sind wegen der fehlenden Beschriftung nicht gleich hoch\n          widths = c(1,0.9),    \n          labels = c(\"a) Lockdown 1\", \"b) Lockdown 2\", \"c) Normal\", \"d) Covid\"),\n          label.x = 0.1,        # wo stehen die Plottitel\n          label.y = 0.99,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\nggsave(\"Tagesgang.png\", width=25, height=25, units=\"cm\", dpi=1000,\n       path = \"results/\")\n\n\n\nAufgabe 4: Kennzahlen\nSchliesslich berechnen wir noch einige Kennzahlen (Anzahl Passagen, Richtungsverteilung, …).\n4a)\nGruppiert nach Phase und berechnet dieses mal die Summe (nicht den Durchschnitt) Total, IN und OUT (ähnlich wie in 2a und 3a).\nSpeichert das Ergebnis als .csv\n4b)\nDie Zeitreihen der 4 Phasen unterscheiden sich deutlich voneinander. Totale Summen sind da kaum miteinander vergleichbar, besser eignet sich der Durchschnitt oder der Median.\nGruppiert nach Phase und berechnet den Durchschnitt Total, IN und OUT und speichert das df unter mean_phase_d.\nErgänzt das mit der prozentualen Richtungsverteilung\n\n\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  ...\n\n\n\nSpeichert das Ergebnis als .csv\nSelektiert nun die absoluten Zahlen im df mean_phase_d sowie die relativen und speichert das jeweils in einem df mean_phase_d_abs und mean_phase_d_proz.\n\n\nmean_phase_d_abs <- mean_phase_d %>% dplyr::select(-c(Total, Proz_IN, Proz_OUT))\n\n\n\ntransformiert beide df mittels pivot_longer() von wide zu long:\n\n\nmean_phase_d_abs <- pivot_longer(mean_phase_d_abs, cols = c(\"IN\",\"OUT\"), \n             names_to = \"Gruppe\", values_to = \"Durchschnitt\")\n\n\n\n4c)\nNun visualisieren wie die Verteilung der absoluten und der relativen Zahlen nach Phasen in einem Barplot.\nErstellt je einen Plot zu den absoluten und den relativen Zahlen nach den unterstehenden Vorgaben und speichert beide Plots im Environment:\n\n\n\nArrangiert beide Plots nebeneinander und exportiert das Ergebnis (Arrangieren siehe 3c).\n\n\n\n",
    "preview": "fallstudien/BE_S_6_Deskriptive_Analysen/Deskriptive_Analysen_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_S_7_Multivariate_Statistik_Loesung/",
    "title": "KW 44: Loesung Multivariate Statistik",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [
      "Biodiversity & Ecosystems (S) Musterloesung"
    ],
    "contents": "\nAufgabe 1: Verbinden von Daten (Join)\n\n\n# 4.1 Einflussfaktoren Besucherzahl ####\n# Erstelle ein df indem die taeglichen Zaehldaten und Meteodaten vereint sind\numwelt <- inner_join(depo_d, meteo, by = c(\"Datum\" = \"time\"))\n# Das zusammenfuehren folgt evtl. in NA-Werten bei gewissen Tagen\nsum(is.na(umwelt))\n\n\n\nAufgabe 2: Convinience Variablen, Faktoren, Skalieren\n\n\n# nochmals einige Convinience Variablen\numwelt <- umwelt%>%\n  mutate(Ferien = if_else(  \n    Datum >= Fruehlingsferien_2019_start & Datum <= Fruehlingsferien_2019_ende |\n      Datum >= Sommerferien_2019_start & Datum <= Sommerferien_2019_ende |\n      Datum >= Herbstferien_2019_start & Datum <= Herbstferien_2019_ende |\n      Datum >= Winterferien_2019_start & Datum <= Winterferien_2019_ende |\n      Datum >= Fruehlingsferien_2020_start & Datum <= Fruehlingsferien_2020_ende |\n      Datum >= Sommerferien_2020_start & Datum <= Sommerferien_2020_ende |\n      Datum >= Herbstferien_2020_start & Datum <= Herbstferien_2020_ende |\n      Datum >= Winterferien_2020_start & Datum <= Winterferien_2020_ende |\n      Datum >= Fruehlingsferien_2021_start & Datum <= Fruehlingsferien_2021_ende |\n      Datum >= Sommerferien_2021_start & Datum <= max(depo$Datum),\n        \"1\", \"0\"))%>%\n  mutate(Ferien = factor(Ferien))\n\n# Faktor und integer\n# Im GLMM wird die Kalenderwoche und das Jahr als random factor definiert. Dazu muss sie als\n# Faktor vorliegen.\numwelt <- umwelt %>% \n  mutate(Jahr = as.factor(Jahr)) %>% \n  mutate(KW = as.factor(KW))\n\n# Unser Modell kann nur mit ganzen Zahlen umgehen. Zum Glueck habe wir die Zaehldaten\n# bereits gerundet.\n\n# pruefe str des df\nsummary(umwelt)\nstr(umwelt)\nsum(is.na(umwelt))\n\n\n# unser Datensatz muss ein df sein, damit scale funktioniert\numwelt <- as.data.frame(umwelt)\n\n#  Variablen skalieren\n# Skalieren der Variablen, damit ihr Einfluss vergleichbar wird \n# (Problem verschiedene Skalen der Variablen (bspw. Temperatur in Grad Celsius, \n# Niederschlag in Millimeter und Sonnenscheindauer in Minuten)\n\numwelt <- umwelt %>% \n  mutate(tre200jx_scaled = scale(tre200jx), \n         rre150j0_scaled = scale(rre150j0), \n         sremaxdv_scaled = scale(sremaxdv))\n\n\n\nAufgabe 3: Korrelationen und Variablenselektion\n\n\n# 4.2 Variablenselektion ####\n# Korrelierende Variablen koennen das Modelergebnis verfaelschen. Daher muss vor der\n# Modelldefinition auf Korrelation getestet werden.\n\n# Erklaerende Variablen definieren\n# Hier wird die Korrelation zwischen den (nummerischen) erklaerenden Variablen berechnet\ncor <-  cor(umwelt[,16:(ncol(umwelt))]) # in den [] waehle ich die skalierten Spalten.\n# Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden\n# hier kann auch die Schwelle für die Korrelation gesetzt werden, \n# 0.7 ist liberal / 0.5 konservativ\n# https://researchbasics.education.uconn.edu/r_critical_value_table/\ncor[abs(cor)<0.7] <-  0 #Setzt alle Werte kleiner 0.7 auf 0 (diese sind dann ok, alles groesser ist problematisch!)\ncor\n\n# Korrelationsmatrix erstellen\n# Zur Visualisierung kann ein einfacher Plot erstellt werden:\nchart.Correlation(umwelt[,16:(ncol(umwelt))], histogram=TRUE, pch=19)\n\n\n\n\nAufgabe 4 (OPTIONAL): Automatische Variablenselektion\n\n\n# # Automatisierte Variablenselektion \n# # fuehre die dredge-Funktion und ein Modelaveraging durch\n# # Hier wird die Formel für die dredge-Funktion vorbereitet\n# f <- Total ~ Wochentag + Ferien + Phase +\n#   tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled\n# # Jetzt kommt der Random-Factor hinzu und es wird eine Formel daraus gemacht\n# f_dredge <- paste(c(f, \"+ (1|KW)\", \"+ (1|Jahr)\"), collapse = \" \") %>% \n#   as.formula()\n# # Das Modell mit dieser Formel ausführen\n# m <- glmer(f_dredge, data = umwelt, family = poisson, na.action = \"na.fail\")\n# # Das Modell in die dredge-Funktion einfügen (siehe auch ?dredge)\n# all_m <- dredge(m)\n# # suche das beste Modell\n# print(all_m)\n# # Importance values der Variablen \n# # hier wird die wichtigkeit der Variablen in den verschiedenen Modellen abgelesen\n# MuMIn::importance(all_m) \n# \n# # Schliesslich wird ein Modelaverage durchgeführt \n# # Schwellenwert für das delta-AIC = 2\n# avgmodel <- model.avg(all_m, rank = \"AICc\", subset = delta < 500) \n# summary(avgmodel)\n\n\n\nAufgabe 5: Verteilung der abhängigen Variabel pruefen\n\n\n# 4.3 Pruefe Verteilung ####\n# pruefe zuerst nochmals, ob wir NA im df haben:\nsum(is.na(umwelt$Total))\n\nf1<-fitdist(umwelt$Total,\"norm\")  # Normalverteilung\n# f1_1<-fitdist((umwelt$Total + 1),\"lnorm\")  # log-Normalvert (beachte, dass ich +1 rechne. \n# log muss positiv sein; allerdings kann man die\n# Verteilungen dann nicht mehr miteinander vergleichen). \nf2<-fitdist(umwelt$Total,\"pois\")  # Poisson\nf3<-fitdist(umwelt$Total,\"nbinom\")  # negativ binomial\nf4<-fitdist(umwelt$Total,\"exp\")  # exponentiell\n# f5<-fitdist(umwelt$Total,\"gamma\")  # gamma (berechnung mit meinen Daten nicht möglich)\nf6<-fitdist(umwelt$Total,\"logis\")  # logistisch\nf7<-fitdist(umwelt$Total,\"geom\")  # geometrisch\n# f8<-fitdist(umwelt$Total,\"weibull\")  # Weibull (berechnung mit meinen Daten nicht möglich)\n\ngofstat(list(f1,f2,f3,f4,f6,f7), \n        fitnames = c(\"Normalverteilung\", \"Poisson\",\n                     \"negativ binomial\",\"exponentiell\", \"logistisch\",\n                     \"geometrisch\"))\n\n# die 2 besten (gemaess Akaike's Information Criterion) als Plot + normalverteilt, \nplot.legend <- c(\"Normalverteilung\", \"exponentiell\", \"negativ binomial\")\n# vergleicht mehrere theoretische Verteilungen mit den empirischen Daten\ncdfcomp(list(f1, f4, f3), legendtext = plot.legend)\n\n\n\n# --> Verteilung ist gemäss AICc negativ binomial. --> ich entscheide \n# mich für letztere.\n\n\n\nAufgabe 6: Multivariates Modell berechnen\n\n\n# 4.4 Berechne verschiedene Modelle ####\n\n# Hinweise zu GLMM: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n# Ich verwende hier die Funktion glmer aus der Bibliothek lme4. glmer sei neuer, \n# schneller und zuverlaessiger als vergleichbare Funktionen.\n# Die Totale Besucheranzahl soll durch verschiedene Parameter erklaert werden. \n# Die saisonalitaet (KW, Jahr) soll hierbei nicht beachtet werden, \n# sie wird als random Faktor bestimmt --> Saisonbereinigung.\n\n# Einfacher Start\n# Auch wenn wir gerade herausgefunden haben, dass die Verteilung negativ binomial ist,\n# berechne ich für den Vergleich zuerst ein einfaches Modell der Familie poisson.\nTages_Model <- glmer(Total ~ Wochentag + Ferien + Phase +\n                       tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                       (1|KW) + (1|Jahr), family = poisson, data = umwelt)\n\nsummary(Tages_Model)\n# Inspektionsplots\nplot(Tages_Model, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model)\n\n\n\n# pruefe auf Overdispersion\ndispersion_glmer(Tages_Model) #it shouldn't be over 1.4\n# wir gut erklaert das Modell?\nr.squaredGLMM(Tages_Model) \n\n\n# Berechne ein negativ binomiales Modell\n# gemäss AICc die beste Verteilung\nTages_Model_nb <- glmer.nb(Total ~ Wochentag + Ferien + Phase +\n                             tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                             (1|KW) + (1|Jahr), data = umwelt)\n\nsummary(Tages_Model_nb)\nplot(Tages_Model_nb, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model_nb)\n\n\n\ndispersion_glmer(Tages_Model_nb)\nr.squaredGLMM(Tages_Model_nb) \n\n\n# auf quadratischen Term testen (\"es gehen weniger Leute in den Wald, wenn es zu heiss ist\")\nTages_Model_nb_quad <- glmer.nb(Total ~ Wochentag + Ferien + Phase +\n                                  tre200jx_scaled + I(tre200jx_scaled^2) + \n                                  rre150j0_scaled + sremaxdv_scaled +\n                                  (1|KW) + (1|Jahr), data = umwelt)\n\nsummary(Tages_Model_nb_quad)\nplot(Tages_Model_nb_quad, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model_nb_quad)\n\n\n\ndispersion_glmer(Tages_Model_nb_quad)\nr.squaredGLMM(Tages_Model_nb_quad) \n\n\n# Interaktion testen, da Ferien und / oder Wochentage einen Einfluss auf\n# die Besuchszahlen waehrend des Lockown haben koennen!\n# (Achtung: Rechenintensiv!)\n# Tages_Model_nb_int <- glmer.nb(Anzahl_Total ~  Wochentag  * Ferien + Phase +\n#                                  tre200jx_scaled + I(tre200jx_scaled^2) * \n#                                  rre150j0_scaled + sremaxdv_scaled +\n#                                  (1|KW) + (1|Jahr), data = umwelt)\n# \n# summary(Tages_Model_nb_int)\n# plot(Tages_Model_nb_int, type = c(\"p\", \"smooth\"))\n# qqmath(Tages_Model_nb_int)\n# dispersion_glmer(Tages_Model_nb_int)\n# r.squaredGLMM(Tages_Model_nb_int) \n\n\n# Vergleich der Modellguete mittels AICc\ncand.models<-list()\ncand.models[[1]] <- Tages_Model\ncand.models[[2]] <- Tages_Model_nb\ncand.models[[3]] <- Tages_Model_nb_quad\n\nModnames<-c(\"Tages_Model\",\"Tages_Model_nb\", \n            \"Tages_Model_nb_quad\")\naictab(cand.set=cand.models,modnames=Modnames)\n#K = Anzahl geschaetzter Parameter (2 Funktionsparameter und die Varianz)\n#Delta_AICc <2 = Statistisch gleichwertig\n#AICcWt =  Akaike weight in %\n\n# --> Ich entscheide mich bei diesen drei Modellen für das Tages_Model_nb_quad\n# Warum: statistisch gleichwertig und ich denke die Quadratur macht Sinn!\n\n\n# Berechne ein Modell mit exponentieller Verteilung:\n# gemäss AICc der Verteilung die zweitbeste\n# https://stats.stackexchange.com/questions/240455/fitting-exponential-regression-model-by-mle\nTages_Model_exp <- glmer((Total+1) ~ Wochentag + Ferien + Phase +\n                           tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                           (1|KW) + (1|Jahr), family = Gamma(link=\"log\"), data = umwelt)\n\nsummary(Tages_Model_exp, dispersion=1)\n# Inspektionsplots\nplot(Tages_Model_exp, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model_exp)\n\n\n\n# pruefe auf Overdispersion\ndispersion_glmer(Tages_Model_exp) #it shouldn't be over 1.4\n# wir gut erklaert das Modell?\nr.squaredGLMM(Tages_Model_exp) \n\n# --> Die zweitbeste Verteilung (exp) führt auch nicht dazu, dass die Modellvoraussetzungen besser\n# erfüllt werden\n\n# 4.5 Transformationen ####\n# Die Modellvoraussetzungen waren überall mehr oder weniger verletzt.\n# Das ist ein Problem, allerdings auch nicht ein so grosses.\n# (man sollte es aber trotzdem ernst nehmen)\n# Schielzeth et al. Robustness of linear mixed‐effects models to violations of distributional assumptions\n# https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13434\n# Lo and Andrews, To transform or not to transform: using generalized linear mixed models to analyse reaction time data\n# https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\n\n# die Lösung ist nun, die Daten zu transformieren:\n# mehr unter: https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/\n\n# berechne skewness coefficient \nlibrary(moments)\nskewness(umwelt$Total)\n# A positive value means the distribution is positively skewed (rechtsschief).\n# The most frequent values are low; tail is toward the high values (on the right-hand side)\n\n# log 10, da stark rechtsschief\nTages_Model_quad_Jahr_log10 <- lmer(log10(Total+1) ~ Wochentag + Ferien + Phase +\n                                             tre200jx_scaled + I(tre200jx_scaled^2) + \n                                             rre150j0_scaled + sremaxdv_scaled +\n                                             (1|KW) + (1|Jahr), data = umwelt)\nsummary(Tages_Model_quad_Jahr_log10)\nplot(Tages_Model_quad_Jahr_log10, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model_quad_Jahr_log10)\n\n\n\ndispersion_glmer(Tages_Model_quad_Jahr_log10)\nr.squaredGLMM(Tages_Model_quad_Jahr_log10) \n# lmer zeigt keine p-Werte, da diese schwer zu berechnen sind. Alternative Packages berechnen diese\n# anhand der Teststatistik. Achtung: die Werte sind wahrscheinlich nicht präzise!\n# https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q2/000904.html\ntab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE)\n\n\n# natural log, da stark rechtsschief\nTages_Model_quad_Jahr_ln <- lmer(log(Total+1) ~ Wochentag + Ferien + Phase +\n                                          tre200jx_scaled + I(tre200jx_scaled^2) + \n                                          rre150j0_scaled + sremaxdv_scaled +\n                                          (1|KW) + (1|Jahr), data = umwelt)\nsummary(Tages_Model_quad_Jahr_ln)\nplot(Tages_Model_quad_Jahr_ln, type = c(\"p\", \"smooth\"))\n\n\n\nqqmath(Tages_Model_quad_Jahr_ln)\n\n\n\ndispersion_glmer(Tages_Model_quad_Jahr_ln)\nr.squaredGLMM(Tages_Model_quad_Jahr_ln) \n\n\n# --> Die Modellvoraussetzungen sind deutlich besser erfüllt jetzt wo wir Transformationen \n# benutzt haben. log10 und ln performen beide gleich gut. Da log10 in meinem Bsp\n# aber deutlich mehr der Varianz erklärt, entscheide ich mich schliesslich für dieses Modell.\n\n# Zusatz: ACHTUNG - Ruecktransformierte Regressionskoeffizienten zu erlangen (fuer die Interpretation, das Plotten), \n# ist zudem nicht moeglich (Regressionskoeffizienten sind nur im transformierten Raum linear). \n# Ein ruecktransformierter Regressionskoeffiziente haette eine nicht-lineare Beziehung mit der \n# abhaengigen Variable.\n\n\n# 4.6 Exportiere die Modellresultate ####\n# (des besten Modells)\ntab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE)\n# The marginal R squared values are those associated with your fixed effects, \n# the conditional ones are those of your fixed effects plus the random effects. \n# Usually we will be interested in the marginal effects.\n\n\n\nAufgabe 7: Modellvisualisierung\n\n\n# 4.7 Visualisiere Modellresultate ####\n\n# ZUSATZ: Wir haben die Wetterparameter skaliert. \n# Fuer die Plots muss das beruecksichtigt werden: wir stellen nicht die wirklichen Werte\n# dar sondern die skalierten. Mit folgendem Befehl kann man die Skalierung nachvollziehen:\n# attributes(umwelt$tre200jx_scaled)\n# Die Skalierung kann rueckgaengig gemacht werden, indem man die Skalierten werte mit\n# dem scaling factor multipliziert und dann den Durchschnitt addiert:\n# Bsp.: d$s.x * attr(d$s.x, 'scaled:scale') + attr(d$s.x, 'scaled:center')\n# mehr dazu: https://stackoverflow.com/questions/10287545/backtransform-scale-for-plotting\n# --> wir bleiben aber bei den skalierten Werten, leben damit und sind uns dessen bewusst.\n\n# Auch beim Plotten der Modellresultate gilt: \n# visualisiere nur die Parameter welche nach der Modellselektion uebig bleiben\n# und signifikant sind!\n# plot_model / type = \"pred\" sagt die Werte \"voraus\"\n# achte auf gleiche Skalierung der y-Achse (Vergleichbarkeit)\n\n\n# Temperatur\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \n                  \"tre200jx_scaled [all]\", # [all] = Model contains polynomial or cubic / \n                #quadratic terms. Consider using `terms=\"tre200jx_scaled [all]\"` \n                # to get smooth plots. See also package-vignette \n                # 'Marginal Effects at Specific Values'.\n                title = \"\", axis.title = c(\"Tagesmaximaltemperatur [°C]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n# fuege die Achsenbeschriftung hinzu. Hier wird auf die unskalierten Werte zugegriffen.\nlabels <- round(seq(floor(min(umwelt$tre200jx)), ceiling(max(umwelt$tre200jx)),\n                    # length.out = ___ --> Anpassen gemaess breaks auf dem Plot\n                    length.out = 5), 0) \n(Tempplot <- t + \n    scale_x_continuous(breaks = c(-2,-1,0,1,2), \n                       labels = c(labels))+\n    # fuege die y- Achsenbeschriftung hinzu. Hier transformieren wir die Werte zurueck\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n\n\n# ggsave(\"temp.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n# Regen\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"rre150j0_scaled\", \n                title = \"\", axis.title = c(\"Halbtagessumme Niederschlag [mm]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\nlabels <- round(seq(floor(min(umwelt$rre150j0)), ceiling(max(umwelt$rre150j0)),\n                    length.out = 4), 0)\n(t + scale_x_continuous(breaks = c(0,4,8,12), labels = c(labels))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n\n\n# ggsave(\"rain.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n# Sonne\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"sremaxdv_scaled\", \n                title = \"\", axis.title = c(\"Sonnenscheindauer [min]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\nlabels <- round(seq(floor(min(umwelt$sremaxdv)), ceiling(max(umwelt$sremaxdv)),\n                    length.out = 3), 0)\n(t + scale_x_continuous(breaks = c(-1,0,1), labels = c(labels))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n\n\n# ggsave(\"sun.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n\n# Phase\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Phase\", \n                title = \"\", axis.title = c(\"Phase\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n(lockplot <- t + \n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n\n\n# ggsave(\"phase.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n# Ferien\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Ferien\", \n                title = \"\", axis.title = c(\"Ferien\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n(ferienplot <- t + scale_x_continuous(breaks = c(0,1), labels = c(\"Nein\", \"Ja\"))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n\n\n# ggsave(\"ferien.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n# Wochentag\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Wochentag\", \n                title = \"\", axis.title = c(\"Wochentag\", \"Fussgaenger:innen pro Tag [log]\"))\n(wdplot <- t + scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                                  labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                                  limits = c(0, 2))+ \n    theme_classic(base_size = 20))+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n# ggsave(\"wd.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n\n\n\n\n\n",
    "preview": "fallstudien/BE_S_7_Multivariate_Statistik_Loesung/Musterloesung_Multivariate_Statistik_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "fallstudien/BE_S_7_Multivariate_Statistik/",
    "title": "KW 45 Aufgabe Multivariate Statistik",
    "description": {},
    "author": [
      {
        "name": "Adrian Hochreutener",
        "url": {}
      }
    ],
    "date": "2021-11-22",
    "categories": [
      "Biodiversity & Ecosystems (S)"
    ],
    "contents": "\nNachdem die deskriptiven Resultate vorliegen, kann jetzt die Berechnung eines multivariaten Modells angegangen werden. Das Ziel ist es, den Zusammenhang zwischen der gesamten Anzahl Besucher:innen (Total) und verschiedenen erklärenden Variablen (Wetter, Ferien, Phase Covid, Wochentag, KW, Jahr) aufzuzeigen.\nAufgabe 1: Verbinden von Daten (Join)\nAktuell haben wir noch zwei einzelne Datensätze von Interesse:\neinen mit den täglichen Besuchszahlen von Besucher:innen mit den dazugehörigen Datumsinformationen (Datensatz “depo_d” - zu Tagen aggregierte Stunden)\nund einen mit den Wetterparametern (“meteo”).\nDiese beiden Datensätze müssen miteinander verbunden werden. Ziel: Ein Datensatz mit den täglichen Zähldaten und Datumsinformationen angereichert mit Wetterdaten. Der neue Datensatz soll \" umwelt \" heissen.\nSind durch das Zusammenführen NA’s entstanden? Falls ja, müssen alle für die weiteren Auswertungen ausgeschlossen werden.\nAufgabe 2: Convinience Variablen, Faktoren, Skalieren\nWir haben bereits verschiedene Convinience Variablen definiert. Nun brauchen wir noch neu die Ferienzeiten als Faktor.\n2a)\nDefiniert mit if_else() alle Ferienzeiträume in euren df umwelt. WENN Ferien waren, DANN = 1, SONST = 0\nHinweis: etwas ganz ähnliches habt ihr unter Import/Vorverarbeitung bereits für die Covid-Phasen gemacht.\n2b)\nMacht aus den Ferien einen Faktor.\nAuch das Jahr und die KW müssen als Faktor vorliegen.\nHinweis: Im Modell werden die Levels der Variablen (z.B. bei der Phase: Normal, Lockdown 1 und 2, Covid) alphabetisch geordnet und die Effektstärken der einzelnen Levels gegenüber dem ersten Level gerechnet. Das macht wenig Sinn, den in diesem Fall zeigt es uns die Effekte von Lockdown 1, Lockdown 2 und Normal gegenüber Covid an.\nBesser wäre aber Lockdown 1, Lockdown 2 und Covid gegenüber Normal.\nDas ändert man, indem ihr die Phase zu einem Faktor macht und die Levels entsprechend ändert (gut möglich, dass ihr das bereits so definiert habt).\nNachfolgende Schritte funktionieren nur, wenn umwelt als data.frame vorliegt! Prüft das und ändert das, falls noch kein data.frame (Hinweis: auch ein “tibble” funktioniert nicht, obwohl bei der Abfrage is.data.frame() TRUE angegeben wird. Damit ihr beim scalen keine NaN Werte erhaltet, wendet ihr darum am besten in allen Fällen zuerst den Befehl as.data.frame() an).\nUnser Modell kann in der abhängigen Variabel nur mit Ganzzahlen (Integer) umgehen. Daher müssen Kommazahlen in Integer umgewandelt werden. Zum Glück haben wir das schon gemacht und uns bleibt nichts weiter zu tun. =)\n2c)\nProblem: verschiedene Skalen der Variablen (z.B. Temperatur in Grad Celsius, Niederschlag in Millimeter und Sonnenscheindauer in %)\nLösung: Skalieren aller Variablen mit Masseinheiten gemäss unterstehendem Code:\n\numwelt <- umwelt %>% \n  mutate(tre200jx_scaled = scale(tre200jx)%>%\n  ...\n\nAufgabe 3: Korrelationen und Variablenselektion\n3a)\nKorrelierende Variablen können das Modellergebnis verfälschen. Daher muss vor der Modelldefinition auf Korrelation zwischen den Messwerten getestet werden. Welches sind die erklärenden Variablen, welches ist die Abhängige? (Ihr müsst nicht prüfen, ob die Voraussetzungen zur Berechnung von Korrelationen erfüllt sind)\nTeste mittels folgendem Code auf eine Korrelation zwischen den Messwerten.\n\ncor <-  cor(umwelt[,ERSTE SPALTE MIT ERKLAERENDEN MESSWERTEN : \n                     LETZTE SPALTE MIT ERKLAERENDEN MESSWERTEN)])\n\n3b)\nKorrelationsmatrix erstellen\nMit dem folgenden Code kann eine Korrelationsmatrix (mit den Messwerten) aufgebaut werden. Hier kann auch die Schwelle für die Korrelation gesetzt werden (0.7 ist liberal / 0.5 konservativ).\n\n\ncor[abs(cor) < 0.7] <-  0 #Setzt alle Werte kleiner 0.7 auf 0\n\n\n\nZur Visualisierung kann ein einfacher Plot erstellt werden.\n\nchart.Correlation(umwelt[,ERSTE SPALTE MIT ERKLAERENDEN MESSWERTEN : \n                     LETZTE SPALTE MIT ERKLAERENDEN MESSWERTEN)], histogram=TRUE, pch=19)\n\nWo kann eine kritische Korrelation beobachtet werden? Kann man es verantworten, trotzdem alle drei Wetterparameter in das Modell zu geben?\nFalls ja: warum? Falls nein: schliesst den betreffenden Parameter aus. Wenn ihr Parameter ausschliesst: welchen der beiden korrelierenden Parameter behaltet ihr im Modell?\nAufgabe 4 (OPTIONAL): Automatische Variablenselektion\nFühre die dredge-Funktion und ein Modelaveraging durch. Der Code dazu ist unten. Was passiert in der Funktion? Macht es Sinn, die Funktion auszuführen?\nHinweis: untenstehender Code ist rechenentensiv.\n\n\nf <- Total ~ Wochentag + Ferien + Phase +\n  tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled\n# Jetzt kommt der Random-Factor hinzu und es wird eine Formel daraus gemacht\nf_dredge <- paste(c(f, \"+ (1|KW)\", \"+ (1|Jahr)\"), collapse = \" \") %>% \n  as.formula()\n# Das Modell mit dieser Formel ausführen\nm <- glmer(f_dredge, data = umwelt, family = poisson, na.action = \"na.fail\")\n# Das Modell in die dredge-Funktion einfügen (siehe auch ?dredge)\nall_m <- dredge(m)\n# suche das beste Modell\nprint(all_m)\n# Importance values der Variablen \n# hier wird die wichtigkeit der Variablen in den verschiedenen Modellen abgelesen\nMuMIn::importance(all_m) \n\n# Schliesslich wird ein Modelaverage durchgeführt \n# Schwellenwert für das delta-AIC = 2\navgmodel <- model.avg(all_m, rank = \"AICc\", subset = delta < 2) \nsummary(avgmodel)\n\n\n\nAufgabe 5: Verteilung der abhaengigen Variabel pruefen\nDie Verteilung der abhängigen Variabel bestimmt generell, was für ein Modell geschrieben werden kann. Die Modelle gehen von einer gegebenen Verteilung aus. Wenn diese Annahme verletzt wird, kann es sein, dass das Modellergebnis nicht valide ist.\nFolgender Codeblock zeigt, wie die Daten auf verschiedene Verteilungen passen.\nHinweis: es kann sein, dass nicht jede Verteilung geplottet werden kann, es erscheint eine Fehlermeldung. Das ist nicht weiter schlimm, die betreffende Verteilung kann gelöscht werden. Analog muss das auch im Befehl gofstat() passieren.\nDie besten drei Verteilungen (gemäss AIC) sollen zur Visualisierung geplottet werden.\n\n\nf1<-fitdist(umwelt$Anzahl_Total,\"norm\")  # Normalverteilung\nf1_1<-fitdist(umwelt$Anzahl_Total,\"lnorm\")  # log-Normalvert. \nf2<-fitdist(umwelt$Anzahl_Total,\"pois\")  # Poisson\nf3<-fitdist(umwelt$Anzahl_Total,\"nbinom\")  # negativ binomial\nf4<-fitdist(umwelt$Anzahl_Total,\"exp\")  # exponentiell\nf5<-fitdist(umwelt$Anzahl_Total,\"gamma\")  # gamma\nf6<-fitdist(umwelt$Anzahl_Total,\"logis\")  # logistisch\nf7<-fitdist(umwelt$Anzahl_Total,\"geom\")  # geometrisch\nf8<-fitdist(umwelt$Anzahl_Total,\"weibull\")  # Weibull\n\ngofstat(list(f1,f1_1,f2,f3,f4,f5,f6,f7,f8), \n        fitnames = c(\"Normalverteilung\", \"log-Normalverteilung\", \"Poisson\",\n                     \"negativ binomial\",\"exponentiell\",\"gamma\", \"logistisch\",\n                     \"geometrisch\",\"weibull\"))\n\n# die 4 besten (gemaess Akaike's Information Criterion) als Plot, \nplot.legend <- c(\"log norm\", \"weibull\", \"gamma \", \"negativ binomial\")\n# vergleicht mehrere theoretische Verteilungen mit den empirischen Daten\ncdfcomp(list(f1_1, f8, f5, f3), legendtext = plot.legend)\n\n\n\nWie sind unsere Daten verteilt? Welche Modelle können wir anwenden?\nAufgabe 6: Multivariates Modell berechnen\nIch verwende die Funktion glmer() aus der Bibliothek lme4. glmer ist neuer, schneller und zuverlässiger als vergleichbare Funktionen (diese Bibliothek wird auch in vielen wissenschaftlichen Papern im Feld Biologie / Wildtiermamagement zitiert).\nHinweise zu GLMM: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n6a)\nHinweis: Auch wenn wir gerade herausgefunden haben, dass die Verteilung negativ binomial (in meinem Fall) ist, berechne ich für den Vergleich zuerst ein “einfaches Modell” der Familie poisson. Alternative Modelle rechnen wir in 6c.\nDie Totale Besucheranzahl soll durch die Wetterparameter, den Wochentag, die Ferien sowie die Covid-Phasen erklärt werden (Datensatz “umwelt”). Die Saisonalität (KW und Jahr) soll hierbei nicht beachtet werden, sie werden als “random factor” bestimmt.\nFrage: Warum bestimmen wir KW und Jahr als random factor?\nFalls ihr der Meinung seid, KW und / oder Jahr sind keine “guten” random factor, dann nehmt sie nicht an random factor ins Modell sondern als erklärende Variable. Begründet das unbedingt in eurer Methodik.\nDie Modellformel lautet:\n\nTages_Model <- glmer(ABHAENGIGE VARIABLE ~ ERKLAERENDE VARIABLE 1 + ERKLAERENDE VARIABLE 2 +\n                      ERKLAERENDE VARIABLE 3 + ERKLAERENDE VARIABLE 4 + \n                      ERKLAERENDE VARIABLE 5 + ERKLAERENDE VARIABLE 6 +\n                     (1|RANDOM FACTOR A)+ (1|RANDOM FACTOR B),\n                     family = poisson, data = DATENSATZ))\n\nsummary(Tages_Model) #Zeigt das Resultat des Modells\n\nFrage: Was bedeutet “family = poisson”?\nLöst zuerst Aufgabe 6b bevor ihr alternative Modelle rechnet; das kommt in Aufgabe 6c!\n6b) Modelldiagnostik\nPrüft optisch ob euer Modell valide ist.\nHinweis: glmer bringt einige eigene Funktionen mit, mit denen sich testen lässt, ob das Modell valide ist. Unten sind sie aufgeführt (–> analog zu den Funktionen aus der Vorlesung, aber halt für glmer).\n\n\n# Verteilung der Residuen (Varainzhomogenitaet)\nplot(Tages_Model, type = c(\"p\", \"smooth\"))\n# Pruefen auf Normalverteilung der Residuen\nqqmath(Tages_Model)\n\n# Overdispersion describes the observation that variation is higher than would be expected.\ndispersion_glmer(Tages_Model) #it shouldn't be over 1.4\n# zeige die erklaerte Varianz (je hoeher r2m ist, desto besser!)\nr.squaredGLMM(Tages_Model) \n\n\n\nSind die Voraussetzungen des Modells erfuellt?\n6c) Alternative Modelle\nWir sind auf der Suche nach dem minimalen adäquaten Modell. Das ist ein iterativer Prozess. Wir schreiben ein Modell, prüfen ob die Voraussetzungen erfüllt sind und ob die abhängige Variable besser erklärt wird als im vorhergehenden. Und machen das nochmals und nochmals…\nÜber family = kann in der Funktion _glmer()__ einiges (aber leider nicht alles so einfach [z.B. negativ binomiale Modelle]) angepasst werden: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html\nAuch über link = kann man anpassen: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/make.link.html\nUnsere (meine) Daten sind negativ binomial verteilt. Daher sollte wir unbedingt ein solches Modell programmieren. –> Funktion glmer.nb()\nFalls die Daten exponentiell Verteilt sind, hier der Link zu einem Blogeintrag dazu: https://stats.stackexchange.com/questions/240455/fitting-exponential-regression-model-by-mle\nHypothese: “Es gehen weniger Leute in den Wald, wenn es zu heiss ist” –> auf quadratischen Term Temperatur testen (Codeblock unten).\n\n\n...\ntre200jx_scaled + I(tre200jx_scaled^2) + \n  ...\n\n\n\nKönnte es zwischen einzelnen Variablen zu Interaktionen kommen, die plausible sind? (z. B.: Im Winter hat Niederschlag einen negativeren Effekt als im Sommer, wenn es heiss ist) –> Falls ja: testen!\nHinweis: Interaktionen berechnen ist sehr rechenintensiv. Auch die Interpretation der Resultate wird nicht unbedingt einfacher. Wenn ihr auf Interaktionen testet, dann geht “langsam” vor, probiert nicht zu viel auf einmal.\nWenn ihr verschiedene Modelle gerechnet habt, können diese über den AICc verglichen werden. Folgender Code kann dazu genutzt werden:\n\n\n# Vergleich der Modellguete mittels AICc\ncand.models<-list()\ncand.models[[1]] <- Tages_Model\ncand.models[[2]] <- Tages_Model_nb\ncand.models[[3]] <- Tages_Model_nb_quad\n\nModnames<-c(\"Tages_Model\",\"Tages_Model_nb\", \n            \"Tages_Model_nb_quad\")\naictab(cand.set=cand.models,modnames=Modnames)\n#K = Anzahl geschaetzter Parameter (2 Funktionsparameter und die Varianz)\n#Delta_AICc <2 = Statistisch gleichwertig\n#AICcWt =  Akaike weight in %\n\n\n\n6d) (OPTIONAL) Transformationen\nBei meinen Daten waren die Modellvoraussetzungen überall mehr oder weniger verletzt. Das ist ein Problem, allerdings auch nicht ein so grosses (man sollte es aber trotzdem ernst nehmen). Mehr dazu unter:\nSchielzeth et al. Robustness of linear mixed‐effects models to violations of distributional assumptions https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13434 Lo and Andrews, To transform or not to transform: using generalized linear mixed models to analyse reaction time data https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\nFalls die Voraussetzungen stark verletzt werden, wäre eine Transformation angezeigt.\nMehr dazu unter:\nhttps://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/\nBerechne den skewness coefficient\n\n\nlibrary(moments)\nskewness(umwelt$Anzahl_Total)\n# A positive value means the distribution is positively skewed (rechtsschief).\n# The most frequent values are low; tail is toward the high values (on the right-hand side)\n\n\n\nWelche Transformation kann angewandt werden?\nWas spricht gegen eine Transformation (auch im Hinblick zur Visualisierung und Interpretation)? Was spricht dafür?\n6c) Exportiere die Modellresultate (des besten Modells)\nModellresultate können mit summary() angezeigt werden. Ich verwende aber lieber die Funktion tab_model()! Die Resultate werden gerundet und praktisch im separaten Fenster angezeigt. Von dort kann man sie via copy + paste ins (z.B.) Word bringen.\n\n\ntab_model(MODELLNAME, transform = NULL, show.se = TRUE)\n# The marginal R squared values are those associated with your fixed effects, \n# the conditional ones are those of your fixed effects plus the random effects. \n# Usually we will be interested in the marginal effects.\n\n\n\nAufgabe 7: Modellvisualisierung\nVisualisiert die (signifikanten) Ergebnisse eures Modells.\nDas Resultat soll sich für kontinuierliche Variablen an untenstehendem Plot orientieren:\n\n\n\nFür diskrete Variablen haltet ihr euch bitte an diesen Plot:\n\n\n\nEinige Codeblocks, die euch dabei helfen können:\n\n\nt <- plot_model(NAME_DES_BESTEN_MODELLS, # hier sagen wir, aus welchem Modell geplottet werden soll\n                \n                # Wir moechten nicht nur die tatsaechlichen Werte geplottet, sondern \n                # \"Vorhersagen\" / predictions (fuer jeden Wert auf der x-Achse soll es auch einen\n                # auf der y-Achse geben)\n                type = \"pred\", \n                \n                # jetzt nennen wir den Term aus dem Modell:\n                # [all] = Unser Modell enthaellt polynomial oder cubic / quadratic Terme. \n                # mit [all] tragen wir dem Rechnung und zeichnen \"smooth\" plots\n                terms = \"tre200jx_scaled [all]\", \n                \n                # und schliesslich setzen wir die Achsentitel\n                title = \"\", axis.title = c(\"Tagesmaximaltemperatur [°C]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\n# Vorbereitungen zum Hinzufuegen der Achsenbeschriftung (Aktuell sehen wir noch die skalierten Werte). \n# Nun sollen aber die unskalierten Werte gezeigt werden.\nlabels <- round(seq(floor(min(umwelt$tre200jx)), ceiling(max(umwelt$tre200jx)),\n                    \n                    # length.out = ___ --> Anpassen gemaess der Anzahl zu sehender breaks auf dem Plot\n                    length.out = 5), 0) \n# Schliesslich fuegen wir die Achsenbeschriftung hinzu.\n(Tempplot <- t + \n    \n    # fuege die x- Achsenbeschriftung hinzu.\n    # breaks = c() --> Anpassen gemaess der zu sehender breaks auf dem Plot\n    scale_x_continuous(breaks = c(-2,-1,0,1,2), \n                       labels = c(labels))+\n    \n    # fuege die y- Achsenbeschriftung hinzu. Hier transformieren wir die Werte zurueck\n    # Hinweis: falls ihr keine Transformation gemacht habt, muessen die y-Werte auch nicht \n    # zuruecktransformiert werden.\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 15))\n\n# Exportiere das Resultat\nggsave(\"temp.png\", width=15, height=15, units=\"cm\", dpi=1000, \n       path = \"_fallstudien/_R_analysis/results/\") \n\n\n\nHinweis: damit unsere Plots verglichen werden können, sollen sie alle dieselbe Skalierung (limits) auf der y-Achse haben. Das wird erreicht, indem man bei jedem Plot die limits in scale_y_continuous() gleichsetzt.\nAbschluss\nNun habt ihr verschiedenste Ergebnisse vorliegen. In einem wissenschaftlichen Bericht sollen aber niemals alle Ergebnisse abgebildet werden. Eine Faustregel besagt, dass nur signifikante Ergebnisse visualisiert werden. Entscheidet euch daher, was ihr in eurem Bericht abbilden wollt und was lediglich besprochen werden soll.\n\n\n\n",
    "preview": "fallstudien/BE_S_7_Multivariate_Statistik/Multivariate_Statistik_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-12-02T09:48:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
