[
  {
    "path": "rauman/RaumAn1_Uebung_A/",
    "title": "Übungen A: Einführung",
    "description": {},
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn1"
    ],
    "contents": "\n\nContents\nAufgabe 1: Vektor Daten runterladen und importieren\nAufgabe 2: Daten Visualisieren\nInput: Koodinatensysteme\nAufgabe 3: Koordinatensyteme transformieren\nAufgabe 4: Chloroplethen Karte\nMusterlösung\n\nEs gibt bereits eine Vielzahl von Packages um in R mit räumlichen Daten zu arbeiten, die ihrerseits wiederum auf weiteren Packages basieren (Stichwort dependencies). Für Vektordaten dominierte lange das Package sp, welches nun durch sf abgelöst wurde. Wir werden wenn immer möglich mit sf arbeiten und nur in Ausnahmefällen auf andere Packages zurück greifen.\nFür die kommenden Übungen könnt ihr folgende Packages installieren bzw. laden:\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nAufgabe 1: Vektor Daten runterladen und importieren\nLade zunächst die Datensätze unter folgenden Links herunter:\n\nkantone.gpkg (Quelle: Swisstopo)\ngemeinden.gpkg (Quelle: Swisstopo)\n\nEs handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), eine alternatives Datenformat zum bekannteren Format “Shapefiles”. Importiere die Datensätze wie folgt:\n\n\nkantone <- read_sf(\"kantone.gpkg\")\ngemeinden <- read_sf(\"gemeinden.gpkg\") \n\n\n\nSchau Dir die importierten Datensätze an. Am meisten Informationen zu sf Objekten bekommst du, wenn du dir den Datensatz in der Konsole anschaust (in dem du den Variabel-Name in der Konsole eintippst). Mit dem RStudio Viewer werden sf Objekte nur sehr langsam geladen und die Metadaten werden nicht angezeigt.\nAufgabe 2: Daten Visualisieren\nVektordaten (sf Objekte) lassen sich teilweise sehr schön in die bekannten Tidyverse workflows integrieren. Das merkt man schnell, wenn man die Daten visualisieren möchte. In InfoVis 1 & 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennen gelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten. Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?\n\n\nggplot(gemeinden) + \n  geom_sf()\n\n\n\nggplot(kantone) + \n  geom_sf()\n\n\n\n\nInput: Koodinatensysteme\nIn der obigen visualierung fällt folgendes auf:\ndie X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen)\nder Umriss der Schweiz sieht in den beiden Datensätzen unterschiedlich aus (kantone ist gegenüber gemeinden gestaucht)\nDies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() könnnen die zugewiesenen Koordinatensysteme abgefragt werden.\n\n\nst_crs(kantone)\n\n\nCoordinate Reference System: NA\n\nst_crs(gemeinden)\n\n\nCoordinate Reference System: NA\n\nLeider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:\nCH1903 LV03: das alte Koordinatensystem der Schweiz\nCH1903+ LV95: das neue Koordinatensystem der Schweiz\nWGS84: ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).\nNun gilt es, anhand der Koordinaten die in der Spalte geometry ersichtlich sind das korrekte Koordinatensystem festzustellen. Wenn man sich auf epsg.io/map die Schweiz anschaut, kann man die Koordinaten in verschiedenen Koordinatensystem betrachten.\nBedienungshinweise:\n\nKoordinanten (des Fadenkreuzes) werden im ausgewählten Koordinatensystem dargestellt\n\nDas Koordinatensystem, in welchem die Koordinaten dargestellt werden sollen, kann mit “Change” angepasst werden\n\nFür Enthusiasten: Schau Dir die Schweiz in verschiedenen Koordinatensystemen an, in dem Du auf “Reproject Map” klickst\n\nWenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz kantone um das Koordinatensystem WGS84 handelt und bei gemeinden das Koordinatensystem CH1903+ LV95. Diese Koordinatensyteme weisen wir nun mit st_set_crs() und dem entsprechenden EPSG-Code (siehe die jeweiligen Links) zu.\n\n\nkantone <- st_set_crs(kantone, 4326)\ngemeinden <- st_set_crs(gemeinden, 2056)\n\n# zuweisen mit st_set_crs(), abfragen mit st_crs()\nst_crs(kantone)\n\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nAuch wenn das CRS der Datensätze bekannt ist, nutzt ggplot immer noch EPSG 4326 um die Achsen zu beschriften. Wenn das stört, kann man coord_sf(datum = 2056) in einem weiteren Layer spezifizieren. Oder aber man blendet die Achsenbeschriftung mit theme_void() komplett aus. Versuche beide Varianten.\n\n\n\nAufgabe 3: Koordinatensyteme transformieren\nIn der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen  wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz kantone mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.\nVor der Transformation (betrachte die Attribute Bounding box sowie Geodetic CRS):\n\n\nkantone\n\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 7\n   NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n * <chr>           <dbl>      <dbl>      <dbl> <chr>        <dbl>\n 1 Graubünden         18         NA     710530 0           199021\n 2 Bern                2      11897     595951 1          1039474\n 3 Valais             23       1060     522463 0           345525\n 4 Vaud               22      39097     321202 1           805098\n 5 Ticino             21       7147     281215 0           351491\n 6 St. Gallen         17       7720     202820 1           510734\n 7 Zürich              1       6811     172894 0          1539275\n 8 Fribourg           10       7818     167142 1           321783\n 9 Luzern              3       6438     149352 0           413120\n10 Aargau             19        870     140380 1           685845\n# … with 41 more rows, and 1 more variable: geom <POLYGON [°]>\n\n\n\n\nNach der Transformation (betrachte die Attribute Bounding box sowie Projected CRS):\n\n\nkantone\n\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2485410 ymin: 1075268 xmax: 2833858 ymax: 1295934\nProjected CRS: CH1903+ / LV95\n# A tibble: 51 × 7\n   NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n * <chr>           <dbl>      <dbl>      <dbl> <chr>        <dbl>\n 1 Graubünden         18         NA     710530 0           199021\n 2 Bern                2      11897     595951 1          1039474\n 3 Valais             23       1060     522463 0           345525\n 4 Vaud               22      39097     321202 1           805098\n 5 Ticino             21       7147     281215 0           351491\n 6 St. Gallen         17       7720     202820 1           510734\n 7 Zürich              1       6811     172894 0          1539275\n 8 Fribourg           10       7818     167142 1           321783\n 9 Luzern              3       6438     149352 0           413120\n10 Aargau             19        870     140380 1           685845\n# … with 41 more rows, and 1 more variable: geom <POLYGON [m]>\n\nAufgabe 4: Chloroplethen Karte\nNun wollen wir die Gemeinden respektive die Kantone nach ihrer Einwohnerzahl einfärben. Dafür verwenden wir wie gewohnt die Methode aes(fill = ...) von ggplot.\nTips:\num die scientific notation (z.B. 3e+03) zu verhindern, könnt ihr den Befehl options(scipen = 999) ausführen\num die Darstellung der Gemeinde- (bzw. Kantons-) Grenzen zu verhindern, könnt ihr im entsprechenden Layer color = NA setzen. Alternativ könnt ihr die Linienbreite mit size = verändern.\n\n\n\nFigure 1: Der Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich\n\n\n\nMusterlösung\n\nR-Code\n\n\n\n\n",
    "preview": "rauman/RaumAn1_Uebung_A/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "rauman/RaumAn1_Uebung_B/",
    "title": "Übung B Spatial Joins",
    "description": {},
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn1"
    ],
    "contents": "\n\nContents\nAufgabe 1: Geopackage “Layers”\nAufgabe 2: Datensätze erkunden\nAufgabe 3: Spatial Join mit Punkten\nAufgabe 4: Spatial Join mit Flächen\nMusterlösung\n\n\n\n\nFür die kommende Übung arbeiten wir mit nachstehendem Datensatz. Lade diesen Herunter und importiere ihn in R.\n\ngruental.gpkg\n\nZudem brauchen wir die folgenden libraries:\n\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\n\n\n\nAufgabe 1: Geopackage “Layers”\nAllenfalls ist euch beim Importieren des Geopackage gruental.pgkg folgende Warnmeldung aufgefallen:\nWarning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.\nDiese Warnmeldung weist darauf hin, dass das Geopackage gruental.gpkg mehrere Layers (rep. Datensätze) enthält und nur der erste Layer importiert wurde. Bringe mit dem Befehl st_layers die Layer Namen in Erfahrung und nutze diese im Anschluss in st_read (als Argument layer =) um die layers einzeln zu importieren und in variablen zu speichern (zB in als Variable wiesen und baeume).\nAufgabe 2: Datensätze erkunden\nNimm dir etwas Zeit und erkunde die beiden Datensätze. Nutze dafür auch die Visualisierungsmöglichkeiten von ggplot (insbesondere geom_sf).\n\n\n\nFigure 1: Beispielsweise kannst du die Daten in dieser Weise visualisieren.\n\n\n\nAufgabe 3: Spatial Join mit Punkten\nWir wollen nun für jeden Baum wissen, ob er sich in einer Wiese befindet oder nicht. Dazu nutzen wir die GIS-Technik Spatial Join, die in der Vorlesung beschrieben wurde. In sf können wir Spatial Joins mit der Funktion st_join durchführen, dabei gibt es nur left sowie inner-Joins (vgl. PrePro 1 & 2). So müssen die Punkte “Links”, also an erste Stelle aufgeführt werden, da wir ja Attribute an die Punkte anheften wollen.\nBeachte, dass der Output eine neue Spalte flaechen_typ aufweist. Diese ist leer (NA) wenn sich der entsprechende Baum nicht in einer Wiese befindet. Wie viele Bäume befinden sich in einer Wiese, wie viele nicht?\n\n\n\nAufgabe 4: Spatial Join mit Flächen\nAnalog der Vorlesung wollen wir nun in Erfahrung bringen, wie hoch der Wiesen-Anteil im Umkreis von 20m um jeden Baum ist. Dazu sind folgende Schritte nötig:\nAls erster Schritt müssen wir jeden Baum mit einem 20m Puffer verstehen. Nutze dazu st_buffer um speichere den Output als baeume_20m. Schau dir baeume_20m nun genau an. Um welchen Geometrietyp handelt es sich dabei nun?\nBerechnen nun die Schnittmenge aus baeume_20m und wiesen mit der Funktion st_intersection und speichere den Output als baeume_wiesen. Exploriere nun baeume_wiesen, auch mit ggplot(). Was ist passiert? Überprüfe die Anzahl Zeilen pro Datensatz. Haben die sich verändert? Wenn ja, warum?\nBerechnen nun die Flächengrösse pro Geometrie mit der Funktion st_area(). Speichere den Output in einer neuen Spalte von baeume_wiesen (z.B. mit dem Namen wiesen_flaeche). Tipp: Konvertiere den Output aus st_area einen nummerischen Vektor mit as.numeric().\nBerechne nun aus wiesen_flaeche den wiesen_anteil. Tipp: 100% ist die Kreisfläche aus \\(r^2\\times \\pi\\), wobei in unserem Fall \\(r = 20\\) entspricht.\nUm die berechneten Werte in den Datensatz baeume zu überführen braucht es noch folgende Schritte:\nKonvertiere baeume_wiesen in eine data.frame mit st_drop_geometry und speichere diese als baeume_wiesen_df\nNutze die Spalte baum_id in baeume_wiesen_df um den berechneten wiesen_anteil in den Datenatz baeume zu überführen. Tipp: Nutze dafür einen left_join\nErsetze alle NA Werte in der Spalte wiesen_anteil mit 0.\n\n\n\n\n\n\nFigure 2: Nach dieser Übung kannst du das Resultat optional in dieser Weise visualisieren.\n\n\n\nMusterlösung\n\nR-Code\n\n\n\n\n",
    "preview": "rauman/RaumAn1_Uebung_B/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "rauman/RaumAn2_Uebung_A/",
    "title": "Übung A: G-Function",
    "description": "Analyse von Punktverteilungen",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn2"
    ],
    "contents": "\n\nContents\nAufgabe 1\nAufgabe 2\nAufgabe 3\nMusterlösung\n\n\nAufgabe 1\nFür die heutige Übung benötigen wir nachstehende Datensätze. Lade diese herunter und importiere sie in R. Prüfe, ob das CRS korrekt gesetzt wurde, setze es wenn nötig. Mache dich mit den Daten vertraut (visualieren, durchscrollen usw).\n\n\n\n\nrotmilan.gpkg\nschweiz.gpkg\nluftqualitaet.gpkg\n\nDer Datensatz rotmilan.gpkg stammt aus einem grösseren Forschungsprojekt der Vogelwarte Sempach Mechanismen der Populationsdynamik beim Rotmilan. Der Datensatz wurde über die Plattform movebank zur Verfügung gestellt. Es handelt sich dabei um ein einzelnes Individuum, welches seit 2017 mit einem Sender versehen ist und über ganz Mitteleuropa zieht. Wir arbeiten in dieser Übung nur mit denjenigen Datenpunkten, die in der Schweiz erfasst wurden. Wer den ganzen Datensatz analysieren möchte, kann sich diesen über den Movebank-Link runterladen.\nDer Datensatz luftqualitaet.gpkg beinhaltet Messungen von Stickstoffdioxid \\(NO_2\\) aus dem Jahr 2015 für 97 Messstellen in der Schweiz. Stickstoffdioxid entstehen beim Verbrennen von Brenn- und Treibstoffen, insbesondere bei hohen Verbrennungstemperaturen, wobei der Strassenverkehr als Hauptquelle gilt. Mehr Informationen dazu findet ihr hier\n\n\n\n\n\n\nFigure 1: Eine solche Visualisierung zeigt dir beispielsweise die räumliche Ausdehnung der Datenpunkte\n\n\n\nAufgabe 2\nAls erstes berechnen wir die G-Function für die Rotmilanpositionen:\nSchritt 1:\nMit st_distance() können Distanzen zwischen zwei sf Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn.\n\n\nrotmilan_distanzmatrix <- st_distance(rotmilan)\n\nnrow(rotmilan_distanzmatrix)\n\n\n[1] 2305\n\nncol(rotmilan_distanzmatrix)\n\n\n[1] 2305\n\n# zeige die ersten 6 Zeilen und Spalten der Matrix\n# jeder Wert ist 2x vorhanden (vergleiche Wert [2,1] mit [1,2])\n# die Diagonale ist die Distanz zu sich selber (gleich 0)\nrotmilan_distanzmatrix[1:6,1:6] \n\n\nUnits: [m]\n         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]\n[1,]     0.00 14362.044 20272.492 35596.07 52519.10 64156.67\n[2,] 14362.04     0.000  8149.486 29752.74 44809.10 53775.25\n[3,] 20272.49  8149.486     0.000 22580.04 36848.93 45662.55\n[4,] 35596.07 29752.737 22580.037     0.00 17223.26 31439.57\n[5,] 52519.10 44809.096 36848.926 17223.26     0.00 16499.19\n[6,] 64156.67 53775.250 45662.554 31439.57 16499.19     0.00\n\nSchritt 2\nNun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn beträgt, also die kürzeste Distanz pro Zeile. Bevor wir diese ermitteln müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer 0. Danach kann mit apply() eine Funktion (FUN = min) über die Zeilen (MARGIN = 1) einer Matrix (X = rotmilan_distanzmatrix) gerechnet werden. Zusätzlich müssen wir noch na.rm = TRUE setzen, damit NA Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix.\n\n\ndiag(rotmilan_distanzmatrix) <- NA # entfernt alle diagonalen Werte\n\nrotmilan_distanzmatrix[1:6,1:6] \n\n\nUnits: [m]\n         [,1]      [,2]      [,3]     [,4]     [,5]     [,6]\n[1,]       NA 14362.044 20272.492 35596.07 52519.10 64156.67\n[2,] 14362.04        NA  8149.486 29752.74 44809.10 53775.25\n[3,] 20272.49  8149.486        NA 22580.04 36848.93 45662.55\n[4,] 35596.07 29752.737 22580.037       NA 17223.26 31439.57\n[5,] 52519.10 44809.096 36848.926 17223.26       NA 16499.19\n[6,] 64156.67 53775.250 45662.554 31439.57 16499.19       NA\n\nrotmilan_mindist <- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE)\n\n\n\nSchritt 3\nNun müssen wir die Distanzen nach ihrer Grösse sortieren\n\n\nrotmilan_mindist <- sort(rotmilan_mindist) \n\n\n\nSchritt 4\nJetzt berechnen wir die kummulierte Häufigkeit von jeder Distanz berechnen. Die kummulierte Häufikgeit vom ersten Wert ist 1 (der Index des ersten Wertes) dividiert durch die Anzahl Werte insgesamt. Mit seq_along erhalten wir die Indizes aller Werte, mit lenth die Anzahl Werte insgesamt.\n\n\nkumm_haeufgikeit <- seq_along(rotmilan_mindist) / length(rotmilan_mindist)\n\n\n\nSchritt 5\nNun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: Empirical Cumulative Distribution Function, ECDF) darstellen. Dafür müssen wir die beiden Vektoren zuerst noch in einen Dataframe packen, damit ggplot damit klar kommt.\n\n\nrotmilan_mindist_df <- data.frame(distanzen = rotmilan_mindist,\n                                  kumm_haeufgikeit = kumm_haeufgikeit)\n\n\n\np <- ggplot() + \n  geom_line(data = rotmilan_mindist_df, aes(distanzen, kumm_haeufgikeit)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\")\n\np\n\n\n\n\nLesehilfe:\n\n\n\nAufgabe 3\nFühre nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots.\n\n\n\nMusterlösung\n\nR-Code\n\n\n\n\n",
    "preview": "rauman/RaumAn2_Uebung_A/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "rauman/RaumAn2_Uebung_B/",
    "title": "Übung B: Räumliche Interpolation",
    "description": "Räumliche Interpolationen",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn2"
    ],
    "contents": "\n\nContents\nAufgabe 1: Raeumliche Interpolation mit IDW\nAufgabe 2: Interpolation mit Nearest Neighbour\nMusterlösung\n\n\n\n\nIn dieser Übung geht es darum, zwei verschiedene Interpolationsverfahren in R umzusetzen. Im ersten Interpolationsverfahren verwenden wir die inverse distance weighted interpolation, später verwenden wir die nearest neighbour methode. Dazu braucht ihr die folgenden Packages:\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nlibrary(gstat) # <- ggf. installieren!\n\n\n\nWeiter benötigt ihr die nachstehenden Datensätze:\n\nschweiz.gpkg\nluftqualitaet.gpkg\n\nDie Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch die inverse distance weighted Methode. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.\nWir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Wenn ihr mit unserer Function arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\n\nmy_idw <- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){\n  library(gstat)\n  library(sf)\n  \n  if(is.null(extent)){\n    extent <- groundtruth\n  }\n  \n  samples <- st_make_grid(extent,cellsize,what = \"centers\")\n  my_formula <- formula(paste(column,\"~1\"))\n  idw_sf <- gstat::idw(formula = my_formula,groundtruth, newdata = samples, nmin = 1, nmax = nmax, maxdist = maxdist, idp = idp)\n  \n  idw_matrix <- cbind(as.data.frame(st_coordinates(idw_sf)),pred = st_drop_geometry(idw_sf)[,1])\n  idw_matrix\n}\n\n\n\nNun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren.\n\n\nmy_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 10000, extent = schweiz)\n\n\n\nFolgende Parameter stehen Euch zur Verfügung:\nNotwendige Parameter:\ngroundtruth: Punktdatensatz mit den Messwerten (sf-Objekt)\ncolumn: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen)\ncellsize: Zellgrösse des output Rasters\n\nOptionale Parameter\nnmax: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: Inf (alle Werte im gegebenen Suchradius)\nmaxdist: Suchradius, welcher für die Interpolation verwendet werden soll. Default Inf (alle Werte bis nmax)\nidp: Inverse Distance Power: die Potenz, mit der der Nenner gesteigert werden soll. Default: 2. Werte werden im Kehrwert des Quadrates gewichtet: \\(\\frac{1}{dist^{idp}}\\).\nextent: Gebiet, für welches die Interpolation durchgeführt werden soll. Wenn nichts angegeben wird (Default NULL), wird die Ausdehnung von groundtruth verwendet.\n\nOuput\nder Output der Funktion ist eine data.frame mit 3 Spalten:\nX, Y Koordinaten der interpolierten Werte\npred: der Interpolierte Wert\n\n\nBeim Output handelt sich hier um einen Raster-ähnlichen Datentyp (siehe Vorlesung Spatial DataScience 1). Diesen können wir mit geom_raster mit ggplot visualisieren. Dafür müsst ihr in aes die X und Y Koordinaten angeben, und der interpolierte Wert mit fill einfärben.\nAufgabe 1: Raeumliche Interpolation mit IDW\nRechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate. Experimentiert mit nmax sowie maxdist. Was stellt ihr fest?\nTips:\nWas für Distanzen bei maxdist Sinn machen, könnt ihr dem Output aus der G-Funktion (vorherige Übung) entnehmen\nWählt am Anfang eine etwas Konvervative (grosse) cellsize und verringert diesen nur wenn euer Rechner damit gut klar kommt\nDa der Output aus der Interpolation im gleichen Koordinatenbezugssystem sind wie schweiz.gpkg kann man diese beiden Datensätze im gleichen ggplot darstellen. Dafür müsst ihr die aesthetics (aes()) für jeden Layer einzeln setzen, und nicht auf der Ebene von ggplot().\n\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n\n\nFigure 1: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz mit der Inverse Distance Weighted Methode. Die verschiedenen Plots zeigen die Veränderung der Interpolation bei steigendem IDP-Wert\n\n\n\nAufgabe 2: Interpolation mit Nearest Neighbour\nEine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. sf liefert dazu die Funktion st_voronoi(), die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: sf möchte für jedes Feature, also für jede Zeile in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher luftqualitaet mit st_union() von einem POINT in ein MULTIPOINT Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind.\n\n\nluftqualitaet_union <- st_union(luftqualitaet)\n\nthiessenpolygone <- st_voronoi(luftqualitaet_union)\n\n\n\n\n\n\nst_voronoi hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit st_intersection() clippen. Auch hier braucht es zwei kleine Vorverarbeitungsschritte:\nwie vorher müssen wir die einzelnen Kantons-Polygone miteinander verschmelzen. Dies erreichen wir mit st_union(). Wir speichern den Output als schweiz, was als Resultat ein einzelnes Polygon der Schweizergrenze retourniert.\nfür die Thiessen-Polygone machen wir genau das Umgekehrte: st_voronoi() liefert ein einzelnes Feature mit allen Polygonen, welches sich nicht gerne clippen lässt. Mit st_cast() wird die GEOMETRYCOLLECTION in Einzelpolygone aufgeteilt.\n\n\nthiessenpolygone <- st_cast(thiessenpolygone)\n\nthiessenpolygone_clip <- st_intersection(thiessenpolygone,schweiz)\n\n\n\n\n\n\nJetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch st_join. Auch hier ist noch ein kleiner Vorverarbeitungsschritt nötig: Wir konvertieren das sfc Objekt (nur Geometrien) in ein sf Objekt (Geometrien mit Attributtabelle).\n\n\nthiessenpolygone_clip <- st_as_sf(thiessenpolygone_clip)\nthiessenpolygone_clip <- st_join(thiessenpolygone_clip,luftqualitaet)\n\n\n\n\n\nggplot() + \n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, aes(fill = value)) +\n  geom_sf(data = luftqualitaet) +\n  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\"))) +\n  theme_void() +\n  theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\nFigure 2: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz nach der Nearest Neighbour Methode.\n\n\n\nMusterlösung\n\nR-Code\n\n\n\n\n",
    "preview": "rauman/RaumAn2_Uebung_B/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 2362,
    "preview_height": 2125
  },
  {
    "path": "rauman/RaumAn2_Uebung_C/",
    "title": "Übung C: Dichteschätzung",
    "description": {},
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn2"
    ],
    "contents": "\n\nContents\nAufgabe 1: Rotmilan Bewegungsdaten visualisieren\nAufgabe 2: Kernel Density Estimation berechnen\nAufgabe 3: Dichteverteilung mit Thiessen Polygonen\nMusterlösung\n\nNun wollen wir für die bereits verwendeten Datensätze luftqualitaet.gpkg und rotmilan.gpkg Dichteschätzungen durchführen. Ladet dafür die notwendigen Package und ladet bei Bedarf die Datensätze herunter.\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nlibrary(MASS) # <- ggf. installieren!\n\n\n\n\nschweiz.gpkg\nluftqualitaet.gpkg\nrotmilan.gpkg\n\nAufgabe 1: Rotmilan Bewegungsdaten visualisieren\nDie erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. Erstellt zur Beantwortung dieser Frage nachstehende Karte.\n\n\n\nAufgabe 2: Kernel Density Estimation berechnen\nIn einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016).\nÄhnlich wie beim IDW sind auch die verfügbaren KDE-Funktionen in R etwas kompliziert in der Handhabung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen.\nHier gilt das gleiche wie schon bei der Funktion my_idw(): Wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Funktion zu verzichten und stattdessen direkt MASS zu verwenden. Wenn ihr mit unserer Funktion arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\n\nmy_kde <- function(points,cellsize, bandwith, extent = NULL){\n  library(MASS)\n  library(sf)\n  library(tidyr)\n  if(is.null(extent)){\n    extent_vec <- st_bbox(points)[c(1,3,2,4)]\n  } else{\n    extent_vec <- st_bbox(extent)[c(1,3,2,4)]\n  }\n  \n  n_y <- ceiling((extent_vec[4]-extent_vec[3])/cellsize)\n  n_x <- ceiling((extent_vec[2]-extent_vec[1])/cellsize)\n  \n  extent_vec[2] <- extent_vec[1]+(n_x*cellsize)-cellsize\n  extent_vec[4] <- extent_vec[3]+(n_y*cellsize)-cellsize\n\n  coords <- st_coordinates(points)\n  mat <- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec)\n\n  mydf <- as.data.frame(mat[[3]])\n  \n  colnames(mydf) <- mat[[2]]\n  mydf$X <- mat[[1]]\n  \n  pivot_longer(mydf, -X,names_to = \"Y\",names_transform = list(Y = as.numeric))\n\n}\n\n\n\nDie Parameter der Funktion sollten relativ klar sein:\npoints: Ein Punktdatensatz aus der Class sf\ncellsize: Die Zellgrösse des output-Rasters\nbandwith: Der Suchradius für die Dichteberechnung\nextent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt.\nWenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir ein data.frame mit X und Y Koordinaten sowie eine Spalte value zurück. Nutzt diese drei Spalten mit geom_raster() um eure Daten mit ggplot zu visualisieren (aes(x = X, y = Y, fill = value).\n\n\nrotmilan_kde <- my_kde(points = rotmilan,cellsize = 1000, bandwith = 10000, extent = schweiz)\n\nrotmilan_kde\n\n\n# A tibble: 77,129 × 3\n          X        Y value\n      <dbl>    <dbl> <dbl>\n 1 2485410. 1075268.     0\n 2 2485410. 1076268.     0\n 3 2485410. 1077268.     0\n 4 2485410. 1078268.     0\n 5 2485410. 1079268.     0\n 6 2485410. 1080268.     0\n 7 2485410. 1081268.     0\n 8 2485410. 1082268.     0\n 9 2485410. 1083268.     0\n10 2485410. 1084268.     0\n# … with 77,119 more rows\n\n\n\n\nDie Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wie erwähnt sind Wissenschaftler häufig nur an den höchsten 95% der Werte interessiert. Folge folgende Schritte um das Resultat etwas besser zu verantschaulichen:\nBerechne die 95. Perzentile aller Werte mit der Funktion quantile und benne diesen q25\nErstelle eine neue Spalte in rotmilan_kde, wo alle Werte tiefer als q25 NA entsprechen\n(Optional): Transformiere die Werte mit log10, um einen differenzierteren Farbverlauf zu erhalten\nWir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Werte und nutzen diesen Wert als “Grenzwert” für die Darstellung.\nZusätzlich hilft eine logarithmische Transformation der Werte, die Farbskala etwas sichtbarer zu machen.\n\n\n\nAufgabe 3: Dichteverteilung mit Thiessen Polygonen\nThiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. Nutze die Anleitung für das Erstellen von Thiessenpolygonen aus der Übung B um Thiessenpolygone für die Rotmilanpositionen zu erstellen.\n\n\n\n\n\n\nFigure 1: Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht.\n\n\n\nMusterlösung\n\nR-Code\n\n\n\n\n",
    "preview": "rauman/RaumAn2_Uebung_C/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "rauman/RaumAn3_Uebung_A/",
    "title": "Übung \"Morans I\"",
    "description": "Heute berechnen wir Moran's I auf verschiedenen Datensätzen und vergleichen die Resultate.",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "RaumAn3"
    ],
    "contents": "\n\nContents\nAufgabe 1: Herleitung der Formel\nBruch 1\nBruch 2\nAuflösung der Formel\n\nAufgabe 2: Morans I für Gemeinde oder Bezirke berechnen\n\n\nFür die Berechnung von Morans \\(I\\) benutzen wir kein externes Package, sondern erarbeiten uns alles selber, basierend auf der Formel von Moran’s \\(I\\):\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nDiese sieht sehr beeindruckend aus, aber wenn wir die Formel in ihre Einzelbestandteile aufteilen, sehen wir, dass diese in sich gar nicht so komplex sind.\nAls erster Schritt müssen wir die notwendigen Libraries und Geodaten laden:\n\nzweitwohnungsinitiative.gpkg\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\n# Das Geopackage beinhaltet 3 Layers (siehe st_layers(\"zweitwohnungsinitiative.gpkg\"))\n# In jedem Layer sind die Abstimmungsresultate auf eine andere politische Ebene\n# aggregiert. Wir started mit der Aggregationsstufe \"kanton\"\nzweitwohnung_kanton<- read_sf(\"zweitwohnungsinitiative.gpkg\", \"kanton\")\n\n\n\n\n\n\nAufgabe 1: Herleitung der Formel\nIn der ersten Übung wollen wir Moran’s \\(I\\) für eine gegebene Choroplethenkarte nachrechnen. Dazu nehmen wir die Formel für Moran’s \\(I\\) und zerlegen sie in Einzelteile, die wir dann Schritt für Schritt für unsere Daten berechnen. So teilen wir ein vermeintlich komplexes Problem in überschaubare Einzelteile. Dieses Vorgehen illustriert ausserdem sehr schön ein generelles Data Science Prinzip. Divide and Conquer - Teile und Herrsche: Teile ein komplexes Problem in kleinere, beherrschbare Unterprobleme. Wir beginnen mit dem ersten Bruch und berechnen dabei zuerst den Zähler, dann dem Nenner. So können wir den Bruch auflösen und uns danach dem zweiten Bruch zuwenden:\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nBruch 1\nWidmen wir uns dem ersten Bruch:\n\\[\\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nZähler (von Bruch 1)\nBeginnen wir mit dem Zähler, \\(n\\). Dies ist lediglich die Anzahl Messwerte in unserem Datensatz, also die Anzahl Kantone.\n\n\nn <- nrow(zweitwohnung_kanton)\nn\n\n\n[1] 26\n\nNenner (von Bruch 1)\nDer Nenner des ersten Bruches (\\({\\sum_{i=1}^n (y_i - \\bar{y})^2}\\)) ist sehr ähnlich der Berechnung der Varianz:\nBerechne den Durchschnitt aller Messwerte (\\(\\bar{y}\\))\nBerechne für jeden Messwert die Differenz zum Durchschnitt (\\(y_i - \\bar{y}\\))\nQuadriere diese Werte \\((y_i - \\bar{y})^2\\)\nSummiere die Quadrierten Werte \\(\\sum_{i=1}^n\\)\nAlso berechnen wir zuerst diese Differenzwerte (Messwert minus Mittelwert):\n\n\n# Die Werte aller Kantone:\ny <- zweitwohnung_kanton$ja_in_percent\n\n# Der Durchschnittswert aller Kantone\nybar <- mean(y, na.rm = TRUE)\n\n# von jedem Wert den Durchschnittswert abziehen:\ndy <- y - ybar\n\n\n\nWelche dieser Zwischenresultate sind Einzelwerte und welche Vektoren? Nun quadrieren wir die Differenzen:\n\n\ndy_2 <- dy^2\n\n\n\nund summieren die Differenzen:\n\n\ndy_sum <- sum(dy_2, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 1)\nBeschliessen wir die Bearbeitung des ersten Bruchs indem wir den Zähler durch den Nennen dividieren: n durch dy_sum.\n\n\nvr <- n/dy_sum\n\n\n\nBruch 2\nWenden wir uns nun also dem Bruches der Formel zu.\n\\[\\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nHier berechnen wir die Summe aller Gewichte sowie die gewichteten Covarianzen. Wir betrachten immer Messwertpaare, sprich paarweise Vergleiche zweier Raumeinheiten (hier Kantone). Deshalb haben die zwei Summenzeichen die beiden unterschiedlichen Laufvariablen (\\(i\\) und \\(j\\)). Solche paarweise Vergleiche von Werten mit allen anderen Werten können wir elegant mit Kreuzmatrizen abbilden. In der Kreuzmatrix vergleichen wir jeden Messwert mit allen anderen Messwerten. Dabei gibt es zwei Kreuzmatrizen: (\\(w_{ij}\\) ist die erste Kreuzmatrix, \\((y_i - \\bar{y})(y_j - \\bar{y})\\) ist die zweite Kreuzmatrix).\nZähler (Bruch 2)\nDer erste Term, \\(w_{ij}\\), beschreibt die räumlichen Gewichte aller Kantone. Sind die Kantone benachbart, dann gilt ein Gewicht von 1, sind sie nicht benachbart, gilt ein Gewicht von 0. Dies entspricht dem Schalter aus der Vorlesung.\nWie wir “benachbart” definieren ist nicht festgelegt. Denkbar wären zum Beispiel folgende Optionen:\nDie Kantone müssen sich berühren (dürfen sich aber nicht überlappen): st_touches()\nDie Kantone müssen innerhalb einer bestimmten Distanz zueinander liegen: st_is_within_distance()\nDie Kantone müssen überlappen: st_overlaps()\nEgal für welche Variante Ihr Euch entscheidet, setzt sparse = FALSE damit eine Kreuzmatrix erstellt wird.\n\n\nw <- st_touches(zweitwohnung_kanton, sparse = FALSE)\n\nw[1:6, 1:6]\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n[2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n[3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n(Lasst Euch nicht davon beirren, dass wir nun TRUE und FALSE statt 1 und 0 haben. In R sind TRUE und 1 äquivalent, sowie auch FALSE und 0).\n\n\n\nZur Überprüfung unserer Operation: Mit w[1,] bekommt ihr ein Vektor, wo bei allen Kantone, die den ersten kanton (Zürich) berühren TRUE steht und bei allen anderen FALSE. Nun können wir überprüfen, ob die räumliche Operation funktioniert hat.\n\n\nberuehrt_1 <- w[1, ]\n\nggplot(zweitwohnung_kanton[beruehrt_1, ]) +\n  geom_sf(aes(fill = KANTONSNAME)) +\n  labs(title = \"Welche Kanton berühren den Kanton Zürich (st_touches)\")\n\n\n\n\nDer nächste Teil sollte Euch nun bekannt vorkommen. Die Differenz aller Werte vom Mittelwert aller Werte \\((y_i - \\bar{y})\\) kennen wir schon vom ersten Bruch und haben wir auch bereits gelöst. Nun gilt es paarweise das Produkt der Abweichungen vom Mittelwert (die Covarianz) zu berechnen \\((y_i - \\bar{y})(y_j - \\bar{y})\\). DAzu müssen wir das Produkt aller Wertekombinationen berechnen. Dies erreichen wir mit der Funktion tcrossprod():\n\n\npm <- tcrossprod(dy)\npm[1:6,1:6]\n\n\n              [,1]         [,2]          [,3]         [,4]\n[1,]  0.0008822509  0.001610338 -0.0006482922 -0.003262762\n[2,]  0.0016103377  0.002939286 -0.0011833021 -0.005955390\n[3,] -0.0006482922 -0.001183302  0.0004763756  0.002397530\n[4,] -0.0032627617 -0.005955390  0.0023975299  0.012066424\n[5,] -0.0019909878 -0.003634071  0.0014630099  0.007363119\n[6,] -0.0023965442 -0.004374317  0.0017610193  0.008862958\n             [,5]         [,6]\n[1,] -0.001990988 -0.002396544\n[2,] -0.003634071 -0.004374317\n[3,]  0.001463010  0.001761019\n[4,]  0.007363119  0.008862958\n[5,]  0.004493089  0.005408314\n[6,]  0.005408314  0.006509967\n\nNun multiplizieren wir die Covarianzen mit den Gewichten \\(w\\) (Schalter), damit wir nur noch die Werte von den Kantonen haben, die auch effektiv benachbart sind (und eliminieren nicht-benachbarte Werte). Beachtet dass wir hier nun eine Matrix mit einer Matrix multiplizieren.\n\n\npmw <- pm * w\nw[1:6,1:6]\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n[2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n[3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\npmw[1:6,1:6]\n\n\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,]  0.000000000  0.000000000  0.000000000  0.000000000 -0.001990988\n[2,]  0.000000000  0.000000000 -0.001183302 -0.005955390  0.000000000\n[3,]  0.000000000 -0.001183302  0.000000000  0.000000000  0.001463010\n[4,]  0.000000000 -0.005955390  0.000000000  0.000000000  0.007363119\n[5,] -0.001990988  0.000000000  0.001463010  0.007363119  0.000000000\n[6,]  0.000000000 -0.004374317  0.001761019  0.008862958  0.000000000\n             [,6]\n[1,]  0.000000000\n[2,] -0.004374317\n[3,]  0.001761019\n[4,]  0.008862958\n[5,]  0.000000000\n[6,]  0.000000000\n\nDen Zähler des ersten Bruches können wir nun fertig berechnen, indem wir die Summe aller gewichten (sprich eingeschalteten) Werten bilden:\n\n\nspmw <- sum(pmw, na.rm = TRUE)\nspmw\n\n\n[1] 0.200995\n\nNenner (Bruch 2)\nFür den Nenner des zweiten Teils der Formal (des zweiten Bruchs) müssen wir nun nur noch alle Gewichte summieren. Diese Summer entspricht der Anzahl effektiv benachbarter Kantone und kann Anzahl der \\(TRUE\\)-Werte in \\(w\\) bestimmt werden.\n\n\nsmw <- sum(w, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 2)\nSo können wir den zweiten Bruch auflösen und berechnen:\n\n\nsw  <- spmw / smw\n\n\n\nAuflösung der Formel\nDer allerletzte Schritt besteht darin, die Werte aus den beiden Brüche miteinander zu multiplizieren.\n\n\nMI <- vr * sw\nMI\n\n\n[1] 0.3117271\n\nDer Global Morans \\(I\\) für die Abstimmungsdaten beträgt auf Kantonsebene also 0.3117271. Wie interpretiert ihr dieses Resultate? Was erwartet ihr für eine Resultat auf Gemeinde- oder Bezirksebene?\nAufgabe 2: Morans I für Gemeinde oder Bezirke berechnen\nNun könnt ihr Morans \\(I\\) auf der Ebene der Gemeine oder Bezirke und untersuchen, ob und wie sich Morans \\(I\\) verändert. Wenn ihr einen wenig leistungsfähigen Rechner habt, berechnet verwendet besser die Ebene “Berzirke”. Importiert dazu den Layer bezrik oder gemeinde aus dem Datensatz zweitwohnungsinitiative.gpkg. Visualisiert in einem ersten Schritt die Abstimmungsresultate.\n\n\nzweitwohnung_gemeinde <- read_sf(\"zweitwohnungsinitiative.gpkg\", \"gemeinde\")\n\nggplot(zweitwohnung_gemeinde) +\n  geom_sf(aes(fill = ja_in_percent), colour = \"white\",lwd = 0.2) +\n  scale_fill_gradientn(\"Ja Anteil\",colours = RColorBrewer::brewer.pal(11, \"RdYlGn\"), limits = c(0,1)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "rauman/RaumAn3_Uebung_A/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 960
  },
  {
    "path": "rauman/RaumAn4_Uebung_A_AHP/",
    "title": "Analytical Hierarchy Process (AHP)",
    "description": "This is a step-by-step guide corresponding to this week's lecture.",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      },
      {
        "name": "Dominic Lüönd",
        "url": {}
      }
    ],
    "date": "2021-12-07",
    "categories": [
      "RaumAn4"
    ],
    "contents": "\n\nContents\nExercise 1: Define initial situation\nExercise 2: Pairwise comparison (Paarweiser Vergleich 1 & 2)\nExercise 3: Calculation of the criteria weights\nExercise 3.1: Normalization of matrix (Berechnung der Kritiriengewichte 1)\nExercise 3.2: Weighting of criteria (Berechnung der Kritiriengewichte 2)\n\nExercise 4: Consistency analysis (Konsistenzanalyse 1 & 2)\nCongratulations!\n\nNow that you have learned the theory, you will carry out concrete example of an Analytical Hierarchy Process (AHP). This is a manual approach to show you the basics of an AHP. If you want to build a more complex AHP you can use specific R AHP packages such as the ahpsurvey package.\nExercise 1: Define initial situation\nFirst think of an actual decision you are currently facing or have previously faced (e.g. buying a bicycle or renting an apartment) and define the following points.\nA goal for your AHP (e.g. Buy a bike)\n4 criteria on which you want to base your decision (e.g. Price, Distance to Work / School, Size, Scenic Beauty)\n3 different options / alternatives (e.g. 3 different apartments)\n\n\n\nExercise 2: Pairwise comparison (Paarweiser Vergleich 1 & 2)\nIn a first step each criterion needs to be compared with another criteria in pairs. Use the following scale for weighting the criteria (see table 1) .\n\nTable 1: Scale for weighting the criteria.\nRating\nDefinition\n1\nThe two characteristics are equally important\n3\nCriteria A is slightly more important than criteria B\n5\nCriteria A is moderately more important than criteria B\n7\nCriteria A is strongly more important than criteria B\n9\nCriteria A is absolutely more important than criteria B\n2, 4, 6, 8\nIntermediate Values\n\nYou can use the following code to create your weighting matrix. In the matrix, two criteria are always compared twice, and these two comparisons should be the reciprocal (“Kehrwert”) of each other. To illustrate this, we have added one comparison which reads as follows:\nRow 1, column 2: Criteria 1 is slightly more important than Criteria 2\nRow 2, column 1: Criteria 2 is slightly less important than Criteria 1\nCreate this matrix comparison matching your criteria, replacing the 0 values with your weights according to table 1. Note that all diagonal values should equal to 1.\n\n\npairwise_comparison <- c(\n  1,   3, 0, 0,\n  1/3, 1, 0, 0,\n  0,   0, 1, 0,\n  0,   0, 0, 1\n) %>% matrix(ncol = 4, byrow = TRUE) \n\n\n\n\n\n\nTip: Add column and row names so your matrix is more readable.\n\n\ncriterias <- c(\"price\", \"distance\",\"size\", \"beauty\")\n\nrownames(pairwise_comparison) <- criterias\ncolnames(pairwise_comparison) <- criterias\n\n\n\nExercise 3: Calculation of the criteria weights\nExercise 3.1: Normalization of matrix (Berechnung der Kritiriengewichte 1)\nIn the next step the matrix needs to be normalized (see figure 1. You can do this in the following two steps:\nCalculate the sum of each column using colSums. Store the output in a variable (e.g. ahp_colsums).\nDivide each value in the matrix by the corresponding column sum. To achieve this, you can use the sweep() function on the matrix, which is very similar to apply (use MARGIN = 2 (columns), STATS = ahp_colsums and FUN = \"/\").\n\n\n\n\n\n\nFigure 1: Normalizing the criteria so that each column equals to 1\n\n\n\nExercise 3.2: Weighting of criteria (Berechnung der Kritiriengewichte 2)\nThis is the final step to calculate the weight of each criteria (see 2). To do so:\ncalculate the sum of each row and store the output in a variable (e.g. criteria_sum).\ndivide the criteria_sum by the sum of criteria_sum and store the output in a variable (e.g. criteria_weight).\nNote: The sum of criteria_weight should equal to 1\n\n\n\nFigure 2: Normalizing the weights so that the sum of all weights equals to 1\n\n\n\nExercise 4: Consistency analysis (Konsistenzanalyse 1 & 2)\nAfter the pairwise comparison is done, a consistency analysis needs to be performed to check whether the pairwise comparisons are consistent or include contradictions. A certain inconsistency is allowed within the framework of an AHP, but it should not be too great.\nTo calculate consistency, you should proceed as explained in Slide 30 (Konsistenzanalyse 1) and the following steps:\ndo a matrix multiplication (%*%) between pairwise_comparison and criteria_weight.\n\n\n\nDivide the result of 1) by criteria_weight\n\n\n\nCalculate \\(\\lambda_{max}\\) by dividing the result of 2) by the number of criteria\n\n\n\ncalculate \\(CI\\) (\\(CI = \\frac{\\lambda_{max} - n}{n-1}\\)), where n equals to the number of criteria\n\n\n\nDetermine \\(RI\\) by consulting the table 3\n\n\n\nCalculate \\(CR\\) (\\(CR = CI / RI\\))\n\n\n\nIf CR > 0.1, you will need to re-evaluate your pairwise comparisons.\n\n[1] TRUE\n\n\n\n\nFigure 3: Random index by Saaty\n\n\n\nCongratulations!\nYou now have determined the weights on top of which you can build your decision and have determined if these weights are consistent or not. These next steps are technically very similar to what you did in the exercise above, so we will leave it up to you if you want to complete these steps or not. For sake of completness, the next step would be to:\ncompare your options / alternatives with each other in a pairwise comparison (similar as to how you compared the criteria with each other). You do this for every criteria\nnormalize your pairwise comparisons of your options (similar as to how how you normalized the pairwise comparisons of the criteria)\nUse the weights you determined in the exercise above to weigh your results from 2)\nAsses the best decision based on the result from 3)\n\n\n\n",
    "preview": "rauman/RaumAn4_Uebung_A_AHP/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 2150,
    "preview_height": 1473
  },
  {
    "path": "rauman/RaumAn4_Uebung_B_Raster_Intro/",
    "title": "Introduction to raster datasets",
    "description": "How we can import a raster dataset using R",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      }
    ],
    "date": "2021-12-07",
    "categories": [
      "RaumAn4"
    ],
    "contents": "\nOne of the important aspects of the upcoming exercise (Multi-Criteria Evaluation (MCE)) is the use and manipulation of raster datasets. In R, two are the main packages used to handle raster data: terra and raster package. The latter is still heavily used, but inevitably is going to be replaced by the first one.\nBelow we will use terra to demonstrate how we can import a raster dataset. In the link below you can download a tif file, representing the Digital Elevation Model (Digitales Höhenmodell, DHM) of Canton Schwyz in Switzerland. Download the dataset and repeat the code provided.\n\ndhm25m.tif\n\n\n\nlibrary(terra)\n\n\n\nImport your raster with the function rast\n\n\ndhm_schwyz <- rast(\"dhm25m.tif\")\n\n\n\nYou get some important metadata on the raster dataset when you type the variable name in the console.\n\n\ndhm_schwyz \n\n\nclass       : SpatRaster \ndimensions  : 1513, 1888, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 671937.5, 719137.5, 193512.5, 231337.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903 / LV03 \nsource      : dhm25m.tif \nname        : dhm25m \n\nTo get a quick look at the raster dataset, we can simply use either of the following plot() function:\n\n\nplot(dhm_schwyz)\n\n\n\n\nUnfortunately, adding raster to ggplot is not very straightforward. Since ggplot is a universal plotting framework we quickly reach the limits of what is possible when creating something as specialized as maps. For this reason, we will introduce a new plotting framework which is specialized on maps and was built in a very similar design as ggplot: tmap. Install and load this package now.\n\n\nlibrary(tmap)\n\n\n\nJust as ggplot, tmap is based on the idea of “layers” that are joined using a +. Each layer has two components:\na dataset component which is always tm_shape(dataset) (replace dataset with your variable)\na geometry component which describes how the preceeding tm_shape() should be visualized. This can be tm_dots() for points, tm_polygons() for polygons, tm_lines() for lines etc. For single band raster (which is the case for dhm_schwyz), it is tm_raster()\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster() \n\n\n\n\nNote that tm_shape() and tm_raster() (in this case) belong together, one cannot live without the other.\nIf you consult the help of ?tm_raster you will see a multitude of options with which to change the visualisation of your data. For example, the default style of tm_raster() is to create “bins” of the data with a descrete colour scale. We can override this using style = \"cont\"\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\") \n\n\n\n\nThis already looks pretty awesome, but maby we want to change the default colour palette. Fortunately, this is much simpler in tmap than in ggplot2. To look at the available palettes, type tmaptools::palette_explorer() or RColorBrewer::display.brewer.all() in the console (the former might require you to install additional packages, e.g. shinyjs).\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\") \n\n\n\n\nYou can make layout adjustments using tm_layout(), check ?tm_layout to see all of the options available!\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\", legend.is.portrait = FALSE, title = \"\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"bottom\",frame = FALSE)\n\n\n\n\n\n\n\n",
    "preview": "rauman/RaumAn4_Uebung_B_Raster_Intro/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "rauman/RaumAn5_Uebung_MCE/",
    "title": "Multi-Criteria Evaluation (MCE)",
    "description": "This is a step-by-step guide corresponding to the PPP document \"Multikriterielle Entscheidungsanalyse 1\".",
    "author": [
      {
        "name": "Patrick Laube",
        "url": {}
      },
      {
        "name": "Nils Ratnaweera",
        "url": {}
      },
      {
        "name": "Nikolaos Bakogiannis",
        "url": {}
      },
      {
        "name": "Dominic Lüönd",
        "url": {}
      }
    ],
    "date": "2021-12-10",
    "categories": [
      "RaumAn5"
    ],
    "contents": "\nThe following exercise may seem familiar to some of you, if you have attended the bachelor program. This MCE was already conducted in the bachelor module “GIS” by using ArcGIS Pro and its ModelBuilder (see figure 1).\n\n\n\nFigure 1: Process model in ArcGIS Pro ModelBuilder\n\n\n\nThe goal of this exercise is to do the same MCE by just using R. We will mainly use functions from the R packages sf and raster. Please have a look at the process model (see figure 1) that was created in ArcGIS Pro and try to figure out which functions in the two R packages correspond to the ones in the model. You will see that there are some similar functions available, but for some calculations a different approach is needed.\nExercises 1: Load and view data\n\ndhm25m.tif\neis25m.tif\nwind25m.tif\nBewohnte_Flaeche.gpkg\nNationale_Schutzgebiete.gpkg\nWaldgebiete.gpkg\nSeeflaechen.gpkg\nStrassen.gpkg\nUntersuchungsgebiet_Schwyz.gpkg\n\nThe data can be loaded by using the functions terra for raster data and read_sf for vector data. View the available data layers (see Table ) and plot them in an appealing way. For visualization you can use the functions plot for raster data and ggplot for vector data.\nTip: As some data sets are exceeding the boundaries of the study area, you can use the crop function on raster data sets and st_intersection for sf/vector data sets, so only relevant data is included.\n\n\nTable 1: Data used in this study\n\n\nname\n\n\ndescription\n\n\ntype\n\n\nres\n\n\ngeometry\n\n\ncrs\n\n\nexclusion_area\n\n\ndhm25m.tif\n\n\nTerrain model (m)\n\n\nRaster\n\n\n25m\n\n\n\n\nCH1903/LV03\n\n\nNo\n\n\neis25m.tif\n\n\nIcing frequency (days/year)\n\n\nRaster\n\n\n25m\n\n\n\n\nCH1903/LV03\n\n\nNo\n\n\nwind25m.tif\n\n\nAverage wind speed (dm/s)\n\n\nRaster\n\n\n25m\n\n\n\n\nCH1903/LV03\n\n\nNo\n\n\nBewohnte_Flaeche.gpkg\n\n\nSettlements (incl. buffer 200m)\n\n\nGeopackage\n\n\n\n\nPolygon\n\n\nCH1903/LV03\n\n\n(Yes)/Distance\n\n\nNationale_Schutzgebiete.gpkg\n\n\nNational protection areas\n\n\nGeopackage\n\n\n\n\nPolygon\n\n\nCH1903/LV03\n\n\n(Yes)/Distance\n\n\nSeeflaechen.gpkg\n\n\nLake areas\n\n\nGeopackage\n\n\n\n\nPolygon\n\n\nCH1903/LV03\n\n\nYes\n\n\nStrassen.gpkg\n\n\nStreets\n\n\nGeopackage\n\n\n\n\nLine\n\n\nCH1903/LV03\n\n\nNo/Distance\n\n\nUntersuchungsgebiet_Schwyz.gpkg\n\n\nStudy area, canton of Schwyz\n\n\nGeopackage\n\n\n\n\nPolygon\n\n\nCH1903/LV03\n\n\nNo\n\n\nWaldgebiete.gpkg\n\n\nForest areas\n\n\nGeopackage\n\n\n\n\nPolygon\n\n\nCH1903/LV03\n\n\n(Yes)/Distance\n\n\n\n\n\nExercise 2: Merge exclusion criteria\nMerge the exclusion criteria settlement areas, national protected areas, lake areas and forest areas. These vector data sets are structured as data frames and therefore can be merged by simply combining them. Keep in mind that the data frames have different sizes. Additionally, we need to create a raster out of the newly created vector data set (exclusion area). For that you can use the function rasterize. The output should be a raster with 0 and 1, where the fields of the exclusion area have values of 0 and the remaining fields have values of 1 (see Figure (fig:figrasterizeexclusioncriteria)).\nTip: To achieve a raster with only 0 and 1 use the rasterize options field = 0 and background = 1.\nTip: In order to rasterize vector data, you need to create an empty raster beforehand. This raster should have the same boundaries (extent), resolution and coordinate system (crs) as the other raster sets. Use the following code to do so.\n\n\nr <- terra::rast(ext(kt_schwyz), \n          resolution = c(25, 25), \n          crs = \"EPSG:21781\")\n\n\n\n\n\n\nFigure 2: Exclusion area in the canton of Schwyz\n\n\n\n\n\n\nExercise 3: Calculate slope\nNext, calculate the slope in degrees based on the terrain model (dhm25m). The terra package gives you a very helpful function called terrain.\nTip: When using the terrain function use the following options: v=“slope”, unit=“degrees”, neighbors=8.\n\n\n\nFigure 3: Example output of the calculated slope\n\n\n\n\n\n\nExercise 4: Calculate distances to criteria\nWithin the evaluation of suitable sites for wind turbines, the distance to roads, forest areas, national protected areas and inhabited areas are relevant. Depending on the criteria, a short or long distance has a positive influence on the evaluation of potential sites. For this purpose, perform a distance analysis with the selected criteria raster layers. Use the raster function distance for this calculation.\nTip: In order to perform the distance function, you need to rasterize the criteria as well. You can use the same command as in exercise 2 but use only option field = 1.\nTip: Use the crop function again to get only relevant data in the study area.\nAs this process will take quite some time (about 2h), you only need to create the distance for the settlement data set, as the logic for the others stays the same. You can download the other 3 data by clicking on the respective links below:\n\nforests_ed.tif\nsettlements_ed.tif\nstreets_ed.tif\n\n\n\n\nFigure 4: Example output for the distances to settlements\n\n\n\n\n\n\nExercise 5: Standardize and grade criteria (grading)\nThe data layers slope, wind speed, icing frequency and the in exercise 4 calculated distance layers have different units (dm/s, degrees, d/yr and m). These units can’t be directly calculated with each other. Therefore, the different layers need to be operationalized by performing a linear grading. The linear grading is done by using the function reclassify. Use the standards for the reclassification in Figure (fig:figreclassify).\nTip: Keep in mind the min and max values of each raster layer.\nTip: Here is an example code to reclassify the distances to settlements.\n\n\nreclass_settlements <- c(0,80,0,\n                        80,160,0.1,\n                        160,240,0.2,\n                        240,320,0.3,\n                        320,400,0.4,\n                        400,480,0.5,\n                        480,560,0.6,\n                        560,640,0.7,\n                        640,720,0.8,\n                        720,800,0.9,\n                        800,minmax(settlements_ed),1.0) %>% matrix(ncol = 3, byrow = TRUE)\nreclass_settlements_ed <- terra::classify(settlements_ed, reclass_settlements)\n\n\n\n\n\n\nFigure 5: Reclassify values for all criteria\n\n\n\n\n\n\nFigure 6: Example output for the reclassified distances to settlements\n\n\n\n\n\n\nExercise 6: Weighting criteria with AHP\nPerform an AHP to weight the criteria underlying the MCE. First compare the criteria in pairs, and then calculate the weights - as you have learned in last week’s lesson. In the end you should have a list of 7 weights as shown below.\nTip: Check exercise 2 and 3 from last week’s session. Use the prepared R code to create your ahp matrix.\n      Wind       Streets          Ice    Settlements       Forest         Slope  Protected areas \n0.33862692    0.09816760    0.06166626    0.24969460    0.03515759    0.18043000      0.03625702 \n\n\nahp_matrix <- c(\n  1, 0, 0, 0, 0, 0, 0, #Wind\n  0, 1, 0, 0, 0, 0, 0, #Distance to streets\n  0, 0, 1, 0, 0, 0, 0, #Ice\n  0, 0, 0, 1, 0, 0, 0, #Distance to settlements\n  0, 0, 0, 0, 1, 0, 0, #Distance to forests\n  0, 0, 0, 0, 0, 1, 0, #Slope\n  0, 0, 0, 0, 0, 0, 1  #Distance to protected areas\n) %>% matrix(ncol = 7, byrow = TRUE)\n\n\n\n\n\n\nExercise 7: Weighted Overlay\nThe linearly ranked criteria (Exercise 5) are now to be combined with each other, taking into account the weighting determined using the AHP (Exercise 6). This weighted overlay can be performed by using raster calculations and simply multiplying each criteria with its weight and adding them together (see figure 7).\nTip: As the raster sets have slightly different origins, increase the tolerance by using rasterOptions(tolerance = 0.5).\nTip: Also keep in mind the order of the weights in your list when doing the multiplication.\n\n\n\nFigure 7: Weighted overall of all criteria\n\n\n\n\n\n\nExercise 8: Intersecting potential areas with exclusion criteria\nBy simply multiplying the exclusion layer (result from exercise 2) with the weighted overlay layer (result from exercise 7) we are excluding all areas with value 0 (exclusion areas) and keeping all other areas with value 1 (e.g. 0x3=0, 1x3=3). As a conclusion of the study, create a final plot of the potential areas for wind power plants in the canton of Schwyz (like figure 8). Furthermore, discuss the results obtained and determine three possible locations within which concrete planning of wind power plants would be possible.\n\n\n\nFigure 8: Potential sites for wind power plants in Kt. Schwyz (red = less suitable, green = more suitable)\n\n\n\n\n\n\n\n\n\n",
    "preview": "rauman/RaumAn5_Uebung_MCE/distill-preview.png",
    "last_modified": "2021-12-10T14:35:19+00:00",
    "input_file": {},
    "preview_width": 2916,
    "preview_height": 1732
  }
]
