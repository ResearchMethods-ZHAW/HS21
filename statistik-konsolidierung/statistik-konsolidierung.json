[
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_Demo_assoziationen/",
    "title": "Konsolidierung 1: Demo Assoziationen",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\nKonsolidierung 1: Demo Assoziationen\n\nDownload R-Skript\n\n\n\n#lade Packages\n\nlibrary(tidyverse)\n\n\n#mytheme\nmytheme <- \n  theme_classic() + \n  theme(\n    axis.line = element_line(color = \"black\"), \n    axis.text = element_text(size = 20, color = \"black\"), \n    axis.title = element_text(size = 20, color = \"black\"), \n    axis.ticks = element_line(size = 1, color = \"black\"), \n    axis.ticks.length = unit(.5, \"cm\")\n  )\n\n\n\n\n\n\n#lade Daten\n# mehr Info darüber: https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# betrachte die Daten\nsummary(cars)\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb          vs_cat         \n Min.   :0.0000   Min.   :3.000   Min.   :1.000   Length:32         \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   Class :character  \n Median :0.0000   Median :4.000   Median :2.000   Mode  :character  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812                     \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000                     \n Max.   :1.0000   Max.   :5.000   Max.   :8.000                     \n    am_cat         \n Length:32         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nglimpse(cars)\n\n\nRows: 32\nColumns: 13\n$ mpg    <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8,~\n$ cyl    <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4,~\n$ disp   <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.~\n$ hp     <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 1~\n$ drat   <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92,~\n$ wt     <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.19~\n$ qsec   <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.0~\n$ vs     <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,~\n$ am     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ gear   <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4,~\n$ carb   <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1,~\n$ vs_cat <chr> \"normal\", \"normal\", \"v-type\", \"v-type\", \"normal\", \"v-~\n$ am_cat <chr> \"manual\", \"manual\", \"manual\", \"automatic\", \"automatic~\n\n\n#Assoziation zwischen Anzahl Zylinder und Motorentyp ()\ntable(cars$vs_cat, cars$am_cat) # Achtung: sieht aus, als gäbe es weniger V-Motoren bei den handgeschalteten Autos\n\n\n        \n         automatic manual\n  normal        12      6\n  v-type         7      7\n\n\n#lass und das überprüfen\n#achtung: bei chi-square test kommt es sehr auf das format drauf an (er erwartet entweder vektoren oder eine matrix!)\n\n#exkurs um in es in ein matrix form zu bringen\nchi_sq_matrix <- xtabs(~ vs_cat + am_cat, data = as.data.frame(cars)) # in diesem Spezialfall haben wir keine Kriteriumsvariable\n\n#1.version\nchisq.test(chi_sq_matrix)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  chi_sq_matrix\nX-squared = 0.34754, df = 1, p-value = 0.5555\n\n\n#2. version\nchi_sq <- chisq.test(cars$am_cat, cars$vs_cat)\n\n#resp. fisher exacter test verwenden, da 2x2 table\nfisher.test(chi_sq_matrix)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  chi_sq_matrix\np-value = 0.4727\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.3825342 10.5916087\nsample estimates:\nodds ratio \n  1.956055 \n\n\n#fisher exakter test\nfisher.test(cars$am_cat, cars$vs_cat)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cars$am_cat and cars$vs_cat\np-value = 0.4727\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.3825342 10.5916087\nsample estimates:\nodds ratio \n  1.956055 \n\n\n\n#visualisieren: kudos goes to https://mgimond.github.io/Stats-in-R/ChiSquare_test.html#3_two_factor_classification\nOP <- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi_sq$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi_sq$expected, cex.axis =1 , main = \"Expected counts\\n(if class had no influence)\")\n\n\n\npar(OP)\n\n\n\n\nmöglicher Text für Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Art des Motors und Art des Fahrwerks statistisch nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen den Variablen “VS” und “AM - Transmission” (\\(\\chi^2\\)(1) = 0.348, p = 0.556. Der Fisher exacter Test bestätigt diesen Befund. Die Odds Ratio (OR) sagt uns hingegen - unter der Prämisse, dass “normale” Motoren eher mit automatischen und V-Motoren eher mit handgeschalteten Fahrwerken ausgestattet sind - dass die Chance doppelt so hoch ist, dass ein Auto mit “normalem” Motor automatisch geschaltet ist, als dies bei einem Auto mit V-Motor der Fall wäre\n\n\n#define dataset\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n\n# bei t-Test immer zuerst visualisieren: in diesem Fall Boxplot mit Variablen Getriebe (v- vs. s-motor) und Anzahl Pferdestärke\nggplot2::ggplot(cars, aes(y = hp, x = vs_cat)) +\n  # stat_boxplot(geom ='errorbar', width = .25) +\n  # geom_boxplot() +\n  geom_violin()+\n  labs(x = \"\\nBauform Motor\", y = \"Pferdestärke (PS)\\n\") +\n  mytheme\n\n\n\n  \n#alternativ     \nboxplot(cars$hp ~ cars$vs_cat) # sieht ganz ok aus, jedoch weist die variable \"normale Motoren\" deutlich eine grössere Streuung aus -> siehe aov.1 und deren Modelgüte-Plots\n\n\n\n\n# Definiere Model: t-Test, wobei die AV metrisch (in unserem Fall eine Zählvariable) sein muss\nttest <- t.test(cars$hp ~ cars$vs_cat)\naov.1 <- aov(cars$hp ~ cars$vs_cat)\n\n\n#schaue Modellgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n\n\n\n#zeige resultate\nttest\n\n\n\n    Welch Two Sample t-test\n\ndata:  cars$hp by cars$vs_cat\nt = 6.2908, df = 23.561, p-value = 1.82e-06\nalternative hypothesis: true difference in means between group normal and group v-type is not equal to 0\n95 percent confidence interval:\n  66.06161 130.66854\nsample estimates:\nmean in group normal mean in group v-type \n           189.72222             91.35714 \n\nsummary.lm(aov.1)\n\n\n\nCall:\naov(formula = cars$hp ~ cars$vs_cat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98.72 -25.61  -4.04  22.55 145.28 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         189.72      11.35  16.720  < 2e-16 ***\ncars$vs_catv-type   -98.37      17.16  -5.734 2.94e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.14 on 30 degrees of freedom\nMultiple R-squared:  0.5229,    Adjusted R-squared:  0.507 \nF-statistic: 32.88 on 1 and 30 DF,  p-value: 2.941e-06\n\n\n#wie würdet ihr nun die Ergebnisse darstellen?\n\n\n\n\n\n# für mehr infos here: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\n\nlibrary(datasauRus)\nif(requireNamespace(\"dplyr\")){\n  suppressPackageStartupMessages(library(dplyr))\n  dt <- datasaurus_dozen %>% \n    group_by(dataset) %>% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n}\n\n# check data structure\nglimpse(dt)\n\n\n# plot two examples  \nif(requireNamespace(\"ggplot2\")){\n  library(ggplot2)\n  \n  dt = filter(datasaurus_dozen, dataset == \"dino\" | dataset == \"slant_up\")\n  \n  ggplot(dt, aes(x=x, y=y, colour=dataset))+\n    geom_point()+\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    facet_wrap(~dataset) +\n    geom_smooth(method = \"lm\", se = FALSE)\n  \n}\n\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung1_Demo_assoziationen/distill-preview.png",
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_open_datasets/",
    "title": "Open Datasets",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\n\n\n\nVerschiedene Datensätze\nIn diesem Dokument findet ihr verschiedene Wege und Quellen, um an Datensätze zu gelangen.\nin R\nIn R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind:\nsleep\nUSAccDeaths\nUSArrests\n…\n\n\ndata() # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind\nhead(chickwts)\n\n\n  weight      feed\n1    179 horsebean\n2    160 horsebean\n3    136 horsebean\n4    227 horsebean\n5    217 horsebean\n6    168 horsebean\n\nstr(chickwts)\n\n\n'data.frame':   71 obs. of  2 variables:\n $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ...\n\nKaggle\nAuf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind:\n911\nfoodPreferences\nS.F. salaries\n…\n\n\n# Load packages and data\ndata_911 <- read_delim(\"911.csv\", delim = \",\")\nstr(data_911)\n\n\nspec_tbl_df [99,492 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ lat      : num [1:99492] 40.3 40.3 40.1 40.1 40.3 ...\n $ lng      : num [1:99492] -75.6 -75.3 -75.4 -75.3 -75.6 ...\n $ desc     : chr [1:99492] \"REINDEER CT & DEAD END;  NEW HANOVER; Station 332; 2015-12-10 @ 17:10:52;\" \"BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP; Station 345; 2015-12-10 @ 17:29:21;\" \"HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-Station:STA27;\" \"AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A; 2015-12-10 @ 16:47:36;\" ...\n $ zip      : num [1:99492] 19525 19446 19401 19401 NA ...\n $ title    : chr [1:99492] \"EMS: BACK PAINS/INJURY\" \"EMS: DIABETIC EMERGENCY\" \"Fire: GAS-ODOR/LEAK\" \"EMS: CARDIAC EMERGENCY\" ...\n $ timeStamp: POSIXct[1:99492], format: \"2015-12-10 17:40:00\" ...\n $ twp      : chr [1:99492] \"NEW HANOVER\" \"HATFIELD TOWNSHIP\" \"NORRISTOWN\" \"NORRISTOWN\" ...\n $ addr     : chr [1:99492] \"REINDEER CT & DEAD END\" \"BRIAR PATH & WHITEMARSH LN\" \"HAWS AVE\" \"AIRY ST & SWEDE ST\" ...\n $ e        : num [1:99492] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   lat = col_double(),\n  ..   lng = col_double(),\n  ..   desc = col_character(),\n  ..   zip = col_double(),\n  ..   title = col_character(),\n  ..   timeStamp = col_datetime(format = \"\"),\n  ..   twp = col_character(),\n  ..   addr = col_character(),\n  ..   e = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nTidytuesday\nTidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind:\nWomen in the Workplace\nDairy production & Consumption\nStar Wars Survey\nGlobal Coffee Chains\nMalaria Deaths\n…\nDownload via Github - 1. Möglichkeit\nGeht zum File, welches ihr herunterladen wollt\nKlickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen \nKlickt mit der rechten Maustaste auf den Knopf “raw” \n(Ziel) Speichern unter…\nDownload via Github - 2. Möglichkeit\n\n\n# Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt \nstar_wars <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv\", locale = readr::locale(encoding = \"latin1\")) #not working yet \n\n\n\nopendata.swiss\nAuf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind:\nStatistik der Schweizer Städte\nVerpflegungsbetriebe nach Jahr und Stadtquartier\nAltpapiermengen\n…\nOpen Data Katalog Stadt Zürich\nAuf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind:\nBevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970\nLuftqualitätsmessungen\nHäufigste Hauptsprachen\n…\n\n\n# lade die Datei \"Häufigste Sprachen\"\nurlfile = \"https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv\"\n\ndat_lang <- read_delim(url(urlfile), delim = \",\", col_names = T)\nhead(dat_lang)\n\n\n# A tibble: 6 x 7\n  Sprache  AntBev AnzBev untAntBevKI obAntBevKI untAnzBevKI obAnzBevKI\n  <chr>     <dbl>  <dbl>       <dbl>      <dbl>       <dbl>      <dbl>\n1 Deutsch    75.6 259680        75.2       76.1      258000     261360\n2 Englisch   12.9  44190        12.5       13.3       42880      45490\n3 Italien~    6.1  21040         5.9        6.4       20120      21950\n4 Französ~    4.8  16460         4.6        5         15650      17280\n5 Spanisch    4.3  14710         4.1        4.5       13920      15500\n6 Serbokr~    3.4  11560         3.2        3.6       10860      12260\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {}
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_suggest_datasets/",
    "title": "Vorgeschlagene Datensätze",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\nVorgeschlagene Datensätze zur Repetition der Übungen\nR data sets\nchickwts\niris\nTitanic (achtung hat das “table” Datenformat)\nstarwars (dplyr package)\nResearch Methods data sets\nNOVANIMAL (Kassendaten oder Gästebefragung)\nUkraine (Demoskript 3)\nIpomopsis (Demoskript 3)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {}
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung2_Demo_ordinationen/",
    "title": "Konsolidierung 2: Ordinationen",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "Statistik_Konsolidierung2"
    ],
    "contents": "\n\nContents\nDemo Ordinationen (PCA)\nPCA mit mtcars\nCA mit mtcars\nNMDS mit mtcars\nPCA mit sveg\nPCA mit Beispiel aus Vorlesung\nCA mit sveg\nNMDS mit sveg\n\n\n\n\n\n\nDownload R-Skript\n\nDemo Ordinationen (PCA)\nPCA mit mtcars\n\n\n\n#Beispiel inspiriert von Luke Hayden: https://www.datacamp.com/community/tutorials/pca-analysis-r\n\n#Ausgangslage: viel zusammenhängende Variablen\n#Ziel: Reduktion der Variablenkomplexität\n#WICHTIG hier: Datenformat muss Wide sein! Damit die Matrixmultiplikation gemacht werden kann\n\n# lade Datei\ncars <- mtcars\n\n# Korrelationen\ncor<- cor(cars[,c(1:7,10,11)])\ncor[abs(cor)<.7] <- 0\ncor\n\n#definiere Datei für PCA\ncars <- mtcars[,c(1:7,10,11)]\n\n\n# pca\n# achtung unterschiedliche messeinheiten, wichtig es muss noch einheitlich transfomiert werden\nlibrary(FactoMineR) # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\no.pca <- PCA(cars, scale.unit = TRUE) # entweder korrelations oder covarianzmatrix\n\n\n\n\n# schaue output an\nsummary(o.pca) # generiert auch automatische plots\n\n\n# plote das ganze\nlibrary(ggbiplot)\nggbiplot(o.pca,choices = c(1,2))\n\n\n\n\n# nehme noch die autonamen hinzu\nggbiplot(o.pca, labels=rownames(mtcars), choices = c(1,2)) # (+ mytheme) # choice gibt die axen an\n\n\n\n\nCA mit mtcars\n\n\n\nlibrary(vegan)\n\n# ebenfalls mit transformierten daten\no.ca<-vegan::cca(cars)\no.ca1 <- FactoMineR::CA(cars) #blau: auots, rot: variablen\n\n\n\n\n# plotten (schwarz: autos, rot: variablen)\nplot(o.ca)\n\n\n\nsummary(o.ca)\nsummary(o.ca1)\n\n#Nur autos plotten; wieso?\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:8]/sum(o.ca$CA$eig)\n\n\n\nNMDS mit mtcars\n\n\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\n\nmde <-vegan::vegdist(cars,method=\"euclidean\")\nmdm <-vegan::vegdist(cars,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.mde.mass <- MASS::isoMDS(mde, k=2) # mit K = Dimensionen\no.mdm.mass <- MASS::isoMDS(mdm)\n\nset.seed(1)\no.mde.vegan <- vegan::metaMDS(mde,k=1) # scheint nicht mit 2 Dimensionen zu konvergieren\no.mdm.vegan <- vegan::metaMDS(mdm, k = 2)\n\n#plot euclidean distance\nplot(o.mde.mass$points)\n\n\n\nplot(o.mde.vegan$points)\n\n\n\n\n#plot manhattan distance\nplot(o.mdm.mass$points)\n\n\n\nplot(o.mdm.vegan$points)\n\n\n\n\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nvegan::stressplot(o.mde.vegan, mde)\n\n\n\nvegan::stressplot(o.mde.mass, mde)\n\n\n\n\nPCA mit sveg\n\n\n\n#Mit Beispieldaten aus Wildi (2013, 2017)\nlibrary(labdsv)\nlibrary(dave) # lade package für Daten sveg\nhead(sveg)\nstr(sveg)\nView(sveg)\n\n#PCA-----------\n#Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\no.pca <- labdsv::pca(sveg^0.25,cor=T)\no.pca2 <- stats::prcomp(sveg^0.25)\n\n#Koordinaten im Ordinationsraum => Y\nhead(o.pca$scores)\nhead(o.pca2$x)\n\n#Korrelationen der Variablen mit den Ordinationsachsen\nhead(o.pca$loadings)\nhead(o.pca2$rotation)\n\n#Erklaerte Varianz der Achsen (sdev ist die Wurzel daraus)\n# früher gabs den Befehl summary()\n# jetzt von hand: standardabweichung im quadrat/totale varianz * 100 (um prozentwerte zu bekommen)\nE<-o.pca$sdev^2/o.pca$totdev*100\nE[1:5] # erste fünf PCA\n\n#package stats funktioniert summary()\nsummary(o.pca2)\n\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(o.pca$scores[,1],o.pca$scores[,2], type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC2\")\npoints(o.pca$scores[,1],o.pca$scores[,2],pch=18)\n\n\n\n\nplot(o.pca$scores[,1],o.pca$scores[,3],type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC3\")\npoints(o.pca$scores[,1],o.pca$scores[,3],pch=18)\n\n\n\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96, 101, 119)\nsnames <- names(sveg[ , sel.sp])\nsnames\n\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen\n#(hier reduction der observationen)\nx <- o.pca$loadings[,1]\ny <- o.pca$loadings[,2]\nplot(x,y,type=\"n\",asp=1)\narrows(0,0,x[sel.sp],y[sel.sp],length=0.08)\ntext(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)\n\n\n\n\n# hier gehts noch zu weiteren Beispielen zu PCA's:\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n# https://stats.stackexchange.com/questions/222/what-are-principal-component-scores\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n\n\n\nPCA mit Beispiel aus Vorlesung\n\n\n#Idee von Ordinationen aus Wildi p. 73-74\n\n\n#Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n#Generieren von Daten\nraw <- matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1), nrow=6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\",\"r2\",\"r3\",\"r4\",\"r5\",\"r6\")\nraw\n\n#originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n\n#Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z,z), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(1,6), ylim=c(0,5),\n     xlab=\"Umweltgradient\",ylab=\"Deckung der Arten\")\npoints(x1~z, pch=21, type=\"b\")\npoints(y1~z, pch=16, type=\"b\")\n\n\n\n\n#zentrierte Daten\ncent <- scale(raw, scale=F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n#rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n\n#Visualisierung der Schritte im Ordinationsraum\nplot(c(y1,y2,y3)~c(x1,x2,x3), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(-4,4), \n     ylim=c(-4,4), xlab=\"Art 1\", ylab=\"Art 2\")\npoints(y1~x1, pch=21, type=\"b\", col=\"green\", lwd=2)\npoints(y2~x2, pch=16, type=\"b\",col=\"red\", lwd=2)\npoints(y3~x3, pch=17, type=\"b\", col=\"blue\", lwd=2)\n\n\n\n\n\n#zusammengefasst:-------\n\n#Durchführung der PCA\npca <- pca(raw)\n\n#Koordinaten im Ordinationsraum\npca$scores\n\n#Korrelationen der Variablen mit den Ordinationsachsen\npca$loadings\n\n#Erklärte Varianz der Achsen in Prozent\nE <- pca$sdev^2/pca$totdev*100\nE\n\n\n### excurs für weitere r-packages####\n\n#mit prcomp, ein weiteres Package für Ordinationen\npca.2 <- stats::prcomp(raw, scale=F)\nsummary(pca.2)\nplot(pca.2)\n\n\n\nbiplot(pca.2)\n\n\n\n\n#mit vegan, ein anderes Package für Ordinationen\npca.3 <- vegan::rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt- und Artdaten definiert werden\n#scores(pca.3,display=c(\"sites\"))\n#scores(pca.3,display=c(\"species\"))\nsummary(pca.3, axes=0)\nbiplot(pca.3, scaling=2)\nbiplot(pca.3, scaling=\"species\")#scaling=species macht das selbe wie scaling=2\n\n\n\n\nCA mit sveg\n\n\nlibrary(vegan)\nlibrary(dave) #for the dataset sveg\nlibrary(FactoMineR)# siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\n\n# ebenfalls mit transformierten daten\no.ca<-cca(sveg^0.5) #package vegan\no.ca1 <- CA(sveg^0.5) #package FactoMineR\n\n\n\n\n#Arten (o) und Communities (+) plotten\nplot(o.ca)\n\n\n\nsummary(o.ca1)\n\n\n#Nur Arten plotten\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)\n\n\n\nNMDS mit sveg\n\n\n#NMDS----------\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\nlibrary(vegan)\n\nmde <-vegdist(sveg,method=\"euclidean\")\nmdm <-vegdist(sveg,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds<-isoMDS(mde, k=2) # mit K = Dimensionen\nset.seed(1)\no.mmds<-metaMDS(mde,k=3) # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\nstressplot(o.mmds,mde)\n\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung2_Demo_ordinationen/distill-preview.png",
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung3_Demo_LM/",
    "title": "Konsolidierung 3: Lineare Modelle",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "Statistik_Konsolidierung3"
    ],
    "contents": "\n\nContents\nDemo Lineare Modelle\nEinfaktorielle ANOVA\nMehrfaktorielle ANOVA\nEinfache Regression\nMultiple regression\n\n\n\n\n\nDemo Lineare Modelle\n\nDownload R-Skript\n\nEinfaktorielle ANOVA\n\n\n# für mehr infos\n#https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n\ncars <- mtcars %>% \n    mutate(cyl = as.factor(cyl)) %>% \n    slice(-31) # lösch die 31ste Zeile\n\n#Alternativ ginge auch das\ncars[-31,]\n\n# schaue daten zuerst mal an\n#1. Responsevariable\nhist(cars$hp) # nur sinnvoll bei grossem n\nboxplot(cars$hp)\n\n\n#2. Responsevariable ~ Prediktorvariable\ntable(cars$cyl) # mögliches probel, da n's unterschiedlich gross\n\nboxplot(cars$hp ~ cars$cyl) # varianzheterogentität weniger das problem, \n# aber normalverteilung der residuen problematisch\n\n# definiere das modell für eine ein-faktorielle anova\naov.1 <- aov(hp ~ cyl, data = cars)\n\n#3. Schaue Modelgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n#4. Schaue output an und ordne es ein\nsummary.lm(aov.1)\n\n\n#5. bei meheren Kategorien wende einen post-hoc Vergleichstest an\nTukeyHSD(aov.1)\n\n#6. Ergebnisse passend darstellen\nlibrary(multcomp)\n\n#erstens die signifikanten Unterschiede mit Buchstaben versehen\nletters <- multcomp::cld(multcomp::glht(aov.1, linfct=multcomp::mcp(cyl=\"Tukey\"))) # Achtung die kategoriale\n#Variable (unsere unabhängige Variable \"cyl\") muss als Faktor\n#definiert sein z.B. as.factor()\n\n#einfachere Variante\nboxplot(hp ~ cyl, data = cars)\nmtext(letters$mcletters$Letters, at=1:3)\n\n#schönere Variante :)\nggplot(cars, aes(x = cyl, y = hp)) +\n  stat_boxplot(geom = \"errorbar\", width = .5) +\n  geom_boxplot(size = 1) +\n  annotate(\"text\", x = 1:3, y = 350, label = letters$mcletters$Letters, size = 7) +\n  labs(x = \"\\nAnzahl Zylinder\", y = \"Pferdestärke\")  +\n  mytheme\n\n#Plot exportieren\nggsave(filename = \"distill-preview.png\",\n       device = \"png\") # hier kann man festlegen, was für ein Bildformat\n#exportiert werden möchte\n\n\n# Sind die Voraussetzungen für eine Anova verletzt, überprüfe alternative \n# nicht-parametische Tests z.B. oneway-Test mit Welch-korrektur für ungleiche\n# Varianzen (Achtung auch dieser Test hat Voraussetzungen -> siehe Skript XY)\nlibrary(rosetta)\nwelch1 <- oneway.test(hp ~ cyl, data = cars, var.equal = FALSE)\nrosetta::posthocTGH(cars$hp, cars$cyl, method = \"games-howell\")\n\n\n\nMehrfaktorielle ANOVA\n\n\n\nEinfache Regression\n\n\n# inspiriert von Simon Jackson: http s://drsimonj.svbtle.com/visualising-residuals\ncars <- mtcars %>% \n  #ändere die unabhängige Variable mpg in 100Km/L\n  mutate(kml = (235.214583/mpg)) # mehr Infos hier: https://www.asknumbers.com/mpg-to-L100km.aspx\n  # %>%  # klone data set\n  # slice(-31) # # lösche Maserrati und schaue nochmals Modelfit an\n\n#############\n##1.Daten anschauen\n############\n\n# Zusammenhang mal anschauen\n# Achtung kml = 100km pro Liter \nplot(hp ~ kml, data = cars)\n\n\n\n# Responsevariable anschauen\nboxplot(cars$hp)\n\n\n\n# Korrelationen uv + av anschauen\n# Reihenfolge spielt hier keine Rolle, wieso?\ncor(cars$kml, cars$hp) # hängen stark zusammen\n\n\n###################\n#2. Modell definieren: einfache regression\n##################\nmodel <- lm(hp ~ kml, data = cars)\nsummary.lm(model)\n\n###############\n#3.Modeldiagnostik und ggf. Anpassungen ans Modell oder ähnliches\n###############\n\n# semi schöne Ergebnisse\nlibrary(ggfortify)\nggplot2::autoplot(model) + mytheme # gitb einige Extremwerte => was tun? (Eingabe/Einlesen \n\n\n\n#überprüfen, Transformation, Extremwerte nur ausschliessen mit guter Begründung)\n\n\n# erzeuge vorhergesagte Werte und Residualwerte\ncars$predicted <- predict(model)   # bilde neue Variable mit geschätzten y-Werten\ncars$residuals <- residuals(model)\n\n# schaue es dir an, sieht man gut was die Residuen sind\nd <- cars %>%  \n    dplyr::select(hp, kml, predicted, residuals)\n\n# schauen wir es uns an\nhead(d, 4)\n\n#visualisiere residuen\nggplot(d, aes(x = kml, y = hp)) +\n  # verbinde beobachtete werte mit vorausgesagte werte\n  geom_segment(aes(xend = kml, yend = predicted)) + \n  geom_point() + # Plot the actual points\n  geom_point(aes(y = predicted), shape = 4) + # plot geschätzten y-Werten\n  # geom_line(aes(y = predicted), color = \"lightgrey\") # alternativ code\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  # Farbe wird hier zu den redisuen gemapped, abs(residuals) wegen negativen zahlen  \n  geom_point(aes(color = abs(residuals))) + \n  # Colors to use here (für mehrere farben verwende color_gradient2)\n  scale_color_continuous(low = \"blue\", high = \"red\") +  \n  scale_x_continuous(limits = c(0, 40)) +\n  scale_y_continuous(limits = c(0, 300)) +\n  guides(color = \"none\") +  # Color legende entfernen\n  labs(x = \"\\nVerbraucht in Liter pro 100km\", y = \"Motorleistung in PS\\n\") +\n  mytheme\n\n\n\n##########\n#4. plotte Ergebnis\n##########\nggplot(d, aes(x = kml, y = hp)) +\n    geom_point(size = 4) +\n    # geom_point(aes(y = predicted), shape = 1, size = 4) +\n    # plot regression line\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    #intercept\n    geom_line(aes(y = mean(hp)), color = \"blue\") +\n    mytheme\n\n\n\n\nMultiple regression\n\n\n# Select data\ncars <- mtcars %>% \n    slice(-31) %>%\n    mutate(kml = (235.214583/mpg)) %>% \n    dplyr::select(kml, hp, wt, disp)\n\n################\n# 1. Multikollinearitüt überprüfen\n# Korrelation zwischen Prädiktoren kleiner .7\ncor <- cor(cars[, -2])\ncor[abs(cor)<0.7] <- 0  \ncor # disp weglassen, vgl. model2\n\n##### info zu Variablen\n#wt = gewicht\n#disp = hubraum\n\n\n###############\n#2. Responsevariable + Kriteriumsvariable anschauen\n##############\n# was würdet ihr tun?\n\n\n############\n#3. Definiere das Model\n############\nmodel1 <- lm(hp ~ kml + wt + disp, data = cars) \nmodel2 <- lm(hp ~ kml + wt, data = cars)\nmodel3 <- lm(log10(hp) ~ kml + wt, data = cars)\n\n#############\n#4. Modeldiagnostik\n############\n\nlibrary(ggfortify)\nggplot2::autoplot(model1)\n\n\n\nggplot2::autoplot(model2) # besser, immernoch nicht ok => transformation? vgl. model3\n\n\n\nggplot2::autoplot(model3)\n\n\n\n############\n#5. Modellfit vorhersagen: wie gut sagt mein Modell meine Daten vorher\n############\n\n#es gibt 3 Mögliche Wege\n\n# gebe dir predicted values aus für model2 (für vorzeigebeispiel einfacher :)\n# gibts unterschidliche varianten die predicted values zu berechnen\n# 1. default funktion predict(model) verwenden\ncars$predicted <- predict(model2)\n\n# 2. datensatz selber zusammenstellen (nicht empfohlen): wichtig, die \n# prädiktoren müssen denselben\n# namen haben wie im Model\n# besser mit Traindata von Beginn an mehr Infos hier: https://www.r-bloggers.com/using-linear-regression-to-predict-energy-output-of-a-power-plant/\n\nnew.data <- tibble(kml = sample(seq(6.9384, 22.61, .3), 31),\n                   wt = sample(seq(1.513, 5.424, 0.01), 31),\n                   disp = sample(seq(71.1, 472.0, .1), 31)) \ncars$predicted_own <- predict(model2, newdata = new.data)\n\n# 3. train_test_split durchführen (empfohlen) muss jedoch von beginn an bereits \n# gemacht werden - Logik findet ihr hier: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 oder https://towardsdatascience.com/6-amateur-mistakes-ive-made-working-with-train-test-splits-916fabb421bb\n# beispiel hier: https://ijlyttle.github.io/model_cv_selection.html\ncars <- mtcars %>% \n  mutate(id = 1:nrow(.)) %>%  # für das mergen der Datensätze\n  mutate(kml = (235.214583/mpg)) %>% \n  dplyr::select(kml, hp, wt, disp, id)\n  \ntrain_data <- cars %>% \n  dplyr::sample_frac(.75) # für das Modellfitting\n\ntest_data  <- dplyr::anti_join(cars, train_data, by = 'id') # für den Test mit predict\n\n\n# erstelle das Modell und \"trainiere\" es auf den train Datensatz\nmodel2_train <- lm(hp ~ kml + wt, data = train_data)\n\n\n# mit dem \"neuen\" Datensatz wird das Model überprüft ob guter Modelfit\ntrain_data$predicted_test <- predict(model2_train, newdata = test_data)\n\n\n# Residuen\ntrain_data$residuals <- residuals(model2_train)\nhead(train_data)\n\n\n\n#weiterführende Infos zu \"machine learning\" Idee hier: https://stat-ata-asu.github.io/MachineLearningToolbox/regression-models-fitting-them-and-evaluating-their-performance.html\n#wichtigstes Packet in dieser Hinsicht ist \"caret\": https://topepo.github.io/caret/\n#beste Philosophie ist tidymodels: https://www.tidymodels.org\n\n\n\n#----------------\n# Schnelle variante mit broom\nd <- lm(hp ~ kml + wt+ disp, data = cars) %>% \n    broom::augment()\n\nhead(d)\n\nggplot(d, aes(x = kml, y = hp)) +\n    geom_segment(aes(xend = kml, yend = .fitted), alpha = .2) +\n    geom_point(aes(color = .resid)) +\n    scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n    guides(color = \"none\") +\n    geom_point(aes(y = .fitted), shape = 4) +\n    scale_y_continuous(limits = c(0,350)) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    mytheme\n\n\n\n############\n# 6. Modellvereinfachung\n############\n\n\n# Varianzpartitionierung\nlibrary(hier.part)\ncars <- mtcars %>% \n  mutate(kml = (235.214583/mpg)) %>% \n  select(-mpg)\n\n\nnames(cars) # finde \"position\" deiner Responsevariable\n\nX = cars[, -3] # definiere all die Prädiktorvariablen im Model (minus Responsevar)\n\n# dauert ein paar sekunden\nhier.part(cars$hp, X, gof = \"Rsqu\")\n\n\n\n# alle Modelle miteinander vergleichen mit dredge Befehl: geht nur bis \n# maximal 15 Variablen\nmodel2 <- lm(hp ~ ., data = cars)\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")\nallmodels <- dredge(model2)\nhead(allmodels)\n\n# Wichtigkeit der Prädiktoren\nMuMIn::importance(allmodels)\n\n# mittleres Model\navgmodel<- MuMIn::model.avg(get.models(allmodels, subset=TRUE))\nsummary(avgmodel)\n\n# adäquatest model gemäss multimodel inference\nmodel_ad <- lm(hp ~ carb + disp + wt, data = mtcars)\nsummary(model_ad)\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung3_Demo_LM/distill-preview.png",
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung4_Demo_GLM/",
    "title": "Konsolidierung 4: GLM",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "Statistik_Konsolidierung4"
    ],
    "contents": "\n\nContents\nDemo GLM\nPoisson Regression\nlogistische Regression\nGAM’s\n\n\n\n\n  |                                                                  \n  |                                                            |   0%\n  |                                                                  \n  |....................                                        |  33%\n  |                                                                  \n  |........................................                    |  67%\n  |                                                                  \n  |............................................................| 100%\n[1] \"Demo_GLM_Konsolidierung4.R\"\n\nDemo GLM\n\nDownload R-Skript\n\nPoisson Regression\n\n\n############\n# quasipoisson regression\n############\n\ncars <- mtcars %>% \n   mutate(kml = (235.214583/mpg))\n\nglm.poisson <- glm(hp ~ kml, data = cars, family = poisson(link = log))\n\nsummary(glm.poisson) # klare overdisperion\n\n\n\nCall:\nglm(formula = hp ~ kml, family = poisson(link = log), data = cars)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.438  -2.238  -1.159   2.457  10.576  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 3.894293   0.050262   77.48   <2e-16 ***\nkml         0.081666   0.003414   23.92   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 958.27  on 31  degrees of freedom\nResidual deviance: 426.59  on 30  degrees of freedom\nAIC: 645.67\n\nNumber of Fisher Scoring iterations: 4\n\n\n# deshalb quasipoisson\nglm.quasipoisson <- glm(hp ~ kml, data = cars, family = quasipoisson(link = log))\n\nsummary(glm.quasipoisson)\n\n\n\nCall:\nglm(formula = hp ~ kml, family = quasipoisson(link = log), data = cars)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.438  -2.238  -1.159   2.457  10.576  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.89429    0.19508  19.963  < 2e-16 ***\nkml          0.08167    0.01325   6.164 8.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 15.06438)\n\n    Null deviance: 958.27  on 31  degrees of freedom\nResidual deviance: 426.59  on 30  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# visualisiere\nggplot2::ggplot(cars, aes(x = kml, y = hp)) + \n    geom_point(size = 8) + \n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = F,\n                color = \"green\", size = 2) + \n    scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    theme_classic()\n\n\n\n\n\n#Rücktransformation meines Outputs für ein besseres Verständnis\nglm.quasi.back <- exp(coef(glm.quasipoisson))\n\n#für ein schönes ergebnis\nglm.quasi.back %>%\n  broom::tidy() %>% \n  knitr::kable(digits = 3)\n\n\nnames\nx\n(Intercept)\n49.121\nkml\n1.085\n\n\n#for more infos, also for posthoc tests\n#here: https://rcompanion.org/handbook/J_01.html\n\n\n\n\nlogistische Regression\n\n\n############\n# logistische regression\n############\ncars <- mtcars\n\n# erstelle das modell\nglm.binar <- glm(vs ~ hp, data = cars, family = binomial(link = logit)) \n\n\n#achtung Model gibt Koeffizienten als logit() zurück\nsummary(glm.binar)\n\n\n\nCall:\nglm(formula = vs ~ hp, family = binomial(link = logit), data = cars)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.12148  -0.20302  -0.01598   0.51173   1.20083  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  8.37802    3.21593   2.605  0.00918 **\nhp          -0.06856    0.02740  -2.502  0.01234 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 16.838  on 30  degrees of freedom\nAIC: 20.838\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n# überprüfe das modell\ncars$predicted <- predict(glm.binar, type = \"response\")\n\n\n# visualisiere\nggplot(cars, aes(x = hp, y = vs)) +    \n    geom_point(size = 8) +\n    geom_point(aes(y = predicted), shape  = 1, size = 6) +\n    guides(color = \"none\") +\n    geom_smooth(method = \"glm\", method.args = list(family = 'binomial'), \n                se = FALSE,\n                size = 2) +\n    # geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    mytheme\n\n\n\n\n#Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.binar$deviance,glm.binar$df.resid)  \n\n\n[1] 0.9744718\n\n\n\n#Modellgüte (pseudo-R²)\n1 - (glm.binar$dev / glm.binar$null)  \n\n\n[1] 0.6161072\n\n\n#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs. x)\nexp(glm.binar$coefficients[2])\n\n\n       hp \n0.9337368 \n\n\n\n#LD50 (wieso negativ: weil zweiter koeffizient negative steigung hat)\nabs(glm.binar$coefficients[1]/glm.binar$coefficients[2])\n\n\n(Intercept) \n   122.1986 \n\n\n\n# kreuztabelle (confusion matrix): fasse die ergebnisse aus predict und \n# \"gegebenheiten, realität\" zusammen\ntab1 <- table(cars$predicted>.5, cars$vs)\ndimnames(tab1) <- list(c(\"M:S-type\",\"M:V-type\"), c(\"T:S-type\", \"T:V-type\"))\ntab1 \n\n\n         T:S-type T:V-type\nM:S-type       15        2\nM:V-type        3       12\n\n\n\nprop.table(tab1, 2) \n\n\n          T:S-type  T:V-type\nM:S-type 0.8333333 0.1428571\nM:V-type 0.1666667 0.8571429\n\n#was könnt ihr daraus ablesen? Ist unser Modell genau?\n\n\n\n\n# Funktion die die logits in Wahrscheinlichkeiten transformiert\n# mehr infos hier: https://sebastiansauer.github.io/convert_logit2prob/\n# dies ist interessant, falls ihr mal ein kategorialer Prädiktor habt\nlogit2prob <- function(logit){\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n\n\n\n\nGAM’s\n\n\n\n###########\n# LOESS & GAM\n###########\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n  geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"blue\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    mytheme\n\n\n\n\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n    # geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"grey\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n  scale_y_continuous(limits = c(0,400)) + \n  mytheme\n\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung4_Demo_GLM/distill-preview.png",
    "last_modified": "2021-11-17T07:39:12+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
